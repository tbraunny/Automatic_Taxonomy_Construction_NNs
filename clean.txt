ImageNet Classification with Deep Convolutional Neural Networks AlexKrizhevsky IlyaSutskever GeoffreyE.Hinton UniversityofToronto UniversityofToronto UniversityofToronto kriz@cs.utoronto.ca ilya@cs.utoronto.ca hinton@cs.utoronto.ca Abstract Wetrainedalarge,deepconvolutionalneuralnetworktoclassifythe1.2million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 dif- ferentclasses. Onthetestdata,weachievedtop-1andtop-5errorratesof37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists offiveconvolutionallayers, someofwhicharefollowedbymax-poolinglayers, and three fully-connected layers with a final 1000-way softmax. To make train- ing faster, we used non-saturating neurons and a very efficient GPU implemen- tation of the convolution operation. To reduce overfitting in the fully-connected layersweemployedarecently-developedregularizationmethodcalled“dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012competitionandachievedawinningtop-5testerrorrateof15.3%, comparedto26.2%achievedbythesecond-bestentry. 1 Introduction Currentapproachestoobjectrecognitionmakeessentialuseofmachinelearningmethods. Toim- prove their performance, we can collect larger datasets, learn more powerful models, and use bet- tertechniquesforpreventingoverfitting. Untilrecently, datasetsoflabeledimageswererelatively small—ontheorderoftensofthousandsofimages(e.g.,NORB,Caltech-101/256[8,9],and CIFAR-10/100 ). Simple recognition tasks can be solved quite well with datasets of this size, especially if they are augmented with label-preserving transformations. For example, the current- best error rate on the MNIST digit-recognition task (<0.3%) approaches human performance . But objects in realistic settings exhibit considerable variability, so to learn to recognize them it is necessary to use much larger training sets. And indeed, the shortcomings of small image datasets havebeenwidelyrecognized(e.g.,Pintoetal.),butithasonlyrecentlybecomepossibletocol- lectlabeleddatasetswithmillionsofimages. ThenewlargerdatasetsincludeLabelMe,which consistsofhundredsofthousandsoffully-segmentedimages,andImageNet,whichconsistsof over15millionlabeledhigh-resolutionimagesinover22,000categories. Tolearnaboutthousandsofobjectsfrommillionsofimages,weneedamodelwithalargelearning capacity. However, the immense complexity of the object recognition task means that this prob- lemcannotbespecifiedevenbyadatasetaslargeasImageNet,soourmodelshouldalsohavelots of prior knowledge to compensate for all the data we don’t have. Convolutional neural networks (CNNs)constituteonesuchclassofmodels[16,11,13,18,15,22,26]. Theircapacitycanbecon- trolledbyvaryingtheirdepthandbreadth,andtheyalsomakestrongandmostlycorrectassumptions about the nature of images (namely, stationarity of statistics and locality of pixel dependencies). Thus, compared to standard feedforward neural networks with similarly-sized layers, CNNs have muchfewerconnectionsandparametersandsotheyareeasiertotrain,whiletheirtheoretically-best performanceislikelytobeonlyslightlyworse. 1

DespitetheattractivequalitiesofCNNs,anddespitetherelativeefficiencyoftheirlocalarchitecture, theyhavestillbeenprohibitivelyexpensivetoapplyinlargescaletohigh-resolutionimages. Luck- ily,currentGPUs,pairedwithahighly-optimizedimplementationof2Dconvolution,arepowerful enoughtofacilitatethetrainingofinterestingly-largeCNNs,andrecentdatasetssuchasImageNet containenoughlabeledexamplestotrainsuchmodelswithoutsevereoverfitting. Thespecificcontributionsofthispaperareasfollows: wetrainedoneofthelargestconvolutional neuralnetworkstodateonthesubsetsofImageNetusedintheILSVRC-2010andILSVRC-2012 competitions  and achieved by far the best results ever reported on these datasets. We wrote a highly-optimized GPU implementation of 2D convolution and all the other operations inherent in trainingconvolutionalneuralnetworks, whichwemakeavailablepublicly1. Ournetworkcontains anumberofnewandunusualfeatureswhichimproveitsperformanceandreduceitstrainingtime, whicharedetailedinSection3.Thesizeofournetworkmadeoverfittingasignificantproblem,even with 1.2 million labeled training examples, so we used several effective techniques for preventing overfitting, which are described in Section 4. Our final network contains five convolutional and three fully-connected layers, and this depth seems to be important: we found that removing any convolutionallayer(eachofwhichcontainsnomorethan1%ofthemodel’sparameters)resultedin inferiorperformance. Intheend,thenetwork’ssizeislimitedmainlybytheamountofmemoryavailableoncurrentGPUs andbytheamountoftrainingtimethatwearewillingtotolerate. Ournetworktakesbetweenfive andsixdaystotrainontwoGTX5803GBGPUs. Allofourexperimentssuggestthatourresults canbeimprovedsimplybywaitingforfasterGPUsandbiggerdatasetstobecomeavailable. 2 TheDataset ImageNetisadatasetofover15millionlabeledhigh-resolutionimagesbelongingtoroughly22,000 categories. The images were collected from the web and labeled by human labelers using Ama- zon’s Mechanical Turk crowd-sourcing tool. Starting in 2010, as part of the Pascal Visual Object Challenge, an annual competition called the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC)hasbeenheld. ILSVRCusesasubsetofImageNetwithroughly1000imagesineachof 1000categories. Inall,thereareroughly1.2milliontrainingimages,50,000validationimages,and 150,000testingimages. ILSVRC-2010 is the only version of ILSVRC for which the test set labels are available, so this is the version on which we performed most of our experiments. Since we also entered our model in theILSVRC-2012competition, inSection6wereportourresultsonthisversionofthedatasetas well,forwhichtestsetlabelsareunavailable.OnImageNet,itiscustomarytoreporttwoerrorrates: top-1andtop-5,wherethetop-5errorrateisthefractionoftestimagesforwhichthecorrectlabel isnotamongthefivelabelsconsideredmostprobablebythemodel. ImageNetconsistsofvariable-resolutionimages,whileoursystemrequiresaconstantinputdimen- sionality. Therefore, we down-sampled the images to a fixed resolution of 256 × 256. Given a rectangularimage,wefirstrescaledtheimagesuchthattheshortersidewasoflength256,andthen croppedoutthecentral256×256patchfromtheresultingimage.Wedidnotpre-processtheimages inanyotherway,exceptforsubtractingthemeanactivityoverthetrainingsetfromeachpixel. So wetrainedournetworkonthe(centered)rawRGBvaluesofthepixels. 3 TheArchitecture The architecture of our network is summarized in Figure 2. It contains eight learned layers — five convolutional and three fully-connected. Below, we describe some of the novel or unusual features of our network’s architecture. Sections 3.1-3.4 are sorted according to our estimation of theirimportance,withthemostimportantfirst. 1http://code.google.com/p/cuda-convnet/ 2

3.1 ReLUNonlinearity The standard way to model a neuron’s output f as a function of its input x is with f(x) = tanh(x) or f(x) = (1 + e−x)−1. In terms of training time with gradient descent, these saturating nonlinearities are much slower than the non-saturating nonlinearity f(x) = max(0,x). Following Nair and Hinton , werefertoneuronswiththisnonlinearityasRectified LinearUnits(ReLUs). Deepconvolutionalneuralnet- workswithReLUstrainseveraltimesfasterthantheir equivalents with tanh units. This is demonstrated in Figure 1, which shows the number of iterations re- quired to reach 25% training error on the CIFAR-10 dataset for a particular four-layer convolutional net- work. This plot shows that we would not have been abletoexperimentwithsuchlargeneuralnetworksfor thisworkifwehadusedtraditionalsaturatingneuron Figure 1: A four-layer convolutional neural models. networkwithReLUs(solidline)reachesa25% trainingerrorrateonCIFAR-10sixtimesfaster We are not the first to consider alternatives to tradi- thananequivalentnetworkwithtanhneurons tional neuron models in CNNs. For example, Jarrett (dashedline). Thelearningratesforeachnet- etal.claimthatthenonlinearityf(x)=|tanh(x)| workwerechosenindependentlytomaketrain- worksparticularlywellwiththeirtypeofcontrastnor- ing as fast as possible. No regularization of malization followed by local average pooling on the anykindwasemployed. Themagnitudeofthe Caltech-101dataset. However,onthisdatasetthepri- effect demonstrated here varies with network mary concern is preventing overfitting, so the effect architecture, butnetworkswithReLUsconsis- they are observing is different from the accelerated tentlylearnseveraltimesfasterthanequivalents abilitytofitthetrainingsetwhichwereportwhenus- withsaturatingneurons. ingReLUs.Fasterlearninghasagreatinfluenceonthe performanceoflargemodelstrainedonlargedatasets. 3.2 TrainingonMultipleGPUs AsingleGTX580GPUhasonly3GBofmemory,whichlimitsthemaximumsizeofthenetworks thatcanbetrainedonit. Itturnsoutthat1.2milliontrainingexamplesareenoughtotrainnetworks whicharetoobigtofitononeGPU.ThereforewespreadthenetacrosstwoGPUs. CurrentGPUs areparticularlywell-suitedtocross-GPUparallelization,astheyareabletoreadfromandwriteto one another’s memory directly, without going through host machine memory. The parallelization scheme that we employ essentially puts half of the kernels (or neurons) on each GPU, with one additional trick: the GPUs communicate only in certain layers. This means that, for example, the kernelsoflayer3takeinputfromallkernelmapsinlayer2. However,kernelsinlayer4takeinput only from those kernel maps in layer 3 which reside on the same GPU. Choosing the pattern of connectivity is a problem for cross-validation, but this allows us to precisely tune the amount of communicationuntilitisanacceptablefractionoftheamountofcomputation. Theresultantarchitectureissomewhatsimilartothatofthe“columnar”CNNemployedbyCires¸an etal.,exceptthatourcolumnsarenotindependent(seeFigure2).Thisschemereducesourtop-1 and top-5 error rates by 1.7% and 1.2%, respectively, as compared with a net with half as many kernelsineachconvolutionallayertrainedononeGPU.Thetwo-GPUnettakesslightlylesstime totrainthantheone-GPUnet2. 2Theone-GPUnetactuallyhasthesamenumberofkernelsasthetwo-GPUnetinthefinalconvolutional layer. Thisisbecausemostofthenet’sparametersareinthefirstfully-connectedlayer,whichtakesthelast convolutionallayerasinput. Sotomakethetwonetshaveapproximatelythesamenumberofparameters,we didnothalvethesizeofthefinalconvolutionallayer(northefully-connecedlayerswhichfollow). Therefore thiscomparisonisbiasedinfavoroftheone-GPUnet,sinceitisbiggerthan“halfthesize”ofthetwo-GPU net. 3

3.3 LocalResponseNormalization ReLUs have the desirable property that they do not require input normalization to prevent them fromsaturating. IfatleastsometrainingexamplesproduceapositiveinputtoaReLU,learningwill happen in that neuron. However, we still find that the following local normalization scheme aids generalization. Denotingbyai theactivityofaneuroncomputedbyapplyingkerneliatposition x,y (x,y) and then applying the ReLU nonlinearity, the response-normalized activity bi is given by x,y theexpression  β min(N−1,i+n/2) (cid:88) bi x,y =ai x,y/k+α (aj x,y)2  j=max(0,i−n/2) wherethesumrunsovern“adjacent”kernelmapsatthesamespatialposition, andN isthetotal numberofkernelsinthelayer.Theorderingofthekernelmapsisofcoursearbitraryanddetermined beforetrainingbegins. Thissortofresponsenormalizationimplementsaformoflateralinhibition inspired by the type found in real neurons, creating competition for big activities amongst neuron outputscomputedusingdifferentkernels. Theconstantsk,n,α,andβ arehyper-parameterswhose valuesaredeterminedusingavalidationset;weusedk = 2,n = 5,α = 10−4,andβ = 0.75. We appliedthisnormalizationafterapplyingtheReLUnonlinearityincertainlayers(seeSection3.5). ThisschemebearssomeresemblancetothelocalcontrastnormalizationschemeofJarrettetal., but ours would be more correctly termed “brightness normalization”, since we do not subtract the meanactivity. Responsenormalizationreducesourtop-1andtop-5errorratesby1.4%and1.2%, respectively.WealsoverifiedtheeffectivenessofthisschemeontheCIFAR-10dataset:afour-layer CNNachieveda13%testerrorratewithoutnormalizationand11%withnormalization3. 3.4 OverlappingPooling PoolinglayersinCNNssummarizetheoutputsofneighboringgroupsofneuronsinthesamekernel map. Traditionally, theneighborhoodssummarizedbyadjacentpoolingunitsdonotoverlap(e.g., [17,11,4]). Tobemoreprecise,apoolinglayercanbethoughtofasconsistingofagridofpooling unitsspacedspixelsapart,eachsummarizinganeighborhoodofsizez×zcenteredatthelocation of the pooling unit. If we set s = z, we obtain traditional local pooling as commonly employed in CNNs. If we set s < z, we obtain overlapping pooling. This is what we use throughout our network,withs = 2andz = 3. Thisschemereducesthetop-1andtop-5errorratesby0.4%and 0.3%, respectively, as compared with the non-overlapping scheme s = 2,z = 2, which produces outputofequivalentdimensions.Wegenerallyobserveduringtrainingthatmodelswithoverlapping poolingfinditslightlymoredifficulttooverfit. 3.5 OverallArchitecture NowwearereadytodescribetheoverallarchitectureofourCNN.AsdepictedinFigure2,thenet containseightlayerswithweights;thefirstfiveareconvolutionalandtheremainingthreearefully- connected.Theoutputofthelastfully-connectedlayerisfedtoa1000-waysoftmaxwhichproduces adistributionoverthe1000classlabels.Ournetworkmaximizesthemultinomiallogisticregression objective,whichisequivalenttomaximizingtheaverageacrosstrainingcasesofthelog-probability ofthecorrectlabelunderthepredictiondistribution. Thekernelsofthesecond,fourth,andfifthconvolutionallayersareconnectedonlytothosekernel mapsinthepreviouslayerwhichresideonthesameGPU(seeFigure2). Thekernelsofthethird convolutionallayerareconnectedtoallkernelmapsinthesecondlayer. Theneuronsinthefully- connectedlayersareconnectedtoallneuronsinthepreviouslayer. Response-normalizationlayers followthefirstandsecondconvolutionallayers.Max-poolinglayers,ofthekinddescribedinSection 3.4, follow both response-normalization layers as well as the fifth convolutional layer. The ReLU non-linearityisappliedtotheoutputofeveryconvolutionalandfully-connectedlayer. Thefirstconvolutionallayerfiltersthe224×224×3inputimagewith96kernelsofsize11×11×3 with a stride of 4 pixels (this is the distance between the receptive field centers of neighboring 3Wecannotdescribethisnetworkindetailduetospaceconstraints,butitisspecifiedpreciselybythecode andparameterfilesprovidedhere:http://code.google.com/p/cuda-convnet/. 4

Figure2: AnillustrationofthearchitectureofourCNN,explicitlyshowingthedelineationofresponsibilities betweenthetwoGPUs.OneGPUrunsthelayer-partsatthetopofthefigurewhiletheotherrunsthelayer-parts atthebottom.TheGPUscommunicateonlyatcertainlayers.Thenetwork’sinputis150,528-dimensional,and thenumberofneuronsinthenetwork’sremaininglayersisgivenby253,440–186,624–64,896–64,896–43,264– 4096–4096–1000. neuronsinakernelmap). Thesecondconvolutionallayertakesasinputthe(response-normalized andpooled)outputofthefirstconvolutionallayerandfiltersitwith256kernelsofsize5×5×48. Thethird,fourth,andfifthconvolutionallayersareconnectedtooneanotherwithoutanyintervening pooling or normalization layers. The third convolutional layer has 384 kernels of size 3 × 3 × 256 connected to the (normalized, pooled) outputs of the second convolutional layer. The fourth convolutionallayerhas384kernelsofsize3×3×192,andthefifthconvolutionallayerhas256 kernelsofsize3×3×192. Thefully-connectedlayershave4096neuronseach. 4 ReducingOverfitting Ourneuralnetworkarchitecturehas60millionparameters. Althoughthe1000classesofILSVRC makeeachtrainingexampleimpose10bitsofconstraintonthemappingfromimagetolabel,this turnsouttobeinsufficienttolearnsomanyparameterswithoutconsiderableoverfitting. Below,we describethetwoprimarywaysinwhichwecombatoverfitting. 4.1 DataAugmentation Theeasiestandmostcommonmethodtoreduceoverfittingonimagedataistoartificiallyenlarge thedatasetusinglabel-preservingtransformations(e.g., [25,4,5]). Weemploytwodistinctforms of data augmentation, both of which allow transformed images to be produced from the original images with very little computation, so the transformed images do not need to be stored on disk. Inourimplementation,thetransformedimagesaregeneratedinPythoncodeontheCPUwhilethe GPUistrainingonthepreviousbatchofimages. Sothesedataaugmentationschemesare,ineffect, computationallyfree. Thefirstformofdataaugmentationconsistsofgeneratingimagetranslationsandhorizontalreflec- tions. Wedothisbyextractingrandom224×224patches(andtheirhorizontalreflections)fromthe 256×256imagesandtrainingournetworkontheseextractedpatches4.Thisincreasesthesizeofour trainingsetbyafactorof2048,thoughtheresultingtrainingexamplesare,ofcourse,highlyinter- dependent.Withoutthisscheme,ournetworksuffersfromsubstantialoverfitting,whichwouldhave forcedustousemuchsmallernetworks. Attesttime,thenetworkmakesapredictionbyextracting five 224×224 patches (the four corner patches and the center patch) as well as their horizontal reflections(hencetenpatchesinall),andaveragingthepredictionsmadebythenetwork’ssoftmax layeronthetenpatches. The second form of data augmentation consists of altering the intensities of the RGB channels in training images. Specifically, we perform PCA on the set of RGB pixel values throughout the ImageNettrainingset. Toeachtrainingimage,weaddmultiplesofthefoundprincipalcomponents, 4ThisisthereasonwhytheinputimagesinFigure2are224×224×3-dimensional. 5

withmagnitudesproportionaltothecorrespondingeigenvaluestimesarandomvariabledrawnfrom aGaussianwithmeanzeroandstandarddeviation0.1. ThereforetoeachRGBimagepixelI = xy [IR,IG,IB]T weaddthefollowingquantity: xy xy xy [p ,p ,p ][α λ ,α λ ,α λ ]T 1 2 3 1 1 2 2 3 3 where p and λ are ith eigenvector and eigenvalue of the 3×3 covariance matrix of RGB pixel i i values, respectively, and α is the aforementioned random variable. Each α is drawn only once i i forallthepixelsofaparticulartrainingimageuntilthatimageisusedfortrainingagain,atwhich pointitisre-drawn. Thisschemeapproximatelycapturesanimportantpropertyofnaturalimages, namely,thatobjectidentityisinvarianttochangesintheintensityandcoloroftheillumination.This schemereducesthetop-1errorratebyover1%. 4.2 Dropout Combiningthepredictionsofmanydifferentmodelsisaverysuccessfulwaytoreducetesterrors [1, 3], but it appears to be too expensive for big neural networks that already take several days to train. There is, however, a very efficient version of model combination that only costs about a factor of two during training. The recently-introduced technique, called “dropout” , consists of setting to zero the output of each hidden neuron with probability 0.5. The neurons which are “dropped out” in this way do not contribute to the forward pass and do not participate in back- propagation.Soeverytimeaninputispresented,theneuralnetworksamplesadifferentarchitecture, butallthesearchitecturesshareweights.Thistechniquereducescomplexco-adaptationsofneurons, since a neuron cannot rely on the presence of particular other neurons. It is, therefore, forced to learnmorerobustfeaturesthatareusefulinconjunctionwithmanydifferentrandomsubsetsofthe other neurons. At test time, we use all the neurons but multiply their outputs by 0.5, which is a reasonableapproximationtotakingthegeometricmeanofthepredictivedistributionsproducedby theexponentially-manydropoutnetworks. Weusedropoutinthefirsttwofully-connectedlayersofFigure2.Withoutdropout,ournetworkex- hibitssubstantialoverfitting.Dropoutroughlydoublesthenumberofiterationsrequiredtoconverge. 5 Detailsoflearning We trained our models using stochastic gradient descent withabatchsizeof128examples,momentumof0.9,and weightdecayof0.0005. Wefoundthatthissmallamount ofweightdecaywasimportantforthemodeltolearn. In Figure3: 96convolutionalkernelsofsize otherwords,weightdecayhereisnotmerelyaregularizer: 11×11×3learnedbythefirstconvolutional it reduces the model’s training error. The update rule for layeronthe224×224×3inputimages.The weightwwas top48kernelswerelearnedonGPU1while (cid:28) ∂L(cid:12) (cid:29) thebottom48kernelswerelearnedonGPU v i+1 := 0.9·v i−0.0005·(cid:15)·w i−(cid:15)· ∂w(cid:12) wi 2.SeeSection6.1fordetails. Di w := w +v i+1 i i+1 whereiistheiterationindex,visthemomentumvariable,(cid:15)isthelearningrate,and(cid:68) ∂L(cid:12) (cid:12) (cid:69) is ∂w wi Di the average over the ith batch D of the derivative of the objective with respect to w, evaluated at i w . i Weinitializedtheweightsineachlayerfromazero-meanGaussiandistributionwithstandardde- viation0.01. Weinitializedtheneuronbiasesinthesecond, fourth, andfifthconvolutionallayers, as well as in the fully-connected hidden layers, with the constant 1. This initialization accelerates theearlystagesoflearningbyprovidingtheReLUswithpositiveinputs. Weinitializedtheneuron biasesintheremaininglayerswiththeconstant0. We used an equal learning rate for all layers, which we adjusted manually throughout training. The heuristic which we followed was to divide the learning rate by 10 when the validation error ratestoppedimprovingwiththecurrentlearningrate. Thelearningratewasinitializedat0.01and 6

reducedthreetimespriortotermination. Wetrainedthenetworkforroughly90cyclesthroughthe trainingsetof1.2millionimages,whichtookfivetosixdaysontwoNVIDIAGTX5803GBGPUs. 6 Results Our results on ILSVRC-2010 are summarized in Table 1. Our network achieves top-1 and top-5 test set error rates of 37.5% and 17.0%5. The best performance achieved during the ILSVRC- 2010 competition was 47.1% and 28.2% with an approach that averages the predictions produced from six sparse-coding models trained on different features , and since then the best pub- lished results are 45.7% and 25.7% with an approach that averages the predictions of two classi- fiers trained on Fisher Vectors (FVs) computed from two types of densely-sampled features . Model Top-1 Top-5 We also entered our model in the ILSVRC-2012 com- petition and report our results in Table 2. Since the Sparsecoding 47.1% 28.2% ILSVRC-2012 test set labels are not publicly available, SIFT+FVs 45.7% 25.7% we cannot report test error rates for all the models that CNN 37.5% 17.0% we tried. In the remainder of this paragraph, we use validation and test error rates interchangeably because Table1: ComparisonofresultsonILSVRC- inourexperiencetheydonotdifferbymorethan0.1% 2010 test set. In italics are best results (seeTable2). TheCNNdescribedinthispaperachieves achievedbyothers. a top-5 error rate of 18.2%. Averaging the predictions of five similar CNNs gives an error rate of 16.4%. Training one CNN, with an extra sixth con- volutional layer over the last pooling layer, to classify the entire ImageNet Fall 2011 release (15M images, 22K categories), and then “fine-tuning” it on ILSVRC-2012 gives an error rate of 16.6%. Averaging the predictions of two CNNs that were pre-trained on the entire Fall 2011 re- lease with the aforementioned five CNNs gives an error rate of 15.3%. The second-best con- test entry achieved an error rate of 26.2% with an approach that averages the predictions of sev- eral classifiers trained on FVs computed from different types of densely-sampled features . Finally, we also report our error rates on the Fall 2009 version of Model Top-1(val) Top-5(val) Top-5(test) ImageNetwith10,184categories SIFT+FVs — — 26.2% and 8.9 million images. On this 1CNN 40.7% 18.2% — datasetwefollowtheconvention 5CNNs 38.1% 16.4% 16.4% in the literature of using half of 1CNN* 39.0% 16.6% — the images for training and half 7CNNs* 36.7% 15.4% 15.3% for testing. Since there is no es- tablishedtestset,oursplitneces- Table 2: ComparisonoferrorratesonILSVRC-2012validationand sarilydiffersfromthesplitsused testsets. Initalicsarebestresultsachievedbyothers. Modelswithan bypreviousauthors,butthisdoes asterisk*were“pre-trained”toclassifytheentireImageNet2011Fall notaffecttheresultsappreciably. release.SeeSection6fordetails. Our top-1 and top-5 error rates on this dataset are 67.4% and 40.9%,attainedbythenetdescribedabovebutwithanadditional,sixthconvolutionallayeroverthe lastpoolinglayer. Thebestpublishedresultsonthisdatasetare78.1%and60.9%. 6.1 QualitativeEvaluations Figure3showstheconvolutionalkernelslearnedbythenetwork’stwodata-connectedlayers. The networkhaslearnedavarietyoffrequency-andorientation-selectivekernels,aswellasvariouscol- oredblobs. NoticethespecializationexhibitedbythetwoGPUs,aresultoftherestrictedconnec- tivitydescribedinSection3.5. ThekernelsonGPU1arelargelycolor-agnostic,whilethekernels ononGPU2arelargelycolor-specific. Thiskindofspecializationoccursduringeveryrunandis independentofanyparticularrandomweightinitialization(moduloarenumberingoftheGPUs). 5TheerrorrateswithoutaveragingpredictionsovertenpatchesasdescribedinSection4.1are39.0%and 18.3%. 7

Figure4: (Left)EightILSVRC-2010testimagesandthefivelabelsconsideredmostprobablebyourmodel. Thecorrectlabeliswrittenundereachimage,andtheprobabilityassignedtothecorrectlabelisalsoshown witharedbar(ifithappenstobeinthetop5).(Right)FiveILSVRC-2010testimagesinthefirstcolumn.The remainingcolumnsshowthesixtrainingimagesthatproducefeaturevectorsinthelasthiddenlayerwiththe smallestEuclideandistancefromthefeaturevectorforthetestimage. IntheleftpanelofFigure4wequalitativelyassesswhatthenetworkhaslearnedbycomputingits top-5 predictions on eight test images. Notice that even off-center objects, such as the mite in the top-left, can be recognized by the net. Most of the top-5 labels appear reasonable. For example, onlyothertypesofcatareconsideredplausiblelabelsfortheleopard. Insomecases(grille,cherry) thereisgenuineambiguityabouttheintendedfocusofthephotograph. Anotherwaytoprobethenetwork’svisualknowledgeistoconsiderthefeatureactivationsinduced by an image at the last, 4096-dimensional hidden layer. If two images produce feature activation vectors with a small Euclidean separation, we can say that the higher levels of the neural network considerthemtobesimilar. Figure4showsfiveimagesfromthetestsetandthesiximagesfrom thetrainingsetthataremostsimilartoeachofthemaccordingtothismeasure. Noticethatatthe pixellevel,theretrievedtrainingimagesaregenerallynotcloseinL2tothequeryimagesinthefirst column. Forexample,theretrieveddogsandelephantsappearinavarietyofposes. Wepresentthe resultsformanymoretestimagesinthesupplementarymaterial. ComputingsimilaritybyusingEuclideandistancebetweentwo4096-dimensional,real-valuedvec- torsisinefficient,butitcouldbemadeefficientbytraininganauto-encodertocompressthesevectors toshortbinarycodes.Thisshouldproduceamuchbetterimageretrievalmethodthanapplyingauto- encoderstotherawpixels,whichdoesnotmakeuseofimagelabelsandhencehasatendency toretrieveimageswithsimilarpatternsofedges,whetherornottheyaresemanticallysimilar. 7 Discussion Our results show that a large, deep convolutional neural network is capable of achieving record- breaking results on a highly challenging dataset using purely supervised learning. It is notable that our network’s performance degrades if a single convolutional layer is removed. For example, removing any of the middle layers results in a loss of about 2% for the top-1 performance of the network. Sothedepthreallyisimportantforachievingourresults. Tosimplifyourexperiments,wedidnotuseanyunsupervisedpre-trainingeventhoughweexpect that it will help, especially if we obtain enough computational power to significantly increase the sizeofthenetworkwithoutobtainingacorrespondingincreaseintheamountoflabeleddata. Thus far,ourresultshaveimprovedaswehavemadeournetworklargerandtraineditlongerbutwestill havemanyordersofmagnitudetogoinordertomatchtheinfero-temporalpathwayofthehuman visual system. Ultimately we would like to use very large and deep convolutional nets on video sequenceswherethetemporalstructureprovidesveryhelpfulinformationthatismissingorfarless obviousinstaticimages. 8

References  R.M.BellandY.Koren.Lessonsfromthenetflixprizechallenge.ACMSIGKDDExplorationsNewsletter, 9(2):75–79,2007.  A. Berg, J. Deng, and L. Fei-Fei. Large scale visual recognition challenge 2010. www.image- net.org/challenges. 2010.  L.Breiman. Randomforests. Machinelearning,45(1):5–32,2001.  D.Cires¸an,U.Meier,andJ.Schmidhuber. Multi-columndeepneuralnetworksforimageclassification. ArxivpreprintarXiv:1202.2745,2012.  D.C. Cires¸an, U. Meier, J. Masci, L.M. Gambardella, and J. Schmidhuber. High-performance neural networksforvisualobjectclassification. ArxivpreprintarXiv:1102.0183,2011.  J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical ImageDatabase. InCVPR09,2009.  J. Deng, A. Berg, S. Satheesh, H. Su, A. Khosla, and L. Fei-Fei. ILSVRC-2012, 2012. URL http://www.image-net.org/challenges/LSVRC/2012/.  L.Fei-Fei,R.Fergus,andP.Perona. Learninggenerativevisualmodelsfromfewtrainingexamples:An incrementalbayesianapproachtestedon101objectcategories. ComputerVisionandImageUnderstand- ing,106(1):59–70,2007.  G.Griffin,A.Holub,andP.Perona. Caltech-256objectcategorydataset. TechnicalReport7694,Cali- forniaInstituteofTechnology,2007. URLhttp://authors.library.caltech.edu/7694.  G.E.Hinton,N.Srivastava,A.Krizhevsky,I.Sutskever,andR.R.Salakhutdinov. Improvingneuralnet- worksbypreventingco-adaptationoffeaturedetectors. arXivpreprintarXiv:1207.0580,2012.  K.Jarrett,K.Kavukcuoglu,M.A.Ranzato,andY.LeCun. Whatisthebestmulti-stagearchitecturefor objectrecognition? InInternationalConferenceonComputerVision,pages2146–2153.IEEE,2009.  A.Krizhevsky. Learningmultiplelayersoffeaturesfromtinyimages. Master’sthesis, Departmentof ComputerScience,UniversityofToronto,2009.  A.Krizhevsky. Convolutionaldeepbeliefnetworksoncifar-10. Unpublishedmanuscript,2010.  A. Krizhevsky and G.E. Hinton. Using very deep autoencoders for content-based image retrieval. In ESANN,2011.  Y.LeCun,B.Boser,J.S.Denker,D.Henderson,R.E.Howard,W.Hubbard,L.D.Jackel,etal. Hand- writtendigitrecognitionwithaback-propagationnetwork.InAdvancesinneuralinformationprocessing systems,1990.  Y.LeCun,F.J.Huang,andL.Bottou. Learningmethodsforgenericobjectrecognitionwithinvarianceto poseandlighting. InComputerVisionandPatternRecognition,2004.CVPR2004.Proceedingsofthe 2004IEEEComputerSocietyConferenceon,volume2,pagesII–97.IEEE,2004.  Y. LeCun, K. Kavukcuoglu, and C. Farabet. Convolutional networks and applications in vision. In CircuitsandSystems(ISCAS),Proceedingsof2010IEEEInternationalSymposiumon,pages253–256. IEEE,2010.  H.Lee,R.Grosse,R.Ranganath,andA.Y.Ng. Convolutionaldeepbeliefnetworksforscalableunsuper- visedlearningofhierarchicalrepresentations.InProceedingsofthe26thAnnualInternationalConference onMachineLearning,pages609–616.ACM,2009.  T.Mensink,J.Verbeek,F.Perronnin,andG.Csurka. MetricLearningforLargeScaleImageClassifi- cation: GeneralizingtoNewClassesatNear-ZeroCost. InECCV-EuropeanConferenceonComputer Vision,Florence,Italy,October2012.  V.NairandG.E.Hinton. Rectifiedlinearunitsimproverestrictedboltzmannmachines. InProc.27th InternationalConferenceonMachineLearning,2010.  N.Pinto,D.D.Cox,andJ.J.DiCarlo. Whyisreal-worldvisualobjectrecognitionhard? PLoScomputa- tionalbiology,4(1):e27,2008.  N.Pinto,D.Doukhan,J.J.DiCarlo,andD.D.Cox. Ahigh-throughputscreeningapproachtodiscovering goodformsofbiologicallyinspiredvisualrepresentation. PLoScomputationalbiology,5(11):e1000579, 2009.  B.C.Russell,A.Torralba,K.P.Murphy,andW.T.Freeman. Labelme:adatabaseandweb-basedtoolfor imageannotation. Internationaljournalofcomputervision,77(1):157–173,2008.  J.SánchezandF.Perronnin.High-dimensionalsignaturecompressionforlarge-scaleimageclassification. InComputerVisionandPatternRecognition(CVPR),2011IEEEConferenceon,pages1665–1672.IEEE, 2011.  P.Y.Simard, D.Steinkraus, andJ.C.Platt. Bestpracticesforconvolutionalneuralnetworksappliedto visualdocumentanalysis.InProceedingsoftheSeventhInternationalConferenceonDocumentAnalysis andRecognition,volume2,pages958–962,2003.  S.C.Turaga,J.F.Murray,V.Jain,F.Roth,M.Helmstaedter,K.Briggman,W.Denk,andH.S.Seung.Con- volutionalnetworkscanlearntogenerateaffinitygraphsforimagesegmentation. NeuralComputation, 22(2):511–538,2010. 9