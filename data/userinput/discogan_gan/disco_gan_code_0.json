[
   {
      "page_content": "kernel_sizes = [4,3,3]\nstrides = [2,2,1]\npaddings=[0,0,1]\nlatent_dim = 300",
      "metadata": {
         "section_header": "Global Variables",
         "type": "python global"
      }
   },
   {
      "page_content": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport ipdb\nimport numpy as np\nclass Discriminator(nn.Module):\n\tdef __init__(\n\t\t\tself,\n\t\t\t):\n\t\tsuper(Discriminator, self).__init__()\n\t\tself.conv1 = nn.Conv2d(3, 64, 4, 2, 1, bias=False)\n\t\tself.relu1 = nn.LeakyReLU(0.2, inplace=True)\n\t\tself.conv2 = nn.Conv2d(64, 64 * 2, 4, 2, 1, bias=False)\n\t\tself.bn2 = nn.BatchNorm2d(64 * 2)\n\t\tself.relu2 = nn.LeakyReLU(0.2, inplace=True)\n\t\tself.conv3 = nn.Conv2d(64 * 2, 64 * 4, 4, 2, 1, bias=False)\n\t\tself.bn3 = nn.BatchNorm2d(64 * 4)\n\t\tself.relu3 = nn.LeakyReLU(0.2, inplace=True)\n\t\tself.conv4 = nn.Conv2d(64 * 4, 64 * 8, 4, 2, 1, bias=False)\n\t\tself.bn4 = nn.BatchNorm2d(64 * 8)\n\t\tself.relu4 = nn.LeakyReLU(0.2, inplace=True)\n\t\tself.conv5 = nn.Conv2d(64 * 8, 1, 4, 1, 0, bias=False)\n\tdef forward(self, input):\n\t\tconv1 = self.conv1( input )\n\t\trelu1 = self.relu1( conv1 )\n\t\tconv2 = self.conv2( relu1 )\n\t\tbn2 = self.bn2( conv2 )\n\t\trelu2 = self.relu2( bn2 )\n\t\tconv3 = self.conv3( relu2 )\n\t\tbn3 = self.bn3( conv3 )\n\t\trelu3 = self.relu3( bn3 )\n\t\tconv4 = self.conv4( relu3 )\n\t\tbn4 = self.bn4( conv4 )\n\t\trelu4 = self.relu4( bn4 )\n\t\tconv5 = self.conv5( relu4 )\n\t\treturn torch.sigmoid( conv5 ), [relu2, relu3, relu4]\nclass Generator(nn.Module):\n\tdef __init__(\n\t\t\tself,\n\t\t\textra_layers=False\n\t\t\t):\n\t\tsuper(Generator, self).__init__()\n\t\tif extra_layers == True:\n\t\t\tself.main = nn.Sequential(\n\t\t\t\tnn.Conv2d(3, 64, 4, 2, 1, bias=False),\n\t\t\t\tnn.LeakyReLU(0.2, inplace=True),\n\t\t\t\tnn.Conv2d(64, 64 * 2, 4, 2, 1, bias=False),\n\t\t\t\tnn.BatchNorm2d(64 * 2),\n\t\t\t\tnn.LeakyReLU(0.2, inplace=True),\n\t\t\t\tnn.Conv2d(64 * 2, 64 * 4, 4, 2, 1, bias=False),\n\t\t\t\tnn.BatchNorm2d(64 * 4),\n\t\t\t\tnn.LeakyReLU(0.2, inplace=True),\n\t\t\t\tnn.Conv2d(64 * 4, 64 * 8, 4, 2, 1, bias=False),\n\t\t\t\tnn.BatchNorm2d(64 * 8),\n\t\t\t\tnn.LeakyReLU(0.2, inplace=True),\n\t\t\t\tnn.Conv2d(64 * 8, 100, 4, 1, 0, bias=False),\n\t\t\t\tnn.BatchNorm2d(100),\n\t\t\t\tnn.LeakyReLU(0.2, inplace=True),\n\t\t\t\tnn.ConvTranspose2d(100, 64 * 8, 4, 1, 0, bias=False),\n\t\t\t\tnn.BatchNorm2d(64 * 8),\n\t\t\t\tnn.ReLU(True),\n\t\t\t\tnn.ConvTranspose2d(64 * 8, 64 * 4, 4, 2, 1, bias=False),\n\t\t\t\tnn.BatchNorm2d(64 * 4),\n\t\t\t\tnn.ReLU(True),\n\t\t\t\tnn.ConvTranspose2d(64 * 4, 64 * 2, 4, 2, 1, bias=False),\n\t\t\t\tnn.BatchNorm2d(64 * 2),\n\t\t\t\tnn.ReLU(True),\n\t\t\t\tnn.ConvTranspose2d(64 * 2,\t 64, 4, 2, 1, bias=False),\n\t\t\t\tnn.BatchNorm2d(64),\n\t\t\t\tnn.ReLU(True),\n\t\t\t\tnn.ConvTranspose2d(\t64,\t  3, 4, 2, 1, bias=False),\n\t\t\t\tnn.Sigmoid()\n\t\t\t)\n\t\tif extra_layers == False:\n\t\t\tself.main = nn.Sequential(\n\t\t\t\tnn.Conv2d(3, 64, 4, 2, 1, bias=False),\n\t\t\t\tnn.LeakyReLU(0.2, inplace=True),\n\t\t\t\tnn.Conv2d(64, 64 * 2, 4, 2, 1, bias=False),\n\t\t\t\tnn.BatchNorm2d(64 * 2),\n\t\t\t\tnn.LeakyReLU(0.2, inplace=True),\n\t\t\t\tnn.Conv2d(64 * 2, 64 * 4, 4, 2, 1, bias=False),\n\t\t\t\tnn.BatchNorm2d(64 * 4),\n\t\t\t\tnn.LeakyReLU(0.2, inplace=True),\n\t\t\t\tnn.Conv2d(64 * 4, 64 * 8, 4, 2, 1, bias=False),\n\t\t\t\tnn.BatchNorm2d(64 * 8),\n\t\t\t\tnn.LeakyReLU(0.2, inplace=True),\n\t\t\t\tnn.ConvTranspose2d(64 * 8, 64 * 4, 4, 2, 1, bias=False),\n\t\t\t\tnn.BatchNorm2d(64 * 4),\n\t\t\t\tnn.ReLU(True),\n\t\t\t\tnn.ConvTranspose2d(64 * 4, 64 * 2, 4, 2, 1, bias=False),\n\t\t\t\tnn.BatchNorm2d(64 * 2),\n\t\t\t\tnn.ReLU(True),\n\t\t\t\tnn.ConvTranspose2d(64 * 2,\t 64, 4, 2, 1, bias=False),\n\t\t\t\tnn.BatchNorm2d(64),\n\t\t\t\tnn.ReLU(True),\n\t\t\t\tnn.ConvTranspose2d(\t64,\t  3, 4, 2, 1, bias=False),\n\t\t\t\tnn.Sigmoid()\n\t\t\t)\n\tdef forward(self, input):\n\t\treturn self.main( input )",
      "metadata": {
         "section_header": "Global Other",
         "type": "python global"
      }
   },
   {
      "page_content": "Functions: __init__, forward",
      "metadata": {
         "section_header": "Discriminator",
         "type": "python class"
      }
   },
   {
      "page_content": "\tdef __init__(\n\t\t\tself,\n\t\t\t):\n\t\tsuper(Discriminator, self).__init__()\n\t\tself.conv1 = nn.Conv2d(3, 64, 4, 2, 1, bias=False)\n\t\tself.relu1 = nn.LeakyReLU(0.2, inplace=True)\n\t\tself.conv2 = nn.Conv2d(64, 64 * 2, 4, 2, 1, bias=False)\n\t\tself.bn2 = nn.BatchNorm2d(64 * 2)\n\t\tself.relu2 = nn.LeakyReLU(0.2, inplace=True)\n\t\tself.conv3 = nn.Conv2d(64 * 2, 64 * 4, 4, 2, 1, bias=False)\n\t\tself.bn3 = nn.BatchNorm2d(64 * 4)\n\t\tself.relu3 = nn.LeakyReLU(0.2, inplace=True)\n\t\tself.conv4 = nn.Conv2d(64 * 4, 64 * 8, 4, 2, 1, bias=False)\n\t\tself.bn4 = nn.BatchNorm2d(64 * 8)\n\t\tself.relu4 = nn.LeakyReLU(0.2, inplace=True)\n\t\tself.conv5 = nn.Conv2d(64 * 8, 1, 4, 1, 0, bias=False)",
      "metadata": {
         "section_header": "__init__",
         "type": "python function"
      }
   },
   {
      "page_content": "\tdef forward(self, input):\n\t\tconv1 = self.conv1( input )\n\t\trelu1 = self.relu1( conv1 )\n\t\tconv2 = self.conv2( relu1 )\n\t\tbn2 = self.bn2( conv2 )\n\t\trelu2 = self.relu2( bn2 )\n\t\tconv3 = self.conv3( relu2 )\n\t\tbn3 = self.bn3( conv3 )\n\t\trelu3 = self.relu3( bn3 )\n\t\tconv4 = self.conv4( relu3 )\n\t\tbn4 = self.bn4( conv4 )\n\t\trelu4 = self.relu4( bn4 )\n\t\tconv5 = self.conv5( relu4 )\n\t\treturn torch.sigmoid( conv5 ), [relu2, relu3, relu4]",
      "metadata": {
         "section_header": "forward",
         "type": "python function"
      }
   },
   {
      "page_content": "Functions: __init__, forward",
      "metadata": {
         "section_header": "Generator",
         "type": "python class"
      }
   },
   {
      "page_content": "\tdef __init__(\n\t\t\tself,\n\t\t\textra_layers=False\n\t\t\t):\n\t\tsuper(Generator, self).__init__()\n\t\tif extra_layers == True:\n\t\t\tself.main = nn.Sequential(\n\t\t\t\tnn.Conv2d(3, 64, 4, 2, 1, bias=False),\n\t\t\t\tnn.LeakyReLU(0.2, inplace=True),\n\t\t\t\tnn.Conv2d(64, 64 * 2, 4, 2, 1, bias=False),\n\t\t\t\tnn.BatchNorm2d(64 * 2),\n\t\t\t\tnn.LeakyReLU(0.2, inplace=True),\n\t\t\t\tnn.Conv2d(64 * 2, 64 * 4, 4, 2, 1, bias=False),\n\t\t\t\tnn.BatchNorm2d(64 * 4),\n\t\t\t\tnn.LeakyReLU(0.2, inplace=True),\n\t\t\t\tnn.Conv2d(64 * 4, 64 * 8, 4, 2, 1, bias=False),\n\t\t\t\tnn.BatchNorm2d(64 * 8),\n\t\t\t\tnn.LeakyReLU(0.2, inplace=True),\n\t\t\t\tnn.Conv2d(64 * 8, 100, 4, 1, 0, bias=False),\n\t\t\t\tnn.BatchNorm2d(100),\n\t\t\t\tnn.LeakyReLU(0.2, inplace=True),\n\t\t\t\tnn.ConvTranspose2d(100, 64 * 8, 4, 1, 0, bias=False),\n\t\t\t\tnn.BatchNorm2d(64 * 8),\n\t\t\t\tnn.ReLU(True),\n\t\t\t\tnn.ConvTranspose2d(64 * 8, 64 * 4, 4, 2, 1, bias=False),\n\t\t\t\tnn.BatchNorm2d(64 * 4),\n\t\t\t\tnn.ReLU(True),\n\t\t\t\tnn.ConvTranspose2d(64 * 4, 64 * 2, 4, 2, 1, bias=False),\n\t\t\t\tnn.BatchNorm2d(64 * 2),\n\t\t\t\tnn.ReLU(True),\n\t\t\t\tnn.ConvTranspose2d(64 * 2,\t 64, 4, 2, 1, bias=False),\n\t\t\t\tnn.BatchNorm2d(64),\n\t\t\t\tnn.ReLU(True),\n\t\t\t\tnn.ConvTranspose2d(\t64,\t  3, 4, 2, 1, bias=False),\n\t\t\t\tnn.Sigmoid()\n\t\t\t)\n\t\tif extra_layers == False:\n\t\t\tself.main = nn.Sequential(\n\t\t\t\tnn.Conv2d(3, 64, 4, 2, 1, bias=False),\n\t\t\t\tnn.LeakyReLU(0.2, inplace=True),\n\t\t\t\tnn.Conv2d(64, 64 * 2, 4, 2, 1, bias=False),\n\t\t\t\tnn.BatchNorm2d(64 * 2),\n\t\t\t\tnn.LeakyReLU(0.2, inplace=True),\n\t\t\t\tnn.Conv2d(64 * 2, 64 * 4, 4, 2, 1, bias=False),\n\t\t\t\tnn.BatchNorm2d(64 * 4),\n\t\t\t\tnn.LeakyReLU(0.2, inplace=True),\n\t\t\t\tnn.Conv2d(64 * 4, 64 * 8, 4, 2, 1, bias=False),\n\t\t\t\tnn.BatchNorm2d(64 * 8),\n\t\t\t\tnn.LeakyReLU(0.2, inplace=True),\n\t\t\t\tnn.ConvTranspose2d(64 * 8, 64 * 4, 4, 2, 1, bias=False),\n\t\t\t\tnn.BatchNorm2d(64 * 4),\n\t\t\t\tnn.ReLU(True),\n\t\t\t\tnn.ConvTranspose2d(64 * 4, 64 * 2, 4, 2, 1, bias=False),\n\t\t\t\tnn.BatchNorm2d(64 * 2),\n\t\t\t\tnn.ReLU(True),\n\t\t\t\tnn.ConvTranspose2d(64 * 2,\t 64, 4, 2, 1, bias=False),\n\t\t\t\tnn.BatchNorm2d(64),\n\t\t\t\tnn.ReLU(True),\n\t\t\t\tnn.ConvTranspose2d(\t64,\t  3, 4, 2, 1, bias=False),\n\t\t\t\tnn.Sigmoid()\n\t\t\t)",
      "metadata": {
         "section_header": "__init__",
         "type": "python function"
      }
   },
   {
      "page_content": "\tdef forward(self, input):\n\t\treturn self.main( input )",
      "metadata": {
         "section_header": "forward",
         "type": "python function"
      }
   }
]