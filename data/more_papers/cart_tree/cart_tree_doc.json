[
    {
        "page_content": "## Abstract\n\nDecision trees with binary splits are popularly constructed using Classification and Regression Trees (CART) methodology. For binary classification and regression models, this approach recursively divides the data into two near-homogenous daughter nodes according to a split point that maximizes the reduction in sum of squares error (the impurity) along a particular variable. This paper aims to study the bias and adaptive properties of regression trees constructed with CART. In doing so, we derive an interesting connection between the bias and the mean decrease in impurity (MDI) measure of variable importance-a tool widely used for model interpretability-defined as the sum of impurity reductions over all non-terminal nodes in the tree. In particular, we show that the probability content of a terminal subnode for a variable is small when the MDI for that variable is large and that this relationship is exponential-confirming theoretically that decision trees with CART have small bias and are adaptive to signal strength and direction. Finally, we apply these individual tree bounds to tree ensembles and show consistency of Breiman's random forests. The context is surprisingly general and applies to a wide variety of multivariable data generating distributions and regression functions. The main technical tool is an exact characterization of the conditional probability content of the daughter nodes arising from an optimal split, in terms of the partial dependence function and reduction in impurity.\n\nIndex terms Decision tree, regression tree, recursive partition, CART, random forest, boosting, nonparametric regression, high-dimensional statistics\n\n∗ This research was supported in part by NSF Grant DMS-1915932.",
        "metadata": {
            "section_header": "Abstract",
            "title": "Analyzing CART",
            "type": "paper"
        }
    },
    {
        "page_content": "## 1 Introduction\n\nDecision trees are the building blocks of some of the most important and powerful algorithms in statistical learning. For example, ensembles of decision trees are used for some bootstrap aggregated prediction rules (e.g., random forests [10]). In addition, at each iteration of gradient tree boosting (e.g., TreeBoost [18]), the pseudo-residuals are fit with decision trees as base learners. From an applied perspective, decision trees have an appealing interpretability and are accompanied by a rich set of analytic and visual diagnostic tools. These attributes make tree-based learning particularly well-suited for applied sciences and related disciplines-which may rely heavily on understanding and interpreting output from a black-box model and the system that generated the data.\n\nAlthough, as with many aspects of statistical learning, good empirical performance often comes at the expense of rigor and transparency. 1 Tree-structured learning with decision trees is no exception-statistical guarantees for popular variants, i.e., those that are actually used in practice, are hard to find. The complicated recursive way in which decision trees are constructed makes them unamenable to analysis. While a complete and thorough picture of decision trees and their role in ensemble learning may be far away or even unattainable, in this paper, we take a significant step forward and aim to tackle the following three questions.\n\n- · Why do decision trees have small bias?\n- · Why are decision trees locally adaptive to signal strength?\n- · Can one connect bias and adaptivity with quantities used to assess variable importance?\n\nThe first two questions above are supported by an abundance of empirical evidence [12], yet remain to be explained or answered by a sensible mathematical theory, especially when the tree construction involves the input and output data. For example, decision trees are known to have small bias when they are grown deeply-a defining characteristic of random forests. But how and why? Moreover, tree-based learning is known to be particularly effective in high-dimensional sparse settings when the distribution of the output depends only on a few predictor variables. What mechanism of trees enables this?\n\nLet us emphasize that we are not content with merely a study of the standard certificates for good predictors (e.g., consistency or rates of convergence). Rather, we aim to identity and explore the unique advantages of tree-structured learning and, in doing so, develop a unifying theory that connects bias and adaptivity via two well-known data-analytic tools for model interpretabilitypartial dependence functions and variable importance measures .\n\nTo make our work informative to the applied user of decision trees, we strive to make the least departure from practice and therefore focus specifically on Classification and Regression Tree (CART) [12] methodology-by far the most popular for regression and classifica-\n\n1 For a delightful read and intriguing perspective on this trade-off, see [9].\n\ntion problems. With this methodology, the tree construction depends on both the input and output data and is therefore data-dependent . This aspect lends itself favorably to the empirical performance of CART, but poses unique mathematical challenges. As far as we know, the only other work that studies the bias of CART is [38], who used it to show asymptotic consistency of Breiman's random forests for additive regression models. One goal of the present paper is to extend this theory to other response surfaces.\n\nBecause individual decision trees are unstable and subject to large sampling variability 2 , we do not concern ourselves with a study of their variance. Indeed, such an endeavor is worthwhile only in the presence of some sort of variance reduction technique like pruning (i.e., removing portions of the tree in order to reduce its complexity) or ensemble averaging (e.g., bagging [8], random forests, boosting), and even so, the bias remains the most challenging aspect of the analysis. This is, of course, not to diminish variance as an essential component of a theoretical investigation. For instance, in the sequel, we will apply our individual tree bias bounds to Breiman's random forests (which use ensembles of trees) in conjunction with existing results for its variance.\n\nLet us now describe the statistical setting and framework that we will operate under for the rest of the paper. For clarity and ease of exposition, we focus specifically on regression trees , where the target outcome is a continuous real value. Although, many of our results hold for binary classification trees as well.\n\n↦\n\nWe assume the learning (training) data is D n = ( { X 1 , Y 1 ) , . . . , ( X n , Y n ) } , where ( X i , Y i ) , 1 ≤ i ≤ n are i.i.d. with common joint distribution P X ,Y and joint density p X ,Y with respect to Lebesgue measure (with marginal distribution and density, P X and p X , defined analogously). Here, X i ∈ [0 , 1] d is the input (feature, covariate, or predictor vector) and Y i ∈ R is a continuous outcome (response or output variable). A generic pair of variables will be denoted as ( X , Y ) . Ageneric coordinate of X will be denoted by X , unless there is a need to highlight the dependence on the coordinate index, say X j , where j = 1 2 , , . . . , d . We will use the terms feature, predictor, or input variable to refer to X interchangeably. The statistical model is Y i = f ( X i ) + ε i , for i = 1 , . . . , n , where f ( x ) = E [ Y | X = x ] is an unknown regression function and { ε i } 1 ≤ ≤ i n are i.i.d. errors. The conditional average of Y given X = x is optimal for prediction if one uses squared error loss L ( X , Y, f ˜ ) = ( Y -˜ f ( X )) 2 since it minimizes the conditional risk ˜ f → E [ ( Y -˜ f ( X )) 2 | X = x ] = ∫ ( y -˜ f ( x )) 2 P Y | X x = ( dy ) . The bias of a prediction rule ̂ Y ( x ) = ̂ Y ( x ; D n ) at a point X = x is henceforth defined to be E [ Y | X = x ] -E D n [ ̂ Y ( x )] .\n\nOne key strength of decision trees is that they can exploit, if present, low local dimensionality of the response surface. This is particularly useful since many real-world input/output systems admit or are approximated well by local sparse representations of simple model forms, e.g., wavelets and neural networks. That is, even though the input/output relationship is determined by a large number of variables overall, the dependence may be locally\n\n2 This is particularly the case for classification trees.\n\ncharacterized by only a small subset of variables. Such local adaptivity is made possible by the recursive partitioning of the input space, in which optimal splits are increasingly affected by local qualities of the data as the tree is grown. Hence, variables that locally have more influence on determining the response are much more likely to be included as candidates for further splitting. In essence, decision trees have a built-in local variable subset selection mechanism, which allows them to overcome many of the undesirable consequences (e.g., overfitting and large sample requirements) of high-dimensional modeling.\n\nIn this paper, we shall work with a more simplistic, yet still exemplary model and assume that the conditional mean response f ( x ) = E [ Y | X = x ] depends only on a small, unknown subset S of the d features. In other words, f is almost surely equal to its restriction f | [0 1] , S to the subspace [0 , 1] S of its 'strong' variables x S = ( x j : j ∈ S ) , where S =",
        "metadata": {
            "section_header": "Introduction",
            "title": "Analyzing CART",
            "type": "paper"
        }
    },
    {
        "page_content": "# S glyph[lessmuch] d . Conversely, the output of f does not dependent on 'weak' variables that belong to S c . Of course, the set S is not known a priori and must be learned from the data.",
        "metadata": {
            "section_header": "S glyph[lessmuch] d . Conversely, the output of f does not dependent on 'weak' variables that belong to S c . Of course, the set S is not known a priori and must be learned from the data.",
            "title": "Analyzing CART",
            "type": "paper"
        }
    },
    {
        "page_content": "## 2 Organization\n\nThis paper is organized according to the following schema. In Section 3, we establish some basic notation and definitions that we will use throughout the paper. Section 4 reviews some of the terminology and quantities associated with growing regression trees. In Section 5, we review two important data-analytic quantities associated with tree-ensembles for visualization and interpretability. A summary of the main results is given in Section 6, including some accompanying examples and simulations studies. Section 7 discusses the main assumptions on the regression function that we use to obtain our bounds. In Section 8, we apply our bounds for individual decision trees from Section 6 to show consistency of Breiman's random forests. Section 9 contains some finite sample results. Finally, in Section 10, we briefly discuss how our results can be extended to binary classification. Proofs of the main statements from Section 6 are given in Section 11 and Appendix A contains proofs of some lemmas and examples from the body of the paper.",
        "metadata": {
            "section_header": "Organization",
            "title": "Analyzing CART",
            "type": "paper"
        }
    },
    {
        "page_content": "## 3 Notation and definitions\n\nglyph[negationslash]\n\nFor S ⊂ { 1 2 , , . . . , d } , let x S = ( x j : j ∈ S ) . With a slight abuse of notation, we write x \\ j = ( x j ′ : j ′ = j ) instead of x { 1 2 , ,...,d }\\{ } j for brevity. If A is a subset of R d , we let A S = { x S : x ∈ A } and A \\ j = { x \\ j : x ∈ A } . For two subsets A,B ⊂ R , dist ( A,B ) = inf a ∈ A, b ∈ B | a - | b . For S ⊂ { 1 2 , , . . . , d } and A ⊂ R d , we define diam S ( A ) = sup x x , ′ ∈ A ‖ x S -x ′ S ‖ , where ‖ x S ‖ 2 = ∑ j ∈S x 2 j .\n\nFor a function g : R d → R and subset A ⊂ R d , we define the oscillation of f on A by ω g A ( ; ) = sup x x , ′ ∈ A | g ( x ) -g ( x ′ ) | . The total variation of a function g : R → R\n\non an interval [ a, b ] is defined by TV ( g ; [ a, b ]) = sup P ∑ L glyph[lscript] =1 | g x ( glyph[lscript] ) -g x ( glyph[lscript] -1 ) | , where P = {P = { x , x 0 1 , . . . , x L } : P is a partition of [ a, b ] } . If g : R d → R , we will write ∂ r ∂x r j g ( x ) to denote the r th order partial derivative of g with respect to the j th variable at the point x . If g : R → R , we will write g ′ ( x ) to denote the first derivative of g at the point x . The r th order derivative of g at the point x is denoted by g ( r ) ( x ) .",
        "metadata": {
            "section_header": "Notation and definitions",
            "title": "Analyzing CART",
            "type": "paper"
        }
    },
    {
        "page_content": "## 4 Preliminaries\n\nAs mentioned earlier, regression trees are commonly constructed with Classification and Regression Tree (CART) [12] methodology. The primary objective of CART is to find partitions of the input variables that produce minimal variance of the response values (i.e., minimal sum of squares error with respect to the average response values). Because of the computational infeasibility of choosing the best overall partition, CART trees are greedily grown with a procedure in which binary splits recursively partition the tree into nearhomogeneous terminal nodes. That is, an effective binary split partitions the data from the parent tree node into two daughter nodes so that the resultant homogeneity of the daughter nodes, as measured through their impurity (within node sum of squares error), is improved from the homogeneity of the parent node. Under the least squares error criterion, it can easily be shown that if one desires to have constant output in the terminal nodes of the tree, then the constant to use in each terminal node should be the average of the response values within the terminal node [12, Proposition 8.10]. Hence, these models produce a histogram estimate of the regression surface. As such, they are often referred to as piecewise constant regression models, since the tree output is constant on each terminal node.\n\nLet us now describe the algorithm with additional precision. Consider splitting a regression tree T at a node t . Let s be a candidate split for a variable X that splits t into left and right daughter nodes t L and t R according to whether X ≤ s or X &gt; s . These two nodes will be denoted by t L = { X ∈ t : X ≤ } s and t R = { X ∈ t : X &gt; s } . As mentioned previously, a tree is grown by recursively reducing node impurity, which, for regression trees grown with CART, is determined by within node sample variance\n\n<!-- formula-not-decoded -->\n\nwhere Y t = 1 N ( t ) ∑ X i ∈ t Y i is the sample mean for t and N ( t ) =",
        "metadata": {
            "section_header": "Preliminaries",
            "title": "Analyzing CART",
            "type": "paper"
        }
    },
    {
        "page_content": "# { X i ∈ t } is the number of observations in t . Similarly, the within sample variance for a daughter node is\n\n<!-- formula-not-decoded -->\n\nwhere Y t L is the sample mean for t L and N ( t L ) is the sample size of t L (similar definitions apply to t R ). The parent node t is split into two daughter nodes using the variable and split\n\npoint producing the largest decrease in impurity (or impurity reduction). For a candidate split s for X , this decrease in impurity equals [12, Definition 8.13]\n\n<!-- formula-not-decoded -->\n\nwhere ̂ P ( t L ) = N ( t L ) /N ( t ) and ̂ P ( t R ) = N ( t R ) /N ( t ) are the proportions of observations in t that are contained in t L and t R , respectively. Note that maximizing ̂ ∆( ; s t ) is also equivalent to minimizing\n\n<!-- formula-not-decoded -->\n\nwhich means that CART seeks the split point that minimizes the weighted sample variance. Yet another way to view ̂ ∆( ; s t ) is via its equivalent representation ̂ ∆( )ˆ t ρ 2 , where ˆ ρ is the empirical correlation between Y and the decision stump ̂ Y = Y t L ✶ { X ≤ } s + Y t R ✶ { X&gt;s } within t . Hence, at each node, CART seeks the step function most correlated (in magnitude) with the response variable.\n\nTo reiterate, the tree T is grown recursively by finding the split point s that maximizes ̂ ∆( ; s t ) . The particular variable chosen is the one that gives the largest reduction in impurity over t . We denote an optimized split point by ˆ s (breaking ties arbitrarily) and the optimally split daughter nodes with ˆ s by ˆ t L and ˆ t R , i.e., ˆ t L = { X ∈ t : X ≤ } ˆ s and ˆ t R = { X ∈ t : X &gt; s ˆ } , respectively. The tree output at a terminal node t is ̂ Y = Y t .\n\nThe use of (2) to evaluate each candidate split involves several passes over the training data with the consequent computational costs when handling problems with a large number of predictor variables and observations. This is particularly serious in the present setting of continuous variables-the major computational bottleneck of growing trees. Fortunately, the use of the least squares error criterion with averages in the terminal nodes permits further simplifications of the formulas described above. That is, using the sum of squares decomposition, ̂ ∆( ; s t ) can equivalently be expressed as [12, Section 9.3]\n\n<!-- formula-not-decoded -->\n\nThis expression implies that one can find the best split for a continuous variable with just a single pass over the data, without the need to calculate multiple averages and sums of squared differences for these averages. In fact, not only does this representation have computational benefits-it will also be crucial in establishing our main results. It should be stressed that this alternative expression is unique to the least squares error criterion with averages in the terminal nodes of the tree.\n\nA direct analysis of regression trees using the finite sample splitting criterion ̂ ∆( ; · t ) is challenging and obfuscates some of the inner and subtle mechanisms that makes tree-based learning with CART desirable. Instead, to remain true to the original CART procedure while still being able theoretically study its dynamics, we work under an asymptotic data setting\n\nfor determining splits and therefore replace ̂ ∆( ; s t ) with its analog based on population parameters:\n\n<!-- formula-not-decoded -->\n\nwhere ∆( ) t is the conditional population variance ∆( ) = t Var [ Y | X ∈ t ] , ∆( t L ) and ∆( t L ) are the daughter conditional variances\n\n<!-- formula-not-decoded -->\n\nand P ( t L ) and P ( t R ) are the conditional probabilities (probability content) of the daughter nodes arising from the split s , i.e.,\n\n<!-- formula-not-decoded -->\n\nThese conditional probabilities can also be interpreted as the infinite sample proportion of observations in the parent node that are contained in the daughter nodes. Note that P ( t L ) is also the distribution function of X | X ∈ t . We will occasionally write P s ( | t ) instead of P ( t L ) to highlight dependence on the split s . The density function of X | X ∈ t , that is, ∂ ∂s P [ X ≤ s | X ∈ t ] , will be denoted by p ( t L ) or p s ( | t ) . The infinite sample analog of (4) is\n\n<!-- formula-not-decoded -->\n\nWe let t ∗ L = { X ∈ t : X ≤ s ∗ } and t ∗ R = { X ∈ t : X &gt; s ∗ } denote the optimally split daughter nodes with optimal split s ∗ , respectively. Note that any node t is a Cartesian product of intervals, which we call subnodes . The subnode of variable X within node t is denoted by [ a ( t ) , b ( t )] , where a ( t ) &lt; b ( t ) .\n\nUnder our framework, we optimize the infinite sample splitting criterion, namely, ∆( ; s t ) , instead of the empirical one (2). Optimizing the splitting protocol ∆( ; s t ) can also be viewed as querying an oracle for the optimal (population) split values. A maximizer of ∆( ; s t ) is denoted by s ∗ , i.e., s ∗ ∈ arg max s ∆( ; s t ) (again, breaking ties arbitrarily). Despite this tweak, we stress that only the splits are determined from an infinite sample quantity; all other aspects of the regression tree (e.g., terminal node values, depth, variables selected for candidate splits) are determined from the learning sample. More specifically, the regression tree still outputs ̂ Y = 1 N ( t ) ∑ X i ∈ t Y i , except that t is determined by an infinite sample (population) objective. If the number of observations within t is large and ∆( ; · t ) has a unique global maximum, then we can expect ˆ s ≈ s ∗ (via an empirical process argument) and hence our infinite sample setting is a good approximation to CART with empirical splits. Indeed, if s ∗ is unique, [3] and [13, Section 3.4.2] show cube root asymptotics (i.e., n 1 3 / (ˆ s -s ∗ ) converges in distribution) of split points for multi-level decision trees using the CART sum of squares error criterion. While these results show that ˆ s and s ∗ are close, they do not reveal what sort of partition of the input space is induced by a sequence of optimal splits-the goal of the present paper. Since this partition is constructed from the distribution of ( X , Y ) , we will need to impose conditions on the regression function and\n\ninput distribution so that the partition yields a predictor with small bias. It is our hope that the applied user of decision trees can benefit from knowing these limitations in the idealized setting of population-level splits.\n\nSince the recursive partition obtained from ̂ ∆( ; s t ) governs the bias of the tree, our study of the partitions created from the infinite sample version ∆( ; s t ) is in much the same vein as the study of kernel or nearest neighbors regression with (oracle) infinite sample, optimal parameters (e.g., kernel bandwidth or number of nearest neighbors), obtained by minimizing the asymptotic mean integrated squared error (AMISE).",
        "metadata": {
            "section_header": "X i ∈ t } is the number of observations in t . Similarly, the within sample variance for a daughter node is",
            "title": "Analyzing CART",
            "type": "paper"
        }
    },
    {
        "page_content": "## 4.1 A comment on notation\n\nUp until now, we have assumed that the split occurs along a generic coordinate X . However, in the multi-dimensional setting, there will be a need to specify the coordinate index. Therefore, we will write ∆( j, s ; t ) to denote ∆( ; s t ) when the coordinate being split is X j . For the same reason, we write P j ( t L ) and P j ( s | t ) in lieu of P ( t L ) and P s ( | t ) , respectively. Similar definitions will hold for the subnode [ a j ( t ) , b j ( t )] when we want to emphasize the dependence on the variable index. We write j t to denote the splitting variable chosen in node t , i.e., j t ∈ arg max j =1 2 , ,...,d ∆( j, s ∗ ; t ) , breaking ties arbitrarily. Finally, note that an optimal split s ∗ for a node t depends on t and therefore should technically be written as s ∗ t , however, we suppress this dependence for brevity and assume that it holds implicitly. We also use similar notation for the finite sample versions of these quantities. As a general rule, if the index j is omitted on any quantity, it should be understood that we are considering a generic variable X .\n\nWe now survey two popular data-analytic quantities that are also intimately connected to the forthcoming results in Section 6.",
        "metadata": {
            "section_header": "A comment on notation",
            "title": "Analyzing CART",
            "type": "paper"
        }
    },
    {
        "page_content": "## 5 Data-analytic quantities for interpretation and visualization\n\nPertinent to many tasks in data analysis is interpretability of the representation of the model. For black-box models such as random forests, this is usually performed through graphical renderings of the prediction surface as a function of one or two predictor variables and assessing the role each variable plays in determining the final output. Here we review two of the most popular quantities associated with decision tree ensembles for this purpose.",
        "metadata": {
            "section_header": "Data-analytic quantities for interpretation and visualization",
            "title": "Analyzing CART",
            "type": "paper"
        }
    },
    {
        "page_content": "## 5.1 Partial dependence functions\n\nOne useful device for visualizing the influence of a particular variable on the output is the so-called partial dependence function . 3 By looking simultaneously at the partial dependence plots -i.e., a trellis of plots of the partial dependence functions for each variableone can obtain a visual description of a multivariable predictor. In the same spirit, for assessing how each variable affects the output conditional on a node, we define the conditional partial dependence function given node t via\n\n<!-- formula-not-decoded -->\n\n↦\n\nwhere P X \\ j | X t ∈ , X j = x j ( d x \\ j ) is the conditional probability measure with conditional density x \\ j → p X ( x , j x \\ j ) / ∫ t \\ j p X ( x , j x ′ \\ j ) d x ′ \\ j . Note that F j ( x j ; t ) is the best least squares approximation to f ( X ) in t as a function of X j alone, i.e., F j ( x j ; t ) = arg min F E [ ( Y -F X ( j )) 2 | X ∈ t , X j = x j ] . For brevity, we write F ′ j ( x j ; t ) to denote the derivative ∂ ∂x j F j ( x j ; t ) , provided it exists. We also define the mean-centered conditional partial dependence function as\n\n<!-- formula-not-decoded -->\n\nthat is, G j ( x j ; t ) is F j ( x j ; t ) minus its mean value over the node.\n\n↦\n\nStrictly speaking, F j ( ; · t ) is not the same as the partial dependence function in the sense of [18, Section 8.2] or [24, Section 10.13.2], where, instead of ignoring the effects of X \\ j , one looks at the dependence of f on X j in t after averaging with respect to the marginal distribution of the other covariates X \\ j , i.e., F j ( x j ; t ) = ∫ f ( x , j x \\ j ) P X \\ j | X t ∈ ( d x \\ j ) , where P X \\ j | X t ∈ ( d x \\ j ) is the conditional probability measure with conditional density x \\ j → p X \\ j ( x \\ j ) / ∫ t \\ j p X \\ j ( x ′ \\ j ) d x ′ \\ j . However, when X j and X \\ j are independent (for example, as in the uniform case), the two definitions coincide with each other.\n\nOf course, in practice, one would integrate an estimate ̂ f of f against the empirical distribution, so that, for example,\n\n<!-- formula-not-decoded -->\n\nWhen the tree is fully grown so that each terminal node contains only a single observation, i.e., N ( t ) = 1 for all t , (8) is the Individual Conditional Expectation (ICE) [21].\n\nLet us finally mention that a desirable property of tree-based models (i.e., if ̂ f is a treestructured predictor) is that the partial dependence function (8) can be quickly computed\n\n3 Visualizing the effect of more than two predictor variables has limited descriptive value.\n\nfrom the tree itself, without passing over the data each time it is to be evaluated. This fact has implications for the rapid production of a trellis of plots for each variable.",
        "metadata": {
            "section_header": "Partial dependence functions",
            "title": "Analyzing CART",
            "type": "paper"
        }
    },
    {
        "page_content": "## 5.2 Variable importance measures and variable influence ranking\n\nAnother attractive feature of tree-based ensembles is that one can compute, essentially for free, various measures of variable importance (or influence) using the optimal split points and their corresponding impurities. These measures are used for sensitivity analysis and to rank the influence of each variable in determining the output, which in turn, can be used to identify and select the most relevant predictors for further investigation (such as plotting their partial dependence functions). For random forests, one canonical and widely used measure is the Mean Decrease in Impurity (MDI) [18, Section 8.1], [12, Section 5.3.4], [24, Sections 10.13.1 &amp; 15.3.2], defined as the weighted sum of largest impurity reductions (i.e., either Gini impurity for classification or variance impurity for regression) over all non-terminal nodes in the tree, averaged over all trees in the forest, i.e.,\n\n<!-- formula-not-decoded -->\n\nwhere\n\n<!-- formula-not-decoded -->\n\nand the sum extends over all non-terminal (internal) nodes t ′ such that j t ′ = j and ̂ P ( t ′ ) = N ( t ′ ) /n is the proportion of observations that land in node t ′ . 4\n\nThe next definition of variable importance generalizes and localizes MDI. Instead of weighting each ̂ ∆( j, s ˆ; t ′ ) by the proportion of observations in t ′ , more flexible weights are allowed. Moreover, rather than summing over all non-terminal nodes, we condition on a particular terminal node so that only nodes that are ancestors of the terminal node are considered.\n\nDefinition 1 (Conditional variable importance) . Let t be a terminal node and let j t ′ denote the index of the variable selected to split along at an ancestor node t ′ of t . The conditional variable importance of X j given t is defined by\n\n<!-- formula-not-decoded -->\n\n4 Another commonly used and possibly more accurate measure is Mean Decrease in Accuracy (MDA), defined as the average difference in out-of-bag error before and after randomly permuting the values of X j in out-of-bag samples over all trees. However, as with all permutation based methods, computational issues are present. Compare MDA with MDI, which can be computed as each tree is grown with no additional cost. Both MDI and MDA are calculated in the R package randomForest with randomForest(..., importance = TRUE) and in the Python library scikit-learn with attribute feature\\_importances\\_\n\nwhere the sum extends over all ancestor nodes t ′ of t such that j t ′ = j and the (possibly data-dependent) weights { ̂ w j, s ( ˆ; t ′ ) } are nonnegative. That is, MDI ̂ ( X j ; t ) is a weighted sum of largest impurity decreases ̂ ∆( j, s ˆ; t ′ ) among all ancestor nodes t ′ of terminal node t such that X j was selected for a split. The infinite sample version of MDI ̂ ( X j ; t ) , denoted by MDI ( X j ; t ) , is defined by\n\n<!-- formula-not-decoded -->\n\nwhere the weights { w j, s ( ∗ ; t ′ ) } are nonnegative.\n\nNote that MDI ̂ ( X j ) is a global measure of importance. Thus, MDI ̂ ( X j ) is relevant for answering questions such as 'what variables are most important in predicting blood pressure?' and MDI ̂ ( X j ; t ) is useful for answering questions like 'for subjects ages 65 and up, what variables are most important in predicting blood pressure?'. A few other variants of local or case-wise (permutation) importance have been proposed to cope with the bias of variable importance measures towards correlated predictor variables. See, for example, [41].",
        "metadata": {
            "section_header": "Variable importance measures and variable influence ranking",
            "title": "Analyzing CART",
            "type": "paper"
        }
    },
    {
        "page_content": "## 6 Summary of results\n\nThis section contains our main results. Due to space constraints, proofs of most of the statements are deferred until Section 11 and Appendix A.",
        "metadata": {
            "section_header": "Summary of results",
            "title": "Analyzing CART",
            "type": "paper"
        }
    },
    {
        "page_content": "## 6.1 Distributional assumptions\n\nBefore we continue, let us first state the main distributional assumptions on the predictor variables. Let us remark, however, that many of the subsequent statements and results (e.g., Section 8, Section 9, and Section 11) hold without them.\n\nAssumption 1. For each variable X j and node t , the distribution function P [ X j ≤ x j | X ∈ t ] is strictly increasing.\n\nAssumption 1 is quite mild and holds if the joint density p X never vanishes. 5\n\nThe next assumption is needed for Theorem 1, our main result.\n\nAssumption 2. For each variable X j , node t , and split s ,\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nfor some universal constant η ∈ (0 , 1] .\n\n5 This does not necessarily mean that the joint density is uniformly bounded below by a positive constant.\n\nAssumption 2 is more restrictive than Assumption 1, yet it still allows for some degree of dependency structures among the predictor variables. For example, it holds if the joint density function of the features is uniformly bounded above and below by constant multiples of the product of its marginal densities, i.e.,\n\n<!-- formula-not-decoded -->\n\nfor all x ∈ [0 , 1] d , where c 1 and c 2 are positive constants. In this case, η can be taken to be c 2 /c 1 .",
        "metadata": {
            "section_header": "Distributional assumptions",
            "title": "Analyzing CART",
            "type": "paper"
        }
    },
    {
        "page_content": "## 6.2 Probability content of terminal subnodes\n\nThe next theorem, our main result, gives a clean, interpretable bound on the P X -probability of a terminal subnode in terms of the variable importance measure, which in turn, controls the bias of the tree. Specifically, it shows that conditional terminal subnode probability content is (exponentially) small in the importance measure attributed to the splitting variable. This bound also corroborates with empirical evidence showing that, although decision trees are highly unstable, their bias tends to be small. For brevity, we furnish the proof in Section 11.\n\nTheorem 1. Suppose Assumption 1 and Assumption 2 hold and [ a j ( t ) , b j ( t )] is a subnode along the j th direction for a terminal node t = ∏ d j =1 [ a j ( t ) , b j ( t )] . Then,\n\n<!-- formula-not-decoded -->\n\nwhere MDI ( X j ; t ) = ∑ t ′ ⊃ t j t ′ = j w j, s ( ∗ ; t ′ )∆( j, s ∗ ; t ′ ) is the conditional variable importance (11) with weights given by\n\n<!-- formula-not-decoded -->\n\nFurthermore, if the first-order partial derivatives of the regression function and joint density function of X exist and are continuous, then\n\n<!-- formula-not-decoded -->\n\nThe main technical tools for proving Theorem 1-Theorem 7 and Theorem 9-characterize P [ X j ≤ s ∗ | X ∈ t ′ ] in terms of the partial dependence function F j ( s ∗ ; t ′ ) and reduction in impurity ∆( j, s ∗ ; t ′ ) at an optimal split. The form of the weights (15) are derived from the first-order conditions (i.e., ∂ ∂s ∆( j, s ; t ′ ) | s = s ∗ = 0 ) of s ∗ as a global maximizer of ∆( j, · ; t ′ ) , whereas the lower bound (16) incorporates both first- and second-order optimality conditions (i.e., ∂ ∂s ∆( j, s ; t ′ ) | s = s ∗ = 0 and ∂ 2 ∂s 2 ∆( j, s ; t ′ ) | s = s ∗ &lt; 0 ).\n\nRemark 1. If the regression function is linear and the input is uniformly distributed, then MDI ( X j ; t ) with weights (15) equals K j ( t ) , the number of times X j was selected among all ancestor nodes of terminal node t . 6\n\nRemark 2. Theorem 1 implies that diam S ( t ) converges to zero in P X -probability when MDI ( X j ; t ) → + ∞ for each j ∈ S . According to classic theory for partitioning-based prediction rules, shrinking terminal node diameters is a necessary condition for asymptotic consistency [40].\n\nIn light of (14), it is tantalizing to interpret MDI ( X j ; t ) (and therefore MDI ̂ ( X j ; T ) ) as truly a measure of variable importance, since it governs the bias of the regression tree along a specific direction, i.e., the conditional probability content of a terminal subnode for a variable is small when the MDI for that variable is large. 7\n\nAs a consequence of Theorem 1, we immediately see two important properties of the regression tree:\n\n- · Terminal subnodes with smaller P X -probability are along more 'important' directions as delineated by MDI, and thus adapt to signal strength in each direction. That is, the tree needs to be split more often in order to create finer partitions along directions that are more relevant to the output.\n- · The adaptation to the importance of the variable is local and depends on a particular terminal node of the tree. That is, each direction and location of the features requires a different level of granularity in the tree in order to detect and adapt to local changes in the regression surface. For example, regions of the input space where the response is 'flat' do not need to be split as often and therefore a crude partition will suffice. On the other hand, complex functional dependencies may require a higher degree of fineness in the final model.\n\nThese observations are consistent with [29, Section 4], in that terminal nodes are on average narrower in directions with strong signals than in directions with weak signals. This is a very appealing property from a statistical perspective-trees with terminal subnodes that have large (resp. small) probability content in directions with weak (resp. strong) signals are less likely to overfit (resp. underfit) the data.\n\nRemark 3. An analogy can be drawn between the terminal subnode probability content P X [ X j ∈ [ a , b j j ]] and the bandwidth size in kernel regression, both of which control the bias of the predictor. Bandwidth selection for kernel regression [48] is often performed using a plug-in estimator of the best bandwidth that minimizes the AMISE. There, just like with (16), the best theoretical bandwidth also depends on the density function of the features and the smoothness of the regression function (albeit, via curvature-\n\n6 Importance measures based on feature selection frequencies are implemented in standard R and Python packages for XGBoost [14].\n\n7 The original motivation for MDI was based solely on simple heuristic arguments and so one is left guessing as to why it seems to work. Overall, little is known about the theoretical properties of variable importance measures; see [26, 30, 28] for recent attempts in this direction.\n\n∫ ∂ 2 ∂x 2 j f ( x ) ∂ 2 ∂x 2 j ′ f ( x ) P X ( d x ) ) [48, Theorem 1] and [47, Section 5.8].\n\nRemark 4. As will be discussed in Section 10, the statement of Theorem 1 also holds verbatim for binary classification trees with appropriate modifications for alternate (i.e., Gini) splitting rules. An illustrative example involving logistic regression will also be provided.",
        "metadata": {
            "section_header": "Probability content of terminal subnodes",
            "title": "Analyzing CART",
            "type": "paper"
        }
    },
    {
        "page_content": "## 6.3 Empirical study\n\nIn Fig. 1, we showcase the aforementioned adaptive properties of trees on four representative datasets from the UC Irvine Machine Learning Repository, namely, the Airfoil SelfNoise (Fig. 1a) and Concrete Compressive Strength (Fig. 1b) datasets for regression and the Blood Transfusion Service Center (Fig. 1c), and HTRU2 (Fig. 1d) datasets for binary classification. We first standardized the input data so that each variable belongs to [0 , 1] , i.e., X ′ = ( X -X (1) ) / X ( ( n ) -X (1) ) . For each dataset, we generated 1000 trees from bootstrap samples of the data. Each tree was generated by rpart in R with default settings, except for the complexity parameter cp , which was set to 0 001 . . The black bars represent the median terminal subnode length glyph[lscript] j ( t ) = b j ( t ) -a j ( t ) for each tree, averaged over all 1000 trees. The white bars represent MDI (9) for each variable. Both quantities are scaled ̂ so that the largest among them is 100 and the variables are ordered according to increasing ̂ MDI. In agreement with Theorem 1, the barplots reveal the inverse relationship between ̂ MDI ( X j ) and the terminal subnode lengths glyph[lscript] j .",
        "metadata": {
            "section_header": "Empirical study",
            "title": "Analyzing CART",
            "type": "paper"
        }
    },
    {
        "page_content": "## 6.4 Lower bounds on MDI\n\nIn light of Theorem 1 and Remark 2, it is natural to ask when MDI ( X j ; t ) diverges. We now provide some answers. First, let us mention that studying MDI ( X j ; t ) directly is hopeless since it is nearly impossible to give a closed form expression for each ∆( j, s ∗ ; t ′ ) . Nevertheless, MDI ( X j ; t ) can still be lower bounded by giving a lower bound on each weight w j, s ( ∗ ; t ) and reduction in impurity ∆( j, s ∗ ; t ) .\n\nBy definition of s ∗ , one can lower bound each ∆( j, s ∗ ; t ′ ) by ∆( j, s ′ ; t ′ ) for any s ′ ∈ [ a j ( t ′ ) , b j ( t ′ )] . Effectively, this means that to lower bound MDI ( X j ; t ) , one can replace ∆( j, s ∗ ; t ′ ) by ∆( j, s ′ ; t ′ ) for any choice s ′ in [ a j ( t ′ ) , b j ( t ′ )] or (per a Bayesian perspective) by the integrated decrease in impurity ∫ b j ( t ′ ) a j ( t ′ ) ∆( j, s ; t ′ )Π( ds ) with respect to a prior Π on the splits. (It is often convenient to choose s ′ to be the median of X j | X ∈ t ′ .) This observation is crucial to the forthcoming analysis, since it reduces the burden of finding s ∗ exactly to finding a suitable choice of s ′ or prior Π for which ∆( j, · ; t ′ ) is tractable to analyze.\n\nTo lower bound the weights w j, s ( ∗ ; t ′ ) in Theorem 1, one is confronted with obtaining a useful upper bound on either | G j ( s ∗ ; t ′ ) | for (15) or | F ′ j ( s ∗ ; t ′ ) | for (16). We will see that it typically suffices to bound either by their supremum norm over splits in the parent subnode,\n\n<!-- image -->\n\n(a) Airfoil Self-Noise : n = 1503 , d = 5 .\n\n<!-- image -->\n\n(c) Blood Transfusion Service Center : n = 748 , d = 4 .\n\n(b) Concrete Compressive Strength : n = 1030 , d = 8 .\n\n<!-- image -->\n\nFigure 1: Median subnode length of each tree, averaged over 1000 bootstrapped trees (black bars) and MDI ̂ ( X j ) (9) (white bars). Both quantities are scaled so that the largest among them is 100 and the variables are ordered according to increasing MDI ̂ ( X j ) .\n\n<!-- image -->\n\nand so no explicit knowledge of s ∗ is required. For the lower bound (16), one additionally needs to lower bound the conditional density of X j | X ∈ t ′ at s ∗ , or p j ( s ∗ | t ′ ) . This too is a simple task if the joint density of X is uniformly bounded away from zero by a positive constant, in which case p j ( s ∗ | t ′ ) ≥ inf s p j ( s | t ′ ) &gt; 0 . For example, if X is uniformly distributed, then p j ( s ∗ | t ′ ) = ( b j ( t ′ ) -a j ( t ′ )) -1 .\n\nUsing these observations, we show in Theorem 2 that MDI ( X j ; t ) can be lower bounded by a positive constant multiple of the selection frequency of X j in the tree. For brevity, we defer its proof until Section 11. Before we state Theorem 2, we first introduce some concepts. Central to the paper is a quantity which we call the node balancedness . It measures the infinite sample proportion of data in the parent node that is contained in either daughter node from an optimal split.\n\nDefinition 2 (Node balancedness) . The balancedness of a node t is defined by\n\n<!-- formula-not-decoded -->\n\nAnother way of thinking about node balancedness is the following. Suppose we randomly generate a new X from P XX t | ∈ and classify Z = +1 if X j ≤ s ∗ or Z = -1 if X &gt; s j ∗ . Then λ j ( t ) is simply the variance of Z .\n\nThe node balancedness is always one (perfectly balanced) when the split is performed at the median of the conditional distribution X j | X ∈ t . This particular situation occurs in the special case that the regression surface is linear and the input distribution is uniform. In general, the quantity λ j depends on the node t -if t changes, so does s ∗ . Wenow introduce a more global measure, which depends only on the regression function.\n\nDefinition 3 (Global balancedness) . The global balancedness Λ j is defined as\n\n<!-- formula-not-decoded -->\n\nwhere the infimum runs over all parent nodes t of the best split left and right daughter nodes t ∗ L and t ∗ R , respectively.\n\nWith these definitions in place, we are now ready to state Theorem 2, which lower bounds MDI ( X j ; t ) in terms of the selection frequency for X j . For brevity, the proof is deferred until Section 11.2.\n\nTheorem 2. Suppose the j th direction of f is not too 'flat' in the sense that there exists a finite integer R ≥ 1 such that for each x j in [0 , 1] , there is a finite-order partial derivative, ∂ r ∂x r j f ( x , j x \\ j ) with 1 ≤ r ≤ R , that is nonzero and continuous for all other input coordinates x \\ j in [0 , 1] d -1 . More formally, assume\n\n<!-- formula-not-decoded -->\n\nis finite. If additionally the features of X are independent and have marginal densities that are continuous and never vanish, then the global balancedness Λ j is strictly positive and\n\n<!-- formula-not-decoded -->\n\nwhere K j ( t ) =",
        "metadata": {
            "section_header": "Lower bounds on MDI",
            "title": "Analyzing CART",
            "type": "paper"
        }
    },
    {
        "page_content": "# { t ′ ⊃ t : j t ′ = j } is the number of times X j was selected among all ancestor nodes of terminal node t .\n\nIt will be shown in Section 7 (see Theorem 3) that any linear combination of Gaussian radial basis functions in R d with positive weights satisfies (17). Furthermore, (17) also holds for any nonconstant, one-dimensional polynomial or partial sum of a Fourier series.\n\nRemark 5. Taken together, Theorem 1 and Theorem 2 imply that diam S ( t ) converges to zero (exponentially fast) in P X -probability when K j ( t ) → + ∞ for each j ∈ S . The feature selection frequencies of important variables are typically large for deeply grown\n\nglyph[negationslash]\n\ndecision trees (in fact, K j ( t ) is usually scales with the tree depth) and hence this condition is typically met. That is, the number of times ∆( j, s ∗ , t ′ ) &gt; max j ′ = j ∆( j , s ′ ∗ , t ′ ) at an ancestor node t ′ of t typically scales with the tree depth.\n\nRemark 6. Condition (17) does not mean that all partial derivatives of f exist and are continuous. For example, the function f ( x ) = x 1 +( x 1 -1 2) / 2 sgn ( x 1 -1 2) / has discontinuous second derivative, yet still satisfies the condition.",
        "metadata": {
            "section_header": "t ′ ⊃ t : j t ′ = j } is the number of times X j was selected among all ancestor nodes of terminal node t .",
            "title": "Analyzing CART",
            "type": "paper"
        }
    },
    {
        "page_content": "## 6.5 Examples\n\nTheorem 2 does does not show how Λ j depends on the structure of the regression function. It seems, at least for now, that such results are only available on a case-by-case basis and obtained with considerable effort. Here we give some example calculations of Λ j for polynomial and trigonometric functions which decay inversely with the degree and periodicity, respectively. These theoretical results are accompanied by plots (see Fig. 2 and Fig. 3) of ∆( j, · ; t ) together with sampling distributions of ˆ s ∈ arg max s ̂ ∆( j, s ; t ) from a sample size of n = 100 over 100 independent replications. Note that here a smaller sample size was purposely chosen to mimic a situation where the split is performed in a deep node and thus likely to contain only a small number of observations. As evidenced by the plots, the optimal splits tend to be closer to the parent subnode edges (in this case 0 and 1 ) with larger degree and periodicity. This phenomenon is manifested in the lower bounds on Λ j in Example 1 and Example 2-some of the terminal nodes will be large if splits from ancestor nodes are close to their parent subnode edges. In conjunction with Theorem 1 and the inverse relationship between the terminal node size and MDI ( X j ; t ) (being a weighted sum of ∆( j, s ∗ ; t ′ ) ), the plots Fig. 2 and Fig. 3 also reveal that the splits tend to be near the edges when the reduction in impurity is small. In future sections, we will theoretically confirm this (see Theorem 7 and Theorem 9) and show, more generally, that splits occur near the edges of the parent subnode whenever ∆( j, s ∗ ; t ′ ) is small. 8 This phenomenon has also been dubbed 'end-cut preference' in the literature [25], [12, Section 11.8]. In Section 11.1, we will study a penalized variant of ∆( j, s ∗ ; t ′ ) in order to mitigate this effect.\n\nFor each of the following three examples, we assume that X is uniformly distributed on [0 , 1] d .\n\nExample 1. Suppose f ( x ) = ∑ d j =1 β x j k j j for nonzero constants { β j } and integer k j ≥ 0 . Suppose j ∈ S so that k j ≥ 1 . Then\n\n<!-- formula-not-decoded -->\n\nIt is possible to show that, more generally, if f ( x ) = ∑ d j =1 β j ( x j -α j ) k j , for constants { α j } , then Λ j ≥ C 4 -k / j 3 k -4 3 / j for some universal constant C &gt; 0 .\n\n8 Note that Theorem 7 and Theorem 9, used to prove this phenomenon, do not need Assumption 2.\n\n<!-- image -->\n\n↦\n\n<!-- image -->\n\n(a) Plot of s → ∆(1 , s ; [0 , 1]) for f ( x ) = x 2 1 .\n\n<!-- image -->\n\n↦\n\n(b) Plot of s → ∆(1 , s ; [0 , 1]) for f ( x ) = x 10 1 .\n\n(c) Histogram of ˆ s for Y = X 2 + ε ( X ∼ Uniform(0 1) , , ε ∼ N (0 1) , ) with n = 100 from 100 replications.\n\n<!-- image -->\n\n(d) Histogram of ˆ s for Y = X 10 + ε ( X ∼ Uniform(0 1) , , ε ∼ N (0 , 1) ) with n = 100 from 100 replications.\n\n↦\n\nFigure 2: Plots of s → ∆(1 , s ; [0 , 1]) and corresponding maxima (dotted vertical lines) for Example 1. Histograms show sampling distribution of ˆ s for n = 100 from 100 replications.\n\nExample 2. Suppose f ( x ) = ∑ d j =1 β j sin(2 πm x j j ) for nonzero constants { β j } and integer m j ≥ 0 . Suppose j ∈ S so that m j ≥ 1 . There exists a universal constant C &gt; 0 such that\n\n<!-- formula-not-decoded -->\n\nglyph[negationslash]\n\nExample 2 implies that to ensure each terminal subnode has small probability content, K j ( t ) (or the tree depth) should be larger for functions that have larger frequencies in their frequency domain. This is not merely a coincidence. In fact, it can be shown more generally that if X is uniformly distributed, then ∆( j, s ∗ ; t ) ≥ π -2 ∑ k =0 | c k | 2 k -2 , where { c k } are the Fourier coefficients of the conditional partial dependence function F j ( x j ; t ) ∼ ∑ k c k e 2 π k x i ( j -a j ( t )) / b ( j ( t ) -a j ( t )) . This can easily be established by lower bounding\n\n<!-- image -->\n\n↦\n\n<!-- image -->\n\n(a) Plot of s → ∆(1 , s ; [0 , 1]) for f ( x ) = sin(4 πx 1 ) .\n\n<!-- image -->\n\n↦\n\n(b) Plot of s → ∆(1 , s ; [0 , 1]) for f ( x ) = sin(20 πx 1 ) .\n\n(c) Histogram of ˆ s for Y = sin(4 πX ) + ε ( X ∼ Uniform(0 1) , , ε ∼ N (0 1) , ) with n = 100 from 100 replications.\n\n<!-- image -->\n\n(d) Histogram of ˆ s for Y = sin(20 πX ) + ε ( X ∼ Uniform(0 1) , , ε ∼ N (0 1) , ) with n = 100 from 100 replications.\n\n↦\n\nFigure 3: Plots of s → ∆(1 , s ; t ) and corresponding maxima (dotted vertical lines) for Example 2. Histograms show sampling distribution of ˆ s for n = 100 from 100 replications.\n\n<!-- formula-not-decoded -->\n\nTable 1: Lower bounds on Λ j for j = 1 2 3 4 5 , , , , in Example 3. For j = 1 2 , , we use Lemma 8, for j = 3 , we use Lemma 7, and for j = 4 5 , , we use Example 1 with k j = 1 .\n\n| Variable   | Λ j ≥          |\n|------------|----------------|\n| X 1        | 1 / (2 π 2 )   |\n| X 2        | 1 / (2 π 2 )   |\n| X 3        | (1 / 12) 2 / 3 |\n| X 4        | (1 / 4) 1 / 3  |\n| X 5        | (1 / 4) 1 / 3  |\n\nusing Parseval's identity. That is, glyph[negationslash]\n\nglyph[negationslash]\n\n<!-- formula-not-decoded -->\n\nglyph[negationslash]\n\nglyph[negationslash]\n\nOur final example is a quintessential non-additive regression function known as 'Friedman #1' from [17, Section 4.3]. This function was used by Friedman to illustrate the efficacy of Multivariate Adaptive Regression Splines (MARS).\n\nExample 3. Suppose\n\n<!-- formula-not-decoded -->\n\nThen each Λ j is bounded from below in Table 1.",
        "metadata": {
            "section_header": "Examples",
            "title": "Analyzing CART",
            "type": "paper"
        }
    },
    {
        "page_content": "## 7 Assumptions on the regression function\n\nglyph[negationslash]\n\nNote that variable 'irrelevance' is not the same as conditional independence, i.e., Y ⊥ X j | X \\ j , X ∈ t , as some variable X j can be irrelevant in the sense that ∆( j, · ; t ) = 0 , yet it still influences the distribution of output values, i.e., Y ⊥ X j | X \\ j , X ∈ t . 9 Therefore, in order to ensure that MDI ( X j ; t ) is large for all nodes t in direction with a truly strong signal, i.e., j ∈ S , we need a condition like (17) so that the regression function\n\n9 See [30] for more details along these lines.\n\nglyph[negationslash]\n\nglyph[negationslash]\n\nis marginally not too 'flat' and hence ∆( j, s ∗ ; t ) &gt; 0 . 10 Condition (17) is sufficient (but not necessary 11 ) for the existence of a positive Λ j which depends only on the regression function. In the additive case, i.e., when f ( x ) = ∑ d j =1 f j ( x j ) , condition (17) is stronger than saying that each f j is continuous and nonconstant on each subnode. For example, flat functions, i.e., f ( x ) = e -1 /x 2 1 ✶ { x 1 =0 } with ∂ r ∂x r 1 f ( x ) | x 1 =0 = 0 for all r , or functions with derivatives having essential discontinuities, i.e., f ( x ) = x 2 1 sin(1 /x 1 ) ✶ { x 1 =0 } may be both continuous and nonconstant on each subnode, yet violate (17). In particular, for both functions, ∆(1 , s ∗ ; t ) &gt; 0 for all nodes-however, Λ 1 must equal zero.\n\nIn the multivariable, non-additive case, the function f ( x ) = x 1 + x 2 -2 x x 1 2 does not satisfy (17) because f ( x , 1 1 2) / and f (1 / , x 2 2 ) are both equal to 1 2 / and therefore are constant. In fact, if X is uniformly distributed and t = [0 1] , 2 , then integrating out either direction yields a constant function on [0 , 1] and hence ∆( j, s ∗ ; t ) = 0 for j = 1 2 , . More generally, if ∆( j, · ; t ) = 0 for j = 1 2 , , . . . , d , then any split along any variable in t results in a zero decrease in impurity [39, Technical Lemma 1], despite the possibility that the regression function is nonconstant on t . Such a situation may lead one to erroneously classify certain features as 'weak' when they may not be so. Practically speaking, this means that the CART algorithm may ignore certain variables and therefore fail to create a fine enough partition of [0 , 1] d , which may introduce a large amount of bias. Consider again the example f ( x ) = x 1 + x 2 -2 x x 1 2 with uniformly distributed predictors and suppose we wish to split at node t = [0 1] , 2 . Any further splitting via the CART protocol will result in zero impurity reduction. Thus, one may be tempted to assume the function is constant on the node when in fact f 'strongly' depends on both variables. Hence, we require some sort of 'self-consistency' property of the regression function, namely, if ∆( j, · ; t ) is zero for all splits s ∈ [ a j ( t ) , b j ( t )] for j = 1 2 , , . . . , d , then f is constant on t and therefore the bias of the tree on that node is zero. With this self-consistency assumption, even though the diameters of the nodes may not converge to zero, for the purpose of controlling the approximation error, one can safely ignore the regression function entirely on that node, regardless of whether the algorithm actually performs any further splitting.\n\nDespite the aforementioned difficulties, we show in the sequel that any linear combination of Gaussian radial basis functions with positive weights satisfies (17). To this end, consider the function class\n\n<!-- formula-not-decoded -->\n\n10 Conversely, ∆( j , s ′ ∗ , t ) for j ′ ∈ S c may be on the same order as ∆( j, s ∗ , t ) for j ∈ S due to spurious correlation between X j and X j ′ . Thus, the CART algorithm may have a selection bias and spend unnecessary time splitting on X j ′ , when conditional on X j , this variable plays no role in determining the output, i.e., Y ⊥ X j ′ | X , j X ∈ t . This situation is less troublesome from an approximation error perspective, but becomes highly relevant for variable selection and interpretability.\n\n11 For example, in one dimension, there are examples of regression functions whose r th order derivatives in (17) have jump or removable discontinuities, yet Λ j &gt; 0 .\n\nWe allow for the possibility that Σ k contains diagonal entries that are equal to zero, with the interpretation that the corresponding term is independent (constant) in that coordinate. It is known that F is a dense subclass of all positive continuous functions on [0 , 1] d [33]. We have the following theorem.\n\nTheorem 3. If j ∈ S , then any f ∈ F satisfies (17).\n\nProof. Without loss of generality, suppose each weight of combination w k is strictly positive. Then f ( x ) as a function of x j has the form of a one-dimensional combination of Gaussian radial basis functions with positive weights, i.e., ∑ K k =1 w ′ k exp { h k ( x j -µ k ) 2 } , where w ′ k &gt; 0 , h k , and µ k belong to R . It can further be assumed without loss of generality that each Gaussian function exp { h k ( x j -µ k ) 2 } in the sum is distinct. Suppose, contrary to hypothesis, that f does not satisfy (17). Then, since f has continuous partial derivatives of all orders, there exists x ′ ∈ [0 , 1] d such that ∂ r ∂x r j f ( x ′ ) = 0 for all r ≥ 1 . However, f as a function of x j is analytic and so f ( x , j x ′ \\ j ) is constant in a neighborhood of x ′ j , say ( a, b ) . This implies that the partial derivative of f with respect to x j at ( x , j x ′ \\ j ) , namely 2 ∑ K k =1 h w k ′ k ( x j -µ k ) exp { h k ( x j -µ k ) 2 } , is equal to zero for all x j ∈ ( a, b ) . Thus, one is confronted with the question about the multiplicity of ways to represent the zero function of an exponential polynomial. To answer this, consider the following lemma, which is a special case of a much more general result in complex analysis [27, Theorem 1.6], [22] and can be deduced using induction and differentiation. For the sake of completeness, we include its proof in Appendix A.\n\nLemma 1 (Linear independence of exponential polynomials) . Suppose P , . . . , P 1 K are distinct (real or complex) polynomials without constant terms and R , . . . , R 1 K are (real or complex) polynomials. If ∑ K k =1 R e k P k = 0 on an open subset (of the reals or complex plane), then R 1 = · · · = R K = 0 .\n\nNote that 2 ∑ K k =1 h w k ′ k ( x j -µ k ) exp { h k ( x j -µ k ) 2 } is an exponential polynomial of the form ∑ K k =1 R k ( x j ) e P k ( x j ) and hence by Lemma 1 above, R k ( x j ) = 2 h w k ′ k ( x j -µ k ) = 0 , implying that h k = 0 for all k (since w ′ k &gt; 0 ). Thus f is constant in x j on [0 , 1] for all x \\ j ∈ [0 , 1] d -1 , which is a contradiction to the assumption that j ∈ S .\n\nRemark 7. A similar argument to Theorem 3 can be used to show that any nonconstant polynomial or partial sum of a Fourier series in R also satisfies (17). Hence, there is a dense collection of additive regression functions f ( x ) = ∑ d j =1 f j ( x j ) that satisfies (17).\n\nIndividual decision trees are not competitive predictors, since their high variability and tendency to overfit makes them generalize poorly to new data. Random forests, on the other hand, are an archetypal example of variance reduction via ensemble averaging, where many weak predictors (such as decision trees) are combined to form a stronger predictor. Next, we will use Theorem 1 and Theorem 2 to show asymptotic consistency of Breiman's random forests grown with the infinite sample CART criterion.",
        "metadata": {
            "section_header": "Assumptions on the regression function",
            "title": "Analyzing CART",
            "type": "paper"
        }
    },
    {
        "page_content": "## 8 Application to random forests\n\nRandom forests are ubiquitous among ensemble averaging algorithms because of their ability to reduce overfitting, handle high-dimensional sparse settings, and efficient implementation. Due to these attractive features, they have been widely adopted and applied to various prediction and classification problems, such as those encountered in bioinformatics and computer vision. The base learner for a random forest is a binary tree constructed using the methodology of CART. Naturally, some of our analysis for decision trees can be carried over to random forests. We explore this connection in this section.\n\nRandom forests grow an ensemble of ntree regression trees. 12 Each tree is grown independently using a bootstrap sample of the original data (also known as a bagged decision tree). As with traditional decision trees, terminal nodes of the tree consist of the predicted values which are then aggregated by averaging to obtain the random forest predictor. Unlike CART decision trees, random forest trees are grown nondeterministically with two levels of randomization. In addition to the randomization introduced by growing the tree using a bootstrap sample, a second layer of randomization is injected with a random feature selection mechanism. Here, instead of splitting a tree node using all d features, the random forest algorithm selects, at each node of each tree, a random subset of mtry potential variables that are used to further refine the tree node by splitting. The number of potential variables mtry is often much smaller than d ; for regression, the default value is glyph[floorleft] d/ 3 glyph[floorright] . This two-level randomization is designed to decorrelate trees and therefore reduce variance. To reduce bias, random forest trees are grown deeply-in fact, each tree is grown as deeply as possible with the stipulation that each terminal node contains at least nodesize observations.\n\nMore concretely, a random forest is a predictor that is built from an ensemble of randomized base regression trees { ̂ Y ( x ; Θ m , D n ) } 1 ≤ m ≤ ntree . The sequence { Θ m } 1 ≤ m ≤ ntree consists of i.i.d. realizations of a random variable Θ , which governs the probabilistic mechanism that builds each tree. These individual random trees are aggregated to form the final output\n\n<!-- formula-not-decoded -->\n\nWhen ntree is large, the law of large numbers justifies using\n\n<!-- formula-not-decoded -->\n\nin lieu of (19), where E Θ denotes expectation with respect to Θ , conditionally on D n . We shall henceforth work with these infinite sample versions (i.e., infinite number of trees) of their empirical counterparts (i.e., finite number of trees).\n\n12 In what follows, we use typewriter fonts for the variable names in the R package randomForest .\n\nLet us now briefly describe the random feature mechanism of random forests in greater detail. To ensure that candidate strong (resp. weak) coordinates have high (resp. low) selection probabilities, at each step, we randomly select without replacement a subset M⊂{ 1 , . . . , d } of cardinality mtry and then select the variable in M and corresponding split s ∗ that most decreases impurity within the current node. That is, for each coordinate in M , calculate a split s ∗ that maximizes ∆( · , t ) and store the corresponding maximum value ∆( j, s ∗ ; t ) . Finally, select one variable X j ∗ at random among the corresponding largest elements of { ∆( j, s ∗ ; t ) } j ∈M to further split along within the current node. For a more detailed discussion of the algorithm, see [38]. As is argued in [5, Section 3], this random feature selection procedure will produce selection probabilities P Θ [ j t (Θ) = j ] that concentrate around 1 /S for j ∈ S and zero otherwise. Hence each 'strong' variable has roughly an equal chance of being selected among all 'strong' variables.\n\nResearchers have spent a great deal of effort in understanding theoretical properties of various streamlined versions of Breiman's original algorithm [19, 20, 2, 7, 16, 5, 37]. See [6] for a comprehensive overview of current theoretical and practical understanding. Unlike Breiman's CART algorithm, these stylized versions are typically analyzed under the assumption that the probabilistic mechanism Θ that governs the construction of each tree does not depend on the pair ( X , Y ) (i.e., the splits to not depend on the data distribution), largely with the intent of reducing the complexity of their theoretical analysis. Such models are referred to as 'purely random forests' [20]. In one variant known as a 'centered random forest' (proposed by Breiman himself in a technical report [10] and later studied by [5]), the splits are performed at the midpoint of each subnode and hence corresponds to a special case of the present paper, where the input distribution is uniform and the regression surface is linear.\n\nInspired by the results of Theorem 1 and [37, Theorem 4.1], we now show asymptotic consistency for Breiman's random forests with splits determined by the infinite sample CART sum of squares error criterion. Consistency of random forests with CART (albeit, with the finite sample splitting criterion) was previously only known when the regression function has an additive structure [38]. While this important work provides insight into the complicated and subtle mechanisms of random forests, it still does not adequately explain its potential as a general nonparametric method. For example, additive models are not flexible enough to allow for interactions among covariates (which limits their flexibility for multi-dimensional statistical modeling), and there are already other highly effective training algorithms such as backfitting [11].\n\nWe follow the terminology of Scornet [37] and call a random forest 'totally nonadaptive' if it is built independently of the training set D n . Let maxnodes denote the maximum number of terminal nodes in the tree built with randomness Θ . Due to space constraints, we defer the proof of the next result, Theorem 4, until Appendix A. For the statement of Theorem 4, we let MDI ( X j ; t ) denote the conditional mean decrease in impurity (11) with weights from Theorem 1.\n\nTheorem 4. Consider a totally nonadaptive forest predictor ̂ Y ( x ) = ̂ Y ( x ; D n ) , where each\n\ntree is grown with the infinite sample CART sum of squares error criterion (3). Suppose that\n\n- (a) f ( x ) is continuous on [0 , 1] d ;\n- (b) n/ maxnodes → + ∞ with P Θ -probability one; and\n- (c) min t MDI ( X j ; t ) → + ∞ with P Θ -probability one for all j ∈ S .\n\n<!-- formula-not-decoded -->\n\nRemark 8. If additionally the regression function satisfies the assumptions of Theorem 2, then min t MDI ( X j ; t ) ≥ Λ min j t K j ( t ) , where Λ j &gt; 0 . By the previous discussion, P Θ [ j t ′ (Θ) = j ] ≈ 1 S &gt; 0 for each nonterminal node t ′ and hence min t K j ( t ) goes to infinity with the tree depth P Θ -almost surely.\n\nRemark 9. When coupled with a study of the variance of the forest, our theory for the bias of individual trees (Theorem 1 and Theorem 2) enables one to determine the quality of convergence of ̂ Y as a function of the parameters of the random forest, e.g., sample size, dimension, sparsity level, and depth to which the individual trees are grown. The analysis also reveals a local bias-variance tradeoff, which highlights the local adaptivity of random forests. However, we shall choose not to pursue this analysis in the present paper and leave it for future work.",
        "metadata": {
            "section_header": "Application to random forests",
            "title": "Analyzing CART",
            "type": "paper"
        }
    },
    {
        "page_content": "## 9 Finite sample analysis\n\nFrom the perspective of studying the theoretical properties of decision trees, it is desirable if the splits do not separate a small fraction of data points from the rest of the sample. That is, the node counts N ( t ) should be large enough to contain enough data points so that local estimation valid. On the other hand, the node sizes should be small enough to identify local changes in the regression surface. This is true, for example, if the splitting criterion encourages splits that are performed away from the parent subnode edges. Perhaps the earliest mention of such a condition is [12, Section 12.2], where, en route to establishing asymptotic consistency, it is assumed that the proportion of data points (from a learning sample of size n ) in each terminal node is at least k n n -1 log n for some sequence k n tending to infinity and that the diameters of the terminal nodes converge to zero in probability. A similar assumption is explicitly made in the analysis of [32] and [46] to ensure that terminal node diameters of the forest tend to zero as the sample size tends to infinity, which as mentioned earlier, is a necessary condition to prove the consistency of partitioning estimates [40], [23, Chapter 4]. This property is also satisfied by q quantile forests [37], where each split contains at least a fraction q ∈ (0 1) , of the observations falling into the parent node. Furthermore, in a Bayesian setting, comparable regularity conditions for partitions have been assumed for theoretical analysis of Bayesian random forests (e.g., BART [15]) [36, Definition 3.1], [42, Definition 2.4], [35, Definition 6.1] (e.g., in so-called median or k d -tree partitions [4], each split roughly halves the number of data points inside the node).\n\nFrom the perspective of adaptive estimation, forcing the recursive partitions to artificially separate a fixed fraction of the data points at each step may be undesirable. Indeed, [25] argues that CART posses a desirable trait of splitting near the edges along noisy variables. This perspective, together with the fact that one does not know a priori which variables are important, leads one to the conclude that it is undesirable to sacrifice the data-dependent nature of the split criterion in order to ensure that a technical condition is met. There are currently no results stating that splits in CART are performed away from the parent subnode edges. However, our results show that the standard assumptions for 'valid partitions' (for example, see [32, Assumption 3], [46, Definition 1], or [45, Definition 4]) are satisfied for CART.\n\nIn the next subsection, we show how one can go from bounds on the conditional probability content of an optimally split subnode to bounds on the distance of an optimal split to its parent subnode edges.",
        "metadata": {
            "section_header": "Finite sample analysis",
            "title": "Analyzing CART",
            "type": "paper"
        }
    },
    {
        "page_content": "## 9.1 From probabilities to distances\n\nLet Q p ( ) be the quantile function of the probability measure on [ a ( t ) , b ( t )] with distribution function P [ X ≤ s | X ∈ t ] , i.e., Q p ( ) = inf { s ∈ [ a ( t ) , b ( t )] : p ≤ P [ X ≤ s | X ∈ t ] } , p ∈ [0 , 1] . Suppose for the moment that X ∼ Uniform([0 1] ) , d . Then P [ X ≤ s | X ∈ t ] = s -a ( t ) b ( t ) -a ( t ) and hence Q p ( ) = a ( t ) + p b ( ( t ) -a ( t )) . From this, one can deduce that s ∗ lies between a ( t )+ b ( t ) -a ( t ) 2 Ψ and b ( t ) -b ( t ) -a ( t ) 2 Ψ , where Ψ = 1 -√ 1 -Λ . This allows us to go from probability estimates to distance estimates. Inspired by this observation, we impose a general condition on the form of the quantile function Q p ( ) so that similar conclusions can be made for other input distributions. The assumption can also be interpreted as a regularity condition on Q p ( ) . Roughly, it says that Q p ( ) cannot be too 'flat' when p is near zero or one.\n\nAssumption 3. There exist two increasing bijections q 1 : [0 , 1] → [0 , 1] and q 2 : [0 , 1] → [0 , 1] such that for all nodes t ,\n\n<!-- formula-not-decoded -->\n\nNote that q 1 and q 2 are both necessarily continuous. This device allows us to bound the distance of s ∗ to the edges a ( t ) and b ( t ) in terms of the global balancedness Λ . To see this, note that by Definition 3, 4 P ( t ∗ L ) P ( t ∗ R ) ≥ Λ and hence both P ( t ∗ L ) and P ( t ∗ R ) are between 1 2 (1 ± √ 1 -Λ) . Let Ψ = 1 -√ 1 -Λ ∈ [0 , 1] so that, by (73),\n\n<!-- formula-not-decoded -->\n\nFinally, using (20), we have that\n\n<!-- formula-not-decoded -->\n\nand, by rearranging terms,\n\n<!-- formula-not-decoded -->\n\nwhere Γ = 2min { q 1 (Ψ 2) 1 / , -q 2 (1 -Ψ 2) / } . These bounds show how far (in terms of distance) any optimal split is from the parent subnode edges. The inequality in (21) bounds the distance between any optimal split and the midpoint of the parent subnode.\n\nWe now provide some examples of joint distributions that satisfy Assumption 3. The proofs of each are given in Appendix A.\n\n- (a) Independent joint density bounded from above and below. If each X is i.i.d. Uniform(0 1) , ∼ Beta(1 1) , , then Q p ( ) = a ( t ) + p b ( ( t ) -a ( t )) and q 1 ( p ) = q 2 ( p ) = p .\n- (b) Independent joint density unbounded from below. If each X is i.i.d. Beta(2 1) , , then Q p ( ) = √ a 2 ( t ) + p b ( 2 ( t ) -a 2 ( t )) , q 1 ( p ) = p , and q 2 ( p ) = √ p .\n- (c) Independent joint density unbounded from above. If each X is i.i.d. Beta(1 / , 2 1) , then Q p ( ) = ( √ a ( t ) + p ( √ b ( t ) -√ a ( t ))) 2 , q 1 ( p ) = p 2 , and q 2 ( p ) = p .",
        "metadata": {
            "section_header": "From probabilities to distances",
            "title": "Analyzing CART",
            "type": "paper"
        }
    },
    {
        "page_content": "## 9.2 Number of observations in nodes\n\nIn this section, we give lower bound on (a) the number of observations contained in a daughter node from an optimally split parent node using the finite sample criterion ̂ ∆( ; · t ) (denoted by N ( ˆ t L ) and N ( ˆ t R ) ) and (b) the distance of the optimal split to the edges of its parent node. 13 Throughout this section, we implicitly assume that Assumption 1 and Assumption 3 hold for each of the covariates. For simplicity, we assume that S is equal to the full set of d variables, although under Assumption 2, one can also develop lower bounds for the number of observations that land in nodes along informative directions in S , i.e., N S = ∑ n i =1 ✶ { X i S ∈ t S } , where x S = ( x j : j ∈ S ) and t S = { x S : x ∈ t } . This is a quantity of interest because, from the perspective of estimation, one could still have consistency even if N S c = 0 .\n\nIn what follows, we define Ψ = 1 -√ 1 -Λ Γ = 2min , { q 1 (Ψ 2) 1 / , -q 2 (1 -Ψ 2) / } , p L = q -1 2 (Γ 2 / 2) , p R = 1 -q -1 1 (1 -Γ 2 / 2) , and p = min { p L , p R } . For example, when X ∼ Uniform([0 1] ) , d , we have Γ = Ψ and p L = p R = Ψ 2 / 2 .\n\nFor the next set of results, we let S ∗ = arg max s ∆( ; s t ) and ˆ s ∈ arg max s ̂ ∆( ; s t ) . Due to space constraints, we prove both theorems in Appendix A.\n\n13 The terminal node counts N ( ˆ t L ) and N ( ˆ t R ) are also called nodesize in the R package randomForest .\n\nTheorem 5. Suppose N ( t ) is large enough so that, given N ( t ) and t , with probability at least 1 -δ ,\n\n<!-- formula-not-decoded -->\n\nThen with probability at least 1 -δ ,\n\n<!-- formula-not-decoded -->\n\nIf t is independent of the training data D n , then, given N ( t ) and t , with probability at least 1 -δ -2 exp {-Np / 2 2 } ,\n\n<!-- formula-not-decoded -->\n\nRemark10. The assumption (22) can be recast by saying that the distance between s ∗ and ˆ s is less than a constant multiple, namely 1 2 Γ(1 -Γ) , of the length of the parent subnode. (Note that by definition, dist (ˆ s, S ∗ ) ≤ b ( t ) -a ( t ) .) Such an assumption is not unrealistic since if ∆( ; · t ) has a unique global maximum, i.e., s ∗ is unique, [25, Theorem 2] shows dist (ˆ s, S ∗ ) converges weakly to zero. In fact, with similar assumptions, one can characterize the rate of convergence [43, Theorem 3.2.5]. Indeed, [3, 13] show cube root asymptotics (i.e., n 1 3 / (ˆ s -s ∗ ) converges in distribution) of split points for one-level decision trees (e.g., decision stumps) using the CART sum of squares error criterion. The work of [13, Section 3.4.2] also extends these rates to multi-level decision trees, which is particularly relevant to our setting.\n\nIn practice, a ( t ) , b ( t ) , and t all depend on the data D n . We now state a refinement of Theorem 5 that allows for data-dependent splits. Essentially it says that if the optimal empirical and population split points are sufficiently close to each other and the fraction of data points contained in the parent node is at least α , then the fraction of data points contained in each daughter node is at least αβ , where β is strictly positive and depends only on q 1 , q 2 , and Λ . In fact, the next theorem shows that one can take β = min { p L , p R } / 2 with high probability. This ensures that the daughter nodes contain a sufficiently large number of training samples, so that subsequent empirical splits will be close to their infinite sample counterparts, and so on and so forth.\n\nTheorem 6. Let α &gt; 0 and suppose n is large enough so that with probability at least 1 -δ ,\n\n<!-- formula-not-decoded -->\n\nThen with probability at least 1 -δ ,\n\n<!-- formula-not-decoded -->\n\nand with probability at least 1 -δ -16( n 2 d +1)exp {-nα p / 2 2 512 } ,\n\n<!-- formula-not-decoded -->\n\nThe previous theorem can be used inductively to show that if the tree is grown to a depth of K , then with high probability, each terminal subnode length is at most (1 -Γ 2 / 2) K . An obvious line of future work would be to make this statement more rigorous and use it to furnish a convergence result for ensembles of decision trees with empirical splits.",
        "metadata": {
            "section_header": "Number of observations in nodes",
            "title": "Analyzing CART",
            "type": "paper"
        }
    },
    {
        "page_content": "## 10 Extension to classification trees\n\nOur results have focused on regression trees, although nearly identical bounds can be developed for classification trees. In the binary classification context, i.e., Y ∈ {-1 +1 , } , the variance impurity (1) equals ∆( ) = 4 t ×",
        "metadata": {
            "section_header": "Extension to classification trees",
            "title": "Analyzing CART",
            "type": "paper"
        }
    },
    {
        "page_content": "# { Y i =+1: X i ∈ } t N ( t ) × # { Y i = -1: X i ∈ } t N ( t ) , which is also known as Gini impurity. Here, instead of averages, the tree outputs the majority vote at a terminal node t , i.e., ̂ Y = +1 if # { Y i = +1 : X i ∈ t } ≥ # { Y i = -1 : X i ∈ t } and ̂ Y = -1 otherwise.\n\nThe infinite sample Gini impurity is\n\n<!-- formula-not-decoded -->\n\nThe analog to the conditional partial dependence function (7) is\n\n<!-- formula-not-decoded -->\n\nand its mean-centered version is\n\n<!-- formula-not-decoded -->\n\nIn agreement with (6), ∆( ; s t ) also has the representation\n\n<!-- formula-not-decoded -->\n\nThus, adapting the proofs of Theorem 7 and Theorem 9-whose original proofs also rely on such a representation for ∆( ; · t ) -it can easily be shown that Theorem 1 and its consequences hold verbatim with these modified definitions of F ( ; · t ) , G ( ; · t ) , and ∆( ; · t ) . In particular, Theorem 2 holds if P [ Y = +1 | X = x ] satisfies (17). As a concrete example (c.f., Example 1 and Example 2), consider the following logistic regression model. We give the proof in Appendix A.\n\nglyph[negationslash]\n\nExample 4. Let P [ Y = +1 | X = x ] = (1 + e -β 0 -〈 x , β 〉 ) -1 with intercept coefficient β 0 and effects coefficients β = ( β , β 1 2 , . . . , β d ) and X ∼ Uniform([0 1] ) , d . Suppose j ∈ S so that β j = 0 . Then,\n\n<!-- formula-not-decoded -->\n\nand hence\n\n<!-- formula-not-decoded -->\n\nCuriously, this lower bound does not depend on any of the parameters other than β j . The importance of X j decreases as we become increasingly more certain that either Y = +1 or Y = -1 , i.e., as the parameter β j or -β j grows, respectively.\n\nIn the next section, we provide proofs of the main results from Section 6-Theorem 1 and Theorem 2.",
        "metadata": {
            "section_header": "Y i =+1: X i ∈ } t N ( t ) × # { Y i = -1: X i ∈ } t N ( t ) , which is also known as Gini impurity. Here, instead of averages, the tree outputs the majority vote at a terminal node t , i.e., ̂ Y = +1 if # { Y i = +1 : X i ∈ t } ≥ # { Y i = -1 : X i ∈ t } and ̂ Y = -1 otherwise.",
            "title": "Analyzing CART",
            "type": "paper"
        }
    },
    {
        "page_content": "## 11 Proofs and additional results\n\nThroughout this section, for notational clarity and brevity, we omit dependence on the input variable index j for all quantities and assume that we are splitting on a generic coordinate X . We also sometimes omit dependence on t , and substitute a ( t ) and b ( t ) with a and b , respectively.\n\nThe first theorem, from which most of the results in the paper are derived, gives a clean expression for the optimally split daughter node probabilities in terms of the partial dependence function and largest reduction in impurity.\n\nTheorem 7. Suppose Assumption 1 holds and ∆( s ∗ ; t ) &gt; 0 . Then\n\n<!-- formula-not-decoded -->\n\nand consequently,\n\n<!-- formula-not-decoded -->\n\nProof. Recall from (6) that one can write\n\n<!-- formula-not-decoded -->\n\nNext, define\n\n<!-- formula-not-decoded -->\n\nso that\n\n<!-- formula-not-decoded -->\n\nAn easy calculation shows that\n\n<!-- formula-not-decoded -->\n\nTaking the derivative of ∆( ; s t ) with respect to s , we find that\n\n<!-- formula-not-decoded -->\n\nSuppose s ∗ is a global maximizer of (27) (in general, it need not be unique). Then a necessary condition (first-order optimality condition) is that the derivative of ∆( ; s t ) is zero at s ∗ . That is, from (29), s ∗ satisfies\n\n<!-- formula-not-decoded -->\n\nIf p ( t ∗ L ) &gt; 0 and ∆( s ∗ ; t ) &gt; 0 , it follows from rearranging (30) that\n\n<!-- formula-not-decoded -->\n\nThis expresses P ( t ∗ L ) as a fixed point of the mapping\n\n↦\n\n<!-- formula-not-decoded -->\n\nwhere ◦ denotes the composition operator and Q denotes the quantile function of the probability measure with distribution function P [ X ≤ s | X ∈ t ] , i.e.,\n\n<!-- formula-not-decoded -->\n\nThe solution to (31) is obtained by solving a simple quadratic equation of the form p = 1 2 / ± c √ p (1 -p ) , which proves the identity (24). The identity for the node balancedness (25) follows immediately from Definition 2 and (24).\n\nRemark 11. One can make connections between the representation in Theorem 7 and other quantities defined in the literature. For example, [25, Section 2.8] define the (empirical) edge-cut preference statistic of a split s as\n\n<!-- formula-not-decoded -->\n\nThe population version is | P ( t L ) -P ( t R ) | 2 , which according to Theorem 7, is equal to 1 2 √ 1 -λ ( t ) = 1 2 √ | G s ( ∗ ; t ) | 2 | G s ( ∗ ; t ) | 2 +∆( s ∗ ; t ) at the optimal split s ∗ .\n\nThe expression in Theorem 7 reveals that the optimal split is a perturbation of the median of the conditional distribution X | X ∈ t , where the gap is governed by the largest decrease in impurity, namely, ∆( s ∗ ; t ) , and the mean-centered partial dependence function, namely, G s ( ∗ ; t ) . At the very extreme, the reduction in weighted variance is smallest when there\n\nTable 2: Table showing the values of η t ′ ∈ {-1 +1 , } from Corollary 1.\n\n| η t ′   | daughter node t ′ s ∗ < median ( X j |   | X ∈ t ′ )   |\n|---------|------------------------------------------|-------------|\n| +1      | right                                    | yes         |\n| +1      | left                                     | no          |\n| - 1     | left                                     | yes         |\n| - 1     | right                                    | no          |\n\nis no signal in the splitting direction-̂ ∆( j, s ∗ ; t ) glyph[greatermuch] ̂ ∆( j , s ′ ∗ ; t ) ≈ 0 for j ∈ S and j ′ ∈ S c . Thus, by Theorem 7, splits along directions that contain a strong signal (as opposed to noisy or directions or directions with weak signals) tend to be further away from the parent node subedges [25]. In fact, this has been empirically observed for some time [12, Section 11.8], i.e., squared error impurity tends to favor end-cut splits-that is, splits in which the proportion of data contained in an optimally split node is close to zero or one. The perturbation from the median of X | X ∈ t is zero and hence P ( t ∗ L ) = 1 2 / when G s ( ∗ ; t ) = 0 , or equivalently, when F s ( ∗ ; t ) = E [ Y | X ∈ t , X = s ∗ ] = E [ Y | X ∈ t ] . This is true in the special case that the regression function is linear and the input distribution is uniform, since in this case it can be shown that s ∗ = ( a ( t ) + b ( t )) / 2 . Next, we state a more general result for other regression functions.\n\nExample 5. Suppose ∆( s ∗ ; t ) &gt; 0 . If X is uniform, then\n\n<!-- formula-not-decoded -->\n\nWe also have the following corollary, which expresses the P X -probability of any terminal node t in terms of the largest decrease in impurity and the conditional partial dependence function. Its proof can be deduced from a simple induction argument and Theorem 7.\n\nCorollary 1. Consider a decision tree with splits determined by optimizing the infinite sample CART objective (5). Suppose Assumption 1 holds and ∆( s ∗ ; t ′ ) &gt; 0 for all nodes t ′ . Then the P X -probability of any terminal node t is\n\n<!-- formula-not-decoded -->\n\nwhere the product extends over all ancestor nodes t ′ of t . The value of η t ′ ∈ {-1 +1 , } is given in Table 2.\n\nFor purposes of obtaining useful upper and lower bounds on p ( t ∗ L ) (and therefore also p ( t ∗ R ) ), we see from (24) that it suffices to lower bound ∆( s , ∗ t ) and upper bound | G s ( ∗ ; t ) | 2 + ∆( s ∗ ; t ) . The next lemma shows that | G s ( ∗ ; t ) | 2 + ∆( s ∗ ; t ) is at most the oscillation of the partial dependence function over the node.\n\nLemma 2. Suppose Assumption 1 and ∆( s ∗ ; t ) &gt; 0 . Then,\n\n<!-- formula-not-decoded -->\n\nFurthermore, if each first-order partial derivative of the regression function and joint density p X exist and are continuous, then\n\n<!-- formula-not-decoded -->\n\nProof. It can be shown that ∆( ; s t ) ≤ 4 P ( t L ) P ( t R ) sup s ∈ [ a ( t ) ,b ( t )] | G s ( ; t ) | 2 . By Theorem 7, 4 P ( t ∗ L ) P ( t ∗ R ) = ∆( s ∗ ; t ) | G s ( ∗ ; t ) | 2 +∆( s ∗ ; t ) . Thus, ∆( s ∗ ; t ) sup s ∈ [ a ( t ) ,b ( t )] | G s ( ; t ) | 2 ≤ ∆( s ∗ ; t ) | G s ( ∗ ; t ) | 2 +∆( s ∗ ; t ) , which is equivalent to the first claimed inequality in (32). Next, we show that sup s ∈ [ a ( t ) ,b ( t )] | G s ( ; t ) | ≤ ω F ( ( ; · t ); [ a ( t ) , b ( t )]) . Since\n\n<!-- formula-not-decoded -->\n\nby the generalized mean value theorem for definite integrals, there exists s ′ ∈ [ a ( t ) , b ( t )] such that G s ( ′ ; t ) = 0 . Hence sup s ∈ [ a ( t ) ,b ( t )] | G s ( ; t ) | can also be bounded by the oscillation of the partial dependence function F s ( ; t ) = E [ Y | X ∈ t , X = ] s on [ a ( t ) , b ( t )] since\n\n<!-- formula-not-decoded -->\n\nThis proves the inequalities in (32).\n\nTo show (33), note that when F ( ; · t ) is smooth, its oscillation is bounded by its total variation TV ( F ( ; · t ); [ a ( t ) , b ( t )]) = ∫ b ( t ) a ( t ) | F ′ ( s ; t ) | ds . This bound is occasionally useful and will be used in the proof of Example 1, Example 2, and Example 3.\n\nCombining Theorem 7 with Lemma 2, we have the following bound on the conditional daughter node probabilities.\n\nTheorem 8. Suppose Assumption 1 and ∆( s ∗ ; t ) &gt; 0 . Then both P ( t ∗ L ) and P ( t ∗ R ) are between\n\n<!-- formula-not-decoded -->\n\nand consequently,\n\n<!-- formula-not-decoded -->\n\nThis implies that P ( t L ) and P ( t R ) tend to be more extreme (i.e., closer to zero or one) if the oscillation of the partial dependence function F ( ; · t ) is large. Indeed, we have seen\n\nfrom Example 2 that the optimal split point for a sinusoidal waveform gets closer and closer to its parent subnode edges as the periodicity increases.\n\nNote that to obtain Theorem 7 and its consequences in Theorem 8, we only used the firstorder optimality condition for s ∗ . Next, we show that by additionally incorporating secondorder conditions, an alternate (and sometimes better) bound can be obtained. First, we state a lemma.\n\nLemma 3. Suppose Assumption 1 holds and each first-order partial derivative of the regression function and joint density p X exist and are continuous. Then,\n\n<!-- formula-not-decoded -->\n\n↦\n\nProof. If the first-order partial derivative of the joint density p X exists and is continuous, then the density of the conditional probability measure P X \\ j | X t ∈ , X j = x j ( d x \\ j ) , namely x \\ j → p X ( x , j x \\ j ) / ∫ t \\ j p X ( x , j x ′ \\ j ) d x ′ \\ j , is continuously differentiable in x j . If, in addition, the first-order partial derivative of the regression function exists and is continuous, then by Leibniz's integral rule, F ′ ( ; · t ) exists and is therefore well-defined. We will show that if ∂ ∂s ∆( ; s t ) | s = s ∗ = 0 , then\n\n<!-- formula-not-decoded -->\n\nThe conclusion (35) then follows from the fact that ∂ 2 ∂s 2 ∆( ; s t ) | s = s ∗ &lt; 0 since s ∗ is a global maximizer. Let us now show (36). We use the expression (29) as a starting point. Since ∂ ∂s ∆( ; s t ) | s = s ∗ = 0 , the second derivative at s = s ∗ is equal to\n\n<!-- formula-not-decoded -->\n\nNext, we compute the derivative in (37) and find that\n\n<!-- formula-not-decoded -->\n\nNext, recall that Ξ ( ′ s ) = p ( t L ) G s ( ; t ) , so that the expression in (38) is equal to\n\n<!-- formula-not-decoded -->\n\nNext, we multiply (39) by Ξ( ; ) s t 2 P ( t L ) P ( t R ) p ( t L ) so that (37) is equal to\n\n<!-- formula-not-decoded -->\n\nFinally, observe that by the first-order condition (29), (1 -2 P ( t ∗ L ))Ξ( s ∗ ; t ) G s ( ∗ ; t ) 2 P ( t ∗ L ) P ( t ∗ R ) = | G s ( ∗ ; t ) | 2 , and by definition, [Ξ( s ; t )] 2 P ( t L ) P ( t R ) = ∆( ; s t ) .\n\nTheorem 9. Suppose Assumption 1 holds, ∆( s ∗ ; t ) &gt; 0 , and each first-order partial derivative of the regression function and joint density p X exist and are continuous. Then both P ( t ∗ L ) and P ( t ∗ R ) are between\n\n<!-- formula-not-decoded -->\n\nand consequently,\n\n<!-- formula-not-decoded -->\n\nglyph[negationslash]\n\n<!-- formula-not-decoded -->\n\nProof. First note that F ′ ( s ∗ ; t ) = 0 , since otherwise, by (36) and the second-order optimality condition, ∆( s ∗ ; t ) = 0 . Using the solutions to the first-order condition (31), we have that and furthermore, by the second-order condition (35)\n\n<!-- formula-not-decoded -->\n\nRearranging yields\n\n<!-- formula-not-decoded -->\n\nFinally, it is a simple exercise to show that if 4 p (1 -p ) ≥ λ , then 1 2 (1 -√ 1 -λ ) ≤ p ≤ 1 2 (1 + √ 1 -λ ) .\n\nRemark 12. If the regression surface is linear and the distribution of X is uniform, then (40) is approximately equal to 0 2 . . Compare this with the true value of 0 5 . for both daughter node conditional probabilities.\n\nWe are now in a position to give the proof of Theorem 1.\n\nProof of Theorem 1. Let t ′ be the parent node of t . Suppose we split along coordinate X Then P [ X ∈ [ a ( t ) , b ( t )]] is at most\n\n.\n\n<!-- formula-not-decoded -->\n\nwhere the first inequality follows from Assumption 2, the penultimate inequality follows from max { p, 1 - } ≤ p e -p (1 -p ) for p ∈ [0 , 1] , and the final inequality follows from Theorem 7. By induction and using P [ X ∈ [0 , 1]] = 1 , we have P [ a j ( t ) ≤ X j ≤ b j ( t )] ≤ exp { -η 4 ∑ t ′ ⊃ t j t ′ = j [∆( j, s ∗ ; t ′ ) + | G j ( s ∗ ; t ′ ) | 2 ] -1 ∆( j, s ∗ ; t ′ ) } , which proves (14) with weights (15). The lower bound on the weights (16) is a direct consequence of Theorem 9.\n\nRemark 13. Note that ∆( s ∗ ; t ′ ) &gt; 0 is not needed for Theorem 1. This is because the inequality (14) remains true even if, for some t ′ , ∆( s ∗ ; t ′ ) = 0 (with the convention that 0 0 = 0 / ).",
        "metadata": {
            "section_header": "Proofs and additional results",
            "title": "Analyzing CART",
            "type": "paper"
        }
    },
    {
        "page_content": "## 11.1 Alternative splitting rules\n\nTo mitigate the effect of end-cut splits, as discussed in Section 6, one can subtract a positive penalty pen ( s ; t ) from ∆( ; s t ) and instead solve\n\n<!-- formula-not-decoded -->\n\nIntuitively, pen ( s ; t ) should be large when s is close to the edges and small when s is far from the edges. The penalty should also be proportional to ∆( ; s t ) so that some influence from original objection function is retained. One natural choice of penalty that meets these criteria is\n\n<!-- formula-not-decoded -->\n\nOf course, in practice one would use\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nThis regularization procedure is not new-[12, Section 11.8] proposed that, to avoid end-cut splits, one should instead maximize ̂ ∆( ; s t ) multiplied by some power of ̂ P ( t L ) ̂ P ( t R ) , say\n\n<!-- image -->\n\n<!-- image -->\n\n↦\n\n(a) Plot of s → ∆ 1 1 . (1 , s ; [0 , 1]) for f ( x ) = sin(20 πx 1 ) .\n\n(b) Histogram of ˆ s 1 1 . for Y = sin(20 πX ) + ε ( X ∼ Uniform(0 1) , , ε ∼ N (0 1) , ) with n = 100 from 100 replications.\n\n↦\n\nFigure 4: Plot of s → ∆ 1 1 . (1 , s ; t ) and corresponding maxima (dotted vertical lines). Histogram shows sampling distribution of ˆ s 1 1 . for n = 100 from 100 replications.\n\nglyph[negationslash]\n\n( ̂ P ( t L ) ̂ P ( t R )) α for α = 1 . Thus, ( ̂ P ( t L ) ̂ P ( t R )) α acts as a multiplicative regularizer that modulates the effect of edge-cut preference in CART. The essential challenge is to choose the regularization parameter large enough so that splits away from the parent subnode edges are encouraged, but not in such a way that the homogeneity of the node (as measured by the variance in ( X , Y ) ) is unimproved. Good values of α can be determined by any number of means, including cross-validation on a hold-out set of the data.\n\nDenote the objective function by ∆ ( ; α s t ) = (4 P ( t L ) P ( t R )) α ∆( ; s t ) and its maximizer by s ∗ α (similarly define ˆ s α as the empirical maximizer). In Fig. 4 (c.f., Fig. 3), we illustrate the effect of regularization in determining the optimal splits for the sinusoidal waveform encountered previously in Example 2.\n\nBy a similar argument to establishing Theorem 7, the optimal P ( t ∗ L ) satisfies\n\n<!-- formula-not-decoded -->\n\nWith v = 1 | -α | 2 ∆ α ( s ∗ α ; t ) | G s ( ∗ α ; t ) | 2 , we have 4 P ( t ∗ L ) P ( t ∗ R ) = 1 -v -1 (4 P ( t ∗ L ) P ( t ∗ R )) α +1 and hence 4 P ( t ∗ L ) P ( t ∗ R ) ≥ v 1 α +1 1+ v 1 α +1 . Let us now obtain a further lower bound on v 1 α +1 1+ v 1 α +1 . To this end, note that by concavity of x → x 1 ( / α +1) , we have\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nThis means that\n\n<!-- formula-not-decoded -->\n\nSolving this for P ( t ∗ L ) and P ( t ∗ R ) yields the following theorem, which is a direct analog to Theorem 7. Using this, we also give a lower bound for Λ α , the global balancedness (see Definition 3) for ∆ ( ; α · t ) .\n\nTheorem 10. Suppose Assumption 1 holds and ∆( s ∗ ; t ) &gt; 0 . Then both P ( t ∗ L ) and P ( t ∗ R ) are between\n\n<!-- formula-not-decoded -->\n\nand consequently\n\n<!-- formula-not-decoded -->\n\nIt is often possible to show that (1 -α ) 2 ∆ ( α s ∗ α ; t ) | G s ( ∗ α ; t ) | 2 +(1 -α ) 2 ∆ ( α s ∗ α ; t ) glyph[equivasymptotic] ∆( s ∗ ; t ) | G s ( ∗ ; t ) | 2 +∆( s ∗ ; t ) and hence, by virtue of (42), (25), and (43), that Λ α glyph[equivasymptotic] Λ 1 (1+ / α ) , where Λ = Λ 0 is the global balancedness for the unpenalized criterion. The next theorem (c.f., Theorem 9) shows that this is improvable to Λ α glyph[equivasymptotic] Λ 1 (3+ / α ) when α ∈ [0 , 1) . Using this, it can easily be shown via a modification of the proofs of Example 1 and Example 2 that Λ α = Ω( k -4 (3+ / α ) ) and Λ α = Ω( m -4 (3+ / α ) ) , respectively. These quantities are larger than their counterparts using the unpenalized ∆( ; · t ) and hence the regularization does indeed encourage splits that are farther away from the parent subnode edges.\n\nTheorem 11. Suppose Assumption 1 holds, ∆( s ∗ ; t ) &gt; 0 , and each first-order partial derivative of the regression function and joint density p X exist and are continuous. Let α ∈ [0 , 1) . Then both P ( t ∗ L ) and P ( t ∗ R ) are between\n\n<!-- formula-not-decoded -->\n\nand consequently,\n\n<!-- formula-not-decoded -->",
        "metadata": {
            "section_header": "Alternative splitting rules",
            "title": "Analyzing CART",
            "type": "paper"
        }
    },
    {
        "page_content": "## Proof. We have the chain of inequalities\n\n<!-- formula-not-decoded -->\n\n↦\n\nThe first inequality is by concavity of x → x 1 ( / α +1) and the second inequality is due to the fact that ∆ ( α s ∗ α ; t ) ≤ ∆( s ∗ α ; t ) . The third inequality comes from the second-order derivative condition (c.f., (36)), i.e., given ∂ ∂s ∆ ( ; α s t ) | s = s ∗ α = 0 , the second derivative ∂ 2 ∂s 2 ∆ ( ; α s t ) | s = s ∗ α equals\n\n<!-- formula-not-decoded -->\n\nFinally, combining (44) with (42) and solving for P ( t ∗ L ) and P ( t ∗ R ) yields both statements of the theorem.",
        "metadata": {
            "section_header": "Proof. We have the chain of inequalities",
            "title": "Analyzing CART",
            "type": "paper"
        }
    },
    {
        "page_content": "## 11.2 Lower bounds on the node balancedness\n\nOf special interest is Λ &gt; 0 , since this provides a nontrivial bound on the distance between any optimal split to its parent subnode edges. But can we expect this to hold in most settings? It is conceivable that λ may become extremely small when a ( t ) and b ( t ) are arbitrarily close to each other, since after all by Theorem 7, its defining quantities-∆( s ∗ ; t ) and ∆( s ∗ ; t ) + | G s ( ∗ ; t ) | 2 -expressed through their ratio, both approach zero. We argue that λ is still controlled in this case. To see this, suppose a ( t ) and b ( t ) are extremely close to each other. Then the partial dependence function is approximately linear, i.e., E [ Y | X ∈ t , X = ] s ≈ As + B for some constants A and B and also P [ X ≤ s | X ∈ t ] ≈ ( s -a ( t )) / b ( ( t ) -a ( t )) . Hence λ ≈ 1 , with equality if X is uniform and the conditional regression surface is exactly linear. The statement in Theorem 2, which we now prove, makes this intuition precise. First, we state two lemmas, but defer their proofs until Appendix A.\n\nglyph[negationslash]\n\nLemma4. If the features of X are independent and satisfy Assumption 1 and the regression function satisfies (17), then sup s ∈ [0 1] , inf r ≥ 1 { r : F ( r ) ( s ; t ) = 0 for all t } is finite and for each node t , ∆( s ∗ ; t ) &gt; 0 .\n\nLemma 5. Suppose the features of X are independent with marginal densities that are continuous and never vanish, and the regression function satisfies (17). If R = inf r ≥ 1 { r :\n\n↦\n\nglyph[negationslash]\n\nF ( r ) ( c ; t ) = 0 for all t } &lt; + ∞ , then\n\n<!-- formula-not-decoded -->\n\nwhere\n\n<!-- formula-not-decoded -->\n\nis the integrated decrease in impurity ∆( ; [0 · , 1]) of the regression function f ( x ) = ( x 1 -1 2) / R with respect to the uniform distribution.\n\nProof of Theorem 2. By Theorem 7, MDI ( X ; t ) with weights w s ( ∗ ; t ) = [ | G s ( ∗ ; t ) | 2 + ∆( s ∗ ; t )] -1 is at least Λ K ( t ) . Hence we need to show that global balancedness Λ is positive. To this end, the first step in the proof involves showing that\n\n<!-- formula-not-decoded -->\n\nglyph[negationslash]\n\nwhere R = inf r ≥ 1 { r : F ( r ) ( c ; t ) = 0 for all t } &lt; + ∞ (by Lemma 4) and ∆ R is the positive constant from Lemma 5. This can be accomplished by Lemma 5 since\n\n<!-- formula-not-decoded -->\n\nNext, consider an R -1 term Taylor expansion of F ′ ( ; · t ) . Then, by definition of R and a Taylor expansion argument, | F ′ ( s ∗ ; t ) | ≤ ( b -a ) R -1 ( R -1)! sup s ∈ [ a,b ] | F ( R ) ( s ; t ) | . Thus, combining (47) with the fact that lim ( a,b ) → ( c,c ) ( b -a p ) ( t L ) = 1 (since p X ( c ) &gt; 0 and p X ( ) · is continuous by assumption) and by continuity of F ( ; · t ) at s = c , we have\n\n<!-- formula-not-decoded -->\n\nglyph[negationslash]\n\nFinally, (41) from Theorem 9 implies (46). The assumption of finite R ′ = sup s ∈ [0 1] , inf r ≥ 1 { r : F ( r ) ( s ; t ) = 0 for all t } implies that\n\n<!-- formula-not-decoded -->\n\n↦\n\nNext, let a (resp. b ) denote the vector of lower (resp. upper) endpoints of the subnodes of t . Now, since the regression function is continuous, it follows that ( s, a b , ) → ∆( ; s t ) and ( s, a b , ) → G s ( ; t ) are both continuous on the domain { ( s, a b , ) ∈ [0 , 1] 2 d +1 : a ( t ) ≤\n\n↦\n\n↦\n\ns ≤ b ( t ) , a &lt; b } . 14 Consequently, by Berge's Maximum Theorem [1, Theorem 17.31] the mapping ( a b , ) → ∆( s ∗ ; t ) is continuous on T = { ( a b , ) ∈ [0 , 1] 2 d : a &lt; b } and ( a b , ) → G s ( ∗ ; t ) is an upper hemicontinuous correspondence on T . In particular, by Theorem 7, 4 P ( t ∗ L ) P ( t ∗ R ) = ∆( s ∗ ; t ) ∆( s ∗ ; t )+ | G s ( ∗ ; t ) | 2 and hence 4 P ( t ∗ L ) P ( t ∗ R ) is an upper hemicontinuous correspondence on T . Next, note that by Lemma 4 and Theorem 7, 4 P ( t ∗ L ) P ( t ∗ R ) &gt; 0 on T , and by (46), 4 P ( t ∗ L ) P ( t ∗ R ) ≥ min R ≤ R ′ ( 4∆ R R 2 ) 1 3 / &gt; 0 for all points ( a b , ) arbitrarily close to the boundary of T . Hence Λ &gt; 0 .",
        "metadata": {
            "section_header": "Lower bounds on the node balancedness",
            "title": "Analyzing CART",
            "type": "paper"
        }
    },
    {
        "page_content": "## Acknowledgment\n\nThe author is indebted to Min Xu, Minge Xie, Samory Kpotufe, Robert McCulloch, Andy Liaw, Richard Baumgartner, and Michael Kosorok for helpful discussion and feedback. The author would also like to thank Joowon Klusowski for her help with the proof of Lemma 5.",
        "metadata": {
            "section_header": "Acknowledgment",
            "title": "Analyzing CART",
            "type": "paper"
        }
    },
    {
        "page_content": "## SUPPLEMENT TO 'BEST SPLIT NODES FOR REGRESSION TREES'",
        "metadata": {
            "section_header": "SUPPLEMENT TO 'BEST SPLIT NODES FOR REGRESSION TREES'",
            "title": "Analyzing CART",
            "type": "paper"
        }
    },
    {
        "page_content": "## BY JASON M. KLUSOWSKI",
        "metadata": {
            "section_header": "BY JASON M. KLUSOWSKI",
            "title": "Analyzing CART",
            "type": "paper"
        }
    },
    {
        "page_content": "## A Supplemental Material\n\nIn this appendix, we give proofs of Lemma 1, Theorem 4, Lemma 4, Lemma 5, Theorem 5, and Theorem 6. We also give proofs of the examples from Section 6.4, Section 9.1, and Section 10.",
        "metadata": {
            "section_header": "A Supplemental Material",
            "title": "Analyzing CART",
            "type": "paper"
        }
    },
    {
        "page_content": "## A Proofs of main lemmas\n\nProof of Lemma 1. We proceed by induction. The case K = 1 is trivial, since R e 1 P 1 = 0 clearly implies R 1 = 0 . Now let K ≥ 2 be arbitrary and assume that the claim is true for all smaller values of K . Let P , . . . , P 1 K be distinct (real or complex) polynomials without constant terms and R , . . . , R 1 K be (real or complex) polynomials with\n\n<!-- formula-not-decoded -->\n\nglyph[negationslash]\n\nIf all R k are zero then we are done. Otherwise (without loss of generality) R K = 0 . First we divide (48) by e P K , yielding\n\n<!-- formula-not-decoded -->\n\nDifferentiating the identity (49) gives\n\n<!-- formula-not-decoded -->\n\nMultiply (49) by R ′ K and (50) by R K . Subtracting the two resultant expressions from each other yields\n\n<!-- formula-not-decoded -->\n\nNow we can apply the induction hypotheses, since the P k -P K are distinct polynomials without constant terms. It follows that\n\n<!-- formula-not-decoded -->\n\nglyph[negationslash]\n\nglyph[negationslash]\n\nIf R k = 0 , then (51) is impossible since P ′ k -P ′ K = 0 and hence deg( R R ′ k K -R R k ′ K ) &lt; deg( R R k K ( P ′ k -P ′ K )) . Therefore R k = 0 for k = 1 , . . . , K -1 , which also implies that R K = 0 .\n\nglyph[negationslash]\n\nProof of Theorem 4. Wefollow the proof of [37, Lemma 2] for quantile forests, but adapted to our setting. Let t k ( X , Θ) denote the node containing X of the tree built with randomness Θ at the k th step. By [37, Theorem 4.1], we will be done if we can show that ω f ( ; t X ( , Θ)) → 0 in P X , Θ -probability, where t X ( , Θ) is the terminal node of the tree containing X . Since f is continuous on [0 , 1] d , it is also uniformly continuous. Hence, for each ξ &gt; 0 , there exists δ &gt; 0 such that if diam S ( t ) ≤ δ , then ω f ( ; t ) ≤ ξ . Hence, we must show that diam S ( t X ( , Θ)) → 0 in probability. To this end, let j ∈ S , H = { x : x j = z } , and D = { A : A ∩ H = ∅} . Let j k (Θ) denote the coordinate selected to split along at the k th step of the tree. Suppose t k ( X , Θ) ∈ D . Then there are two cases:\n\n- 1. The next split in t k ( X , Θ) is performed along the j th coordinate and, in that case, one of the two resulting nodes has an empty intersection with H .\n- 2. The next split in t k ( X , Θ) is performed along a coordinate other than the j th and, in that case, the two resultant nodes have a non-empty intersection with H .\n\nThus, glyph[negationslash]\n\nglyph[negationslash]\n\n<!-- formula-not-decoded -->\n\nwhere t k ( D, Θ) is the (unique) node at the k th step of the forest construction that contains z . By induction and (25), this implies that P [ t X ( , Θ) ∈ D ] → 0 if min t MDI ( X j ; t ) → + ∞ with P Θ -probability one. Finally, consider a partition of [0 , 1] S into hypercubes of side length glyph[epsilon1] with sides determined by the hyperplanes { x : x j ′ = glyph[lscript]glyph[epsilon1] } , where j ′ ∈ S and glyph[lscript] = 0 1 , , . . . , glyph[ceilingleft] glyph[epsilon1] -1 glyph[ceilingright] . If t X ( , Θ) belongs to one of the hypercubes, then diam S ( t X ( , Θ)) ≤ √ Sglyph[epsilon1] . There are at most O glyph[epsilon1] ( -S ) such hyperplanes and hence\n\n<!-- formula-not-decoded -->\n\nif min t MDI ( X j ; t ) → + ∞ with P Θ -probability one, where D ranges over all hyperplanes of the form { x : x j ′ = glyph[lscript]glyph[epsilon1] } , with j ′ ∈ S and glyph[lscript] = 0 1 , , . . . , glyph[ceilingleft] glyph[epsilon1] -1 glyph[ceilingright] . The conditions of the theorem and [37, Theorem 4.1] imply the conclusion.\n\nProof of Lemma 4. Let r ≥ 1 and suppose ∂ r ∂x r j f ( x ) exists and is continuous for all x \\ j ∈\n\n[0 , 1] d -1 . By Leibniz's integral rule, we have\n\n<!-- formula-not-decoded -->\n\nglyph[negationslash]\n\nglyph[negationslash]\n\nBy (52) and the generalized mean value theorem for integrals, there exists x ′ \\ j ∈ t \\ j such that ∂ r ∂x r j f ( x , j x ′ \\ j ) = ∂ r ∂x r j F j ( x j ; t ) . By assumption that (17) holds there exists a finite integer R ≥ 1 such that for each x j ∈ [ a j ( t ) , b j ( t )] , there exists an integer 1 ≤ r ≤ R such that ∂ r ∂x r j f ( x , j x \\ j ) = 0 for all x \\ j ∈ [0 , 1] d -1 . In particular, ∂ r ∂x r j F j ( x j ; t ) = ∂ r ∂x r j f ( x , j x ′ \\ j ) = 0 and hence sup s ∈ [0 1] , inf r ≥ 1 { r : F ( r ) ( s ; t ) = 0 for all t } is finite. Since x j was arbitrary in [ a j ( t ) , b j ( t )] , this implies that F j ( ; · t ) is nonconstant on [ a j ( t ) , b j ( t )] . Finally, it is easy to show that if ∆( j, s ∗ ; t ) = 0 , then F j ( ; · t ) = E [ Y | X ∈ t ] .\n\nglyph[negationslash]\n\nProof of Lemma 5. First, note that\n\n<!-- formula-not-decoded -->\n\nSince a maximum is larger than an average, for any prior Π on [0 , 1] with density π ,\n\n<!-- formula-not-decoded -->\n\nIn particular, we choose the uniform prior, i.e., π s ( ) = ✶ { ∈ s [0 1] , } . Next, by assumption, p X ( ) · is positive and continuous and hence\n\n<!-- formula-not-decoded -->\n\nand\n\n<!-- formula-not-decoded -->\n\nwhere the convergence is uniform. Thus, we assume henceforth that X is uniform. The proof for general X follows similarly.\n\nLet D s ( ) denote the divided difference F a ( + ( s b -a ); t ) -F c ( ; t ) ( a + ( s b -a ) -c ) R and δ = c -a b -a . Then, we can rewrite ( b -a ) -R G a ( + s ′ ( b -a ); t ) via\n\n<!-- formula-not-decoded -->\n\nFix s ∈ [ a, b ] and use a Taylor expansion of F ( ; · t ) about the point s = c and continuity of F ( R ) ( ; · t ) at s = c to argue that\n\n<!-- formula-not-decoded -->\n\nwhere the convergence is uniform and the limit is nonzero by assumption. Without loss of generality, assume F ( R ) ( c ; t ) &gt; 0 . By uniform continuity, there exists ξ &gt; 0 such that if √ ( c -a ) 2 +( b -c ) 2 &lt; ξ , then\n\n<!-- formula-not-decoded -->\n\nUsing the fact that s (1 -s ) ≤ 1 4 / and Jensen's inequality, the expression\n\n<!-- formula-not-decoded -->\n\nis at least\n\n<!-- formula-not-decoded -->\n\nwhich by Fubini's theorem is equal to\n\n<!-- formula-not-decoded -->\n\nThe leading terms in δ in the integrand of (54) are, up to signs, δ R ∫ 1 0 D s ( )( s -1 2) / ds and Rδ R -1 ∫ 1 0 D s s s ( ) ( -1 2) / ds . However, note that\n\n<!-- formula-not-decoded -->\n\nwhich, per (53), is at most δ R ∫ 1 0 | D s ( ) -F ( R ) ( c ; t ) R ! || s -1 2 / | ds ≤ δ R -2 / 4 . Furthermore, per (53), D s ( ) is between F ( R ) ( c ; t ) 2 R ! and 3 F ( R ) ( c ; t ) 2 R ! . Hence, we have that ∫ 1 0 D s s s ( ) ( -1 2) / ds ≥ 1 48 F ( R ) ( c ; t ) R ! &gt; 0 . This shows that (54) is Ω( | δ | R -1 ) as | δ | → + ∞ . Hence, we may assume that δ = c -a b -a is bounded in the limit infimum (45). Using this, we have that lim inf ( a,b ) → ( c,c ) ∆( s ∗ ; t ) ( ( b -a R F ) ( R ) ( c ; t ) R ! ) 2 is at least\n\n<!-- formula-not-decoded -->\n\nNext, let us evaluate the infimum (55). In fact, we will show that it is achieved at δ = 1 2 / .\n\nTowards this end, elementary calculations reveal that the expression inside the infimum (55) is equal to\n\n<!-- formula-not-decoded -->\n\nDefine V R ( s, δ ) = s δ ( -1) R +(1 -s δ ) R -( δ -s ) R , so that the integral (56) can be written as\n\n<!-- formula-not-decoded -->\n\nWe first catalogue some facts about V R .\n\n- 1. If R is even, then V R ( δ, s ) ≥ 0 .\n- 2. V R ( δ, s ) = ( -1) R V R (1 -δ, 1 -s ) .\n- 3. If R is even, δ ≥ 1 2 / , and 0 ≤ s ≤ 1 2 / , then V R ( δ, s ) ≥ V R ( δ, 1 -s ) .\n- 4. ∂ ∂δ V R ( δ, s ) = RV R -1 ( δ, s ) .\n- 5. ∂ ∂δ V 2 R ( δ, s ) = 2 RV R ( δ, s ) V R -1 ( δ, s ) .\n\nBy the second fact and the representation (57), it follows that (56) is symmetric about δ = 1 2 / . Thus, it can be assumed that δ ≥ 1 2 / .\n\nUsing the fifth fact, we have that the derivative of (56) with respect to δ is\n\n<!-- formula-not-decoded -->\n\nAssume without loss of generality that R is even. By the first and fourth facts, V R +1 ( δ, s ) is increasing in δ and V R is nonnegative. Hence (58) is at least\n\n<!-- formula-not-decoded -->\n\nNote that V R +1 (1 / , s 2 ) = (1 -2 s -(1 -2 ) s R +1 ) / 2 R +1 is odd about s = 1 2 / and positive when s ≤ 1 2 / . Using this observation, we have that (59) is equal to\n\n<!-- formula-not-decoded -->\n\nand nonnegative by the third fact. Therefore, (56) is increasing when δ ≥ 1 2 / and hence minimized at δ = 1 2 / . Thus, the infimum (55) is equal to\n\n<!-- formula-not-decoded -->\n\nRoutine calculations also reveal that (60) is Ω(4 -R /R 2 ) .\n\n↦\n\nFinally, let us verify all five facts above. The second, fourth, and fifth facts are straightforward. The first fact holds since V R is the difference between a point and a chord that lies above it on the convex function s → s R . To show the third fact, note that\n\n<!-- formula-not-decoded -->\n\nwhich is negative since δ -s ≥ | 1 -δ - | s for δ ≥ 1 2 / and s ≤ 1 2 / . Since V R ( δ, s ) -V R ( δ, 1 -s ) has roots at s = 0 and s = 1 2 / , it follows that V R ( δ, s ) ≥ V R ( δ, 1 -s ) in this regime.\n\nRemark 14. The same argument also works if the uniform prior Π is replaced by any symmetric prior about 1 2 / .\n\nProof of Theorem 5. Combining (21) and (22), it can be deduced via the triangle inequality that\n\n<!-- formula-not-decoded -->\n\nwith probability at least 1 -δ . Next, by (74) and continuity of X , we have that\n\n<!-- formula-not-decoded -->\n\nWe use this identity to derive lower bounds on P ( t L ) and P ( t R ) over split points s ∈ [ a + c, b -c ] with c = b -a 2 Γ 2 . Now, from (61), we have that\n\n<!-- formula-not-decoded -->\n\nand hence P ( t L ) = P [ X ≤ s | X ∈ t ] ≥ P [ X ≤ Q p ( L ) | X ∈ t ] = p L . Furthermore,\n\n<!-- formula-not-decoded -->\n\nand hence P ( t R ) = P [ X &gt; s | X ∈ t ] ≥ P [ X &gt; Q (1 -p R ) | X ∈ t ] = p R . We deduce that if s ∈ [ a + c, b -c ] , then\n\n<!-- formula-not-decoded -->\n\nBy the assumption that t is independent of the training data D n and Lemma 9 in Appendix A, given N ( t ) , N ( t ) ̂ P ( t L ) is distributed Bin( N ( t ) , P ( t L )) and N ( t ) ̂ P ( t R ) is\n\ndistributed Bin( N ( t ) , P ( t R )) (the features of X need not be independent for this to hold). If W ∼ X | X ∈ t , then given N ( t ) , ̂ P ( t L ) has the same distribution as the empirical distribution function of a sample { W i } N ( t ) i =1 from the distribution of W . Hence by the Dvoretzky-Kiefer-Wolfowitz inequality [31], it can be shown that with probability at least 1 -2 exp {-2 Nglyph[epsilon1] 2 p 2 } , uniformly over all split points s ∈ [ a + c, b -c ] ,\n\n<!-- formula-not-decoded -->\n\nwhere glyph[epsilon1] ∈ (0 , 1 2) / . In particular, for t L = ˆ t L and t R = ˆ t R , with probability at least\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nSince ̂ P ( t L ) = N ( t L ) /N ( t ) and ̂ P ( t R ) = N ( t R ) /N ( t ) , the quantities in (63) are interpretable as lower bounds on the fraction of data points in the optimal daughter node ( ˆ t L and ˆ t R ) that are contained in the parent node t . A consequence of this analysis is that if Λ is not too small, this fraction is non-negligible.\n\nProof of Theorem 6. The proof is similar to Theorem 5, although we must use a stronger concentration inequality to control an empirical process over a collection of nodes. That is, the proof is based on simultaneous control of the empirical processes\n\n<!-- formula-not-decoded -->\n\nwhere T is the collection of all nodes in d dimensions. To this end, define P ( t ) = P [ X ∈ t ] and P s, ( t ) = P [ X ∈ t , X ≤ s ] so that P ( t L ) = P s, ( t ) /P ( t ) . We make use of the inequality\n\n<!-- formula-not-decoded -->\n\nwhich can be deduced from the triangle inequality. We would like to obtain an upper bound on the probability that\n\n<!-- formula-not-decoded -->\n\nfor glyph[epsilon1] = 1 2 / . On an event with probability at least 1 -δ , it holds that N ( t ) ≥ nα and P ( ˆ t L ) ≥ p L and P ( ˆ t R ) ≥ p R (using (62) from the proof of Theorem 5), and hence (65) is contained in the event\n\n<!-- formula-not-decoded -->\n\nwith probability at least 1 -δ , where p = min { p L , p R } . Using (64), this event is also contained in\n\n<!-- formula-not-decoded -->\n\nwith probability at least 1 -δ . By Lemma 6, each event above has probability at most 8( n 2 d + 1)exp {-nglyph[epsilon1] 2 α p / 2 2 128 } for a total probability of at most 16( n 2 d + 1) exp {-nglyph[epsilon1] 2 α p / 2 2 128 } . The proof is completed by choosing glyph[epsilon1] = 1 2 / .\n\nLemma 6. Let T be the set of all nodes in R d . Let X and { X i } n i =1 be i.i.d. random vectors in R d . Then for all glyph[epsilon1] &gt; 0 ,\n\n<!-- formula-not-decoded -->\n\nwhere s ( T , n ) ≤ n 2 d +1 .\n\nProof of Lemma 6. This follows from [44] and the fact that the VC-dimension of T is 2 d .",
        "metadata": {
            "section_header": "A Proofs of main lemmas",
            "title": "Analyzing CART",
            "type": "paper"
        }
    },
    {
        "page_content": "## B Proofs of example regression functions\n\nIn this subsection, we give proofs of Example 1, Example 2, and Example 3 from Section 6.4 and Example 4 from Section 10.\n\nProof of Example 1. Without loss of generality, we will prove the theorem when f ( x ) = x k . The objective function ∆( ; s t ) can be expressed as\n\n<!-- formula-not-decoded -->\n\nNote that √ ∆( s ∗ ; t ) is at least\n\n<!-- formula-not-decoded -->\n\nand the derivative of the partial dependence function at s ∗ is\n\n<!-- formula-not-decoded -->\n\nThus, by (41),\n\n<!-- formula-not-decoded -->\n\nThe penultimate line above follows from the inequality ( k +1 j +2 ) = k k ( +1) ( j +1)( +2) j ( k -1 j ) ≥ ( k -1 j ) for j = 0 1 , , . . . , k -1 and the binomial theorem. Since a &lt; b is arbitrary, it follows that Λ ≥ ( 1 k k ( +1) ) 2 3 / .\n\nRemark 15. If a = 0 and b = 1 , note that if s = k/ k ( +1) , then √ ∆( ; s t ) ∼ (1 -e -1 ) / √ k as k → + ∞ . Since sup s ∈ [0 1] , | F ′ ( s ; t ) | ≤ k , this shows that λ ([0 , 1]) ≥ (1 -e -1 ) 2 3 / /k . Although we have not given a formal proof, this calculation provides evidence that Λ = Ω(1 /k ) .\n\nProof of Example 2. Without loss of generality, we will prove the theorem when f ( x ) = sin(2 πmx ) . In this case, √ ∆( ; s t ) is equal to\n\n<!-- formula-not-decoded -->\n\nNote the term which is equal to the difference of cos(2 πms ) and the line segment between cos(2 πma ) and cos(2 πmb ) at s , or the so-called 'Jensen gap'. 15 A major task of the proof is in choosing a suitable s so that the Jensen gap is large. Next, note that\n\n<!-- formula-not-decoded -->\n\nwhich combines a pointwise inequality of one and the total variation inequality (33) from Lemma 2.\n\nWe break the proof into two parts, depending on whether a -b ≤ 1 (4 / m ) or a -b &gt; 1 (4 / m ) .\n\nCase I: a -b ≤ 1 (4 / m ) Suppose that b -a ≤ 1 (4 / m ) and that sin(2 πms ) is monotone on [ a, b ] . Consider √ ∆( ; s t ) at s = ( a + ) 2 b / . Then it can be shown that\n\n<!-- formula-not-decoded -->\n\nSince sin(2 πms ) is monotone on [ a, b ] , it follows that its total variation is equal to | sin(2 πmb ) -sin(2 πma ) | and hence\n\n<!-- formula-not-decoded -->\n\n15 Typically, this terminology is reserved for convex or concave functions, but we nevertheless use it here.\n\nCombining the estimates (67) and (68) and using | sin( z ) | ≤ 2 sin( | z/ 2) | for z ∈ [0 , 2 π ] and sin( z ) ≥ (2 √ 2 /π z ) for z ∈ [0 , π/ 4] , we conclude from (34) that\n\n<!-- formula-not-decoded -->\n\nThus, λ ≥ 1 2 π 2 . Next, suppose that sin(2 πms ) is neither increasing or decreasing on [ a, b ] . This means that for some positive integer k , the point s ′ = (2 k -1) / (4 m ) belongs to [ a, b ] . Thus, there are choices s ∈ [ a, b ] such that\n\n<!-- formula-not-decoded -->\n\nSince √ ( s -a )( b -s ) ≤ ( b -a / ) 2 , it follows from (34) that √ ∆( ; s t ) ≥ ( C/π m ) 2 ( b -a ) 2 . Moreover, using the total variation bound in (66),\n\n<!-- formula-not-decoded -->\n\nwhere the penultimate line follows from a Taylor expansion argument. Thus, by (34), λ 1 2 / ≥ ( C/π m ) 2 ( b -a ) 2 2 π m 2 2 ( b -a ) 2 = C/ (2 π 3 ) and hence λ ≥ C / 2 (4 π 6 ) .\n\nCase II: a -b &gt; 1 (4 / m ) Next, we investigate when b -a &gt; 1 (4 / m ) . In this regime, cos(2 πms ) is allowed to make at least a quarter period on [ a, b ] . This means that there exists a universal constant C &gt; 0 and s ∈ [ a, b ] such that the Jensen gap\n\n<!-- formula-not-decoded -->\n\nand hence √ ∆( ; s t ) ≥ C/m . Furthermore, sup s ∈ [ a,b ] | F ′ ( s ; t ) | ≤ 2 πm . Thus, by (41), λ ≥ ( C / π m 2 ( 2 4 )) 1 3 / .\n\nRemark 16. If a = 0 and b = 1 , and s = 1 /m then √ ∆( ; s t ) = 1 2 π √ 1 -1 /m m . Furthermore, sup s ∈ [0 1] , | G s ( ; t ) | ≤ 1 . Together these estimates imply that λ ([0 , 1]) ≥ 1 -1 /m 4 π m 2 . As with the case of polynomials (c.f., Remark 15), it is likely that the bound is improvable to Λ = Ω(1 /m ) , and we leave its proof as an open question for future investigation.\n\nLemma 7. Suppose f ( x ) = ( x -1 2) / 2 . Then Λ ≥ ( 1 12 ) 2 3 / .\n\nProof. In this case,\n\n<!-- formula-not-decoded -->\n\nThe derivative of the partial dependence function is F ′ ( s ; t ) = 2 s -1 . If b ≤ 1 2 / or a ≥ 1 2 / , then a lower bound on λ is easy to state. In this case, | F ′ ( s ; t ) | ≤ | a + b - | 1 .\n\nChoosing s = ( a + ) 2 b / , we have from (41) that\n\n<!-- formula-not-decoded -->\n\nIf b &gt; 1 2 / or a &lt; 1 2 / , then | F ′ ( s ; t ) | ≤ 2 max 1 2 { / -a, b -1 2 / } and there are two cases to consider for obtaining lower bounds on ∆( ; s t ) .\n\nCase I: a + b ≥ 1 Choose s = a + ( δ b -a ) , where δ &gt; 1 2 / . Then,\n\n<!-- formula-not-decoded -->\n\nLet δ = 1 2 + 1 2 √ 2 . Then, √ ∆( ; s t ) ≥ b -a 12 ( b -1 2) / , and hence by (41), λ ≥ ( 1 12 ) 2 3 / .\n\nCase II: a + b &lt; 1 Choose s = a + ( δ b -a ) , where δ &lt; 1 / 2 . Then,\n\n<!-- formula-not-decoded -->\n\nChoosing δ = 1 2 -1 2 √ 2 yields √ ∆( ; s t ) ≥ b -a 12 (1 / 2 -a ) , and hence (41), λ ≥ ( 1 12 ) 2 3 / .\n\nLemma 8. Suppose f ( x ) = sin( πx x 1 2 ) . Then Λ ≥ 1 2 π 2 .\n\nProof. First, note that E [ Y | X ∈ t , X 1 = ] s is equal to\n\n<!-- formula-not-decoded -->\n\nIn this case,\n\n<!-- formula-not-decoded -->\n\nUsing the total variation bound (66), one can easily show that\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nwhere the last line follows from sin( z ) ≤ 2 sin( z/ 2) for z ∈ [0 , π ] . Using sin( z ) ≥ (2 √ 2 /π z ) for z ∈ [0 , π/ 4] , we conclude from (34) that\n\n<!-- formula-not-decoded -->\n\nThis implies that Λ ≥ 1 2\n\n<!-- formula-not-decoded -->\n\nProof of Example 4. Consider a generic coefficient of β , say β . First, note that we may assume without loss of generality that β &gt; 0 (since otherwise, we can consider P [ Y = -1 | X = x ] = 1 1+ e β 0 + 〈 x , β 〉 ). In this case, √ ∆( s ∗ ; t ) is at least\n\n<!-- formula-not-decoded -->\n\nand\n\n<!-- formula-not-decoded -->\n\nThe above inequality (70) is due to the fact that F ( ; · t ) is decreasing. Combining inequalities (69) and (70) with (41), we have that\n\n<!-- formula-not-decoded -->\n\nWe now consider two cases for evaluating the integral in (71).\n\nCase I: β b ( -a ) ≥ 2 log 2 In this regime,\n\n<!-- formula-not-decoded -->\n\nThus, λ ≥ β -4 3 / .\n\nCase II: β b ( -a ) &lt; 2 log 2 Alternatively, in this regime,\n\n<!-- formula-not-decoded -->\n\nThus, λ ≥ (1 / 8) 2 3 / . In summary, λ ≥ min { β -4 3 / , (1 / 8) 2 3 / } . Since a &lt; b was arbitrary, this shows that Λ ≥ min { β -4 3 / , (1 / 8) 2 3 / } .",
        "metadata": {
            "section_header": "B Proofs of example regression functions",
            "title": "Analyzing CART",
            "type": "paper"
        }
    },
    {
        "page_content": "## C Proof of example distributions\n\nBelow we list the examples from Section 9.1 and give their proofs.\n\n(b) If each X is i.i.d. Beta(2 1) , , then P [ X ≤ s | X ∈ t ] = s 2 -a 2 b 2 -a 2 and Q p ( ) = √ a 2 + ( p b 2 -a 2 ) and hence we can write\n\n<!-- formula-not-decoded -->\n\nBy concavity of the square root function, √ pb 2 +(1 -p a ) 2 ≥ pb +(1 -p a ) . Thus, √ pb 2 +(1 -p a ) 2 -a b -a ≥ p for all a &lt; b and so q 1 ( p ) = p . For the other direction, we can choose q 2 ( p ) = √ p if we can show that √ p a ( + ) b √ pb 2 +(1 -p a ) 2 + a ≤ 1 , or equivalently, that\n\n<!-- formula-not-decoded -->\n\nBy concavity of the square root function, the expression in (72) is at least 4 a 2 (1 -p ) ≥ 0 .\n\n(c) If each X is i.i.d. Beta(1 / , 2 1) , then P [ X ≤ s | X ∈ t ] = √ - √ s a √ b - √ a and Q p ( ) = ( √ a + p ( √ b - √ a )) 2 and hence we can write\n\n<!-- formula-not-decoded -->\n\nThe proof is finished by noting that 0 ≤ √ b - √ a √ b + √ a ≤ 1 for all a &lt; b .",
        "metadata": {
            "section_header": "C Proof of example distributions",
            "title": "Analyzing CART",
            "type": "paper"
        }
    },
    {
        "page_content": "## D Miscellaneous lemmas\n\nLemma 9. Suppose A and B are events and let W and { W i } n i =1 be i.i.d. random variables. If M = ∑ n i =1 ✶ { W i ∈ A } and M ′ = ∑ n i =1 ✶ { W i ∈ A B ∩ } , then M ′ | M = m ∼ Bin( m, P [ W ∈ B | W ∈ A ]) .\n\nProof. Let p A = P [ W ∈ A ] and p AB = P [ W ∈ A ∩ B ] . Note that p AB /p A = P [ W ∈ B | W ∈ A ] . Then\n\n<!-- formula-not-decoded -->\n\nwhich is the mass function of Bin( m, P [ W ∈ B | W ∈ A ]) .\n\nBefore we state the next lemma, let us first introduce some notation. For a function g , we write g x ( -) (resp. g x ( +) ) to denote the left (resp. right) side limits of g at x , i.e., g x ( -) = lim z ↑ x g z ( ) and g x ( +) = lim z ↓ x g z ( ) .\n\nLemma 10. Let F be a distribution function and Q its quantile function, i.e., Q p ( ) = inf { x ∈ R : p ≤ F x ( ) } . Then,\n\n<!-- formula-not-decoded -->\n\nand\n\n<!-- formula-not-decoded -->\n\nFurthermore, if F is continuous and strictly increasing, then all inequalities are equalities.\n\nProof. These are standard facts from probability theory and can be deduced from the Galois inequalities. See, for example, [34, Section 2.5.2].",
        "metadata": {
            "section_header": "D Miscellaneous lemmas",
            "title": "Analyzing CART",
            "type": "paper"
        }
    }
]