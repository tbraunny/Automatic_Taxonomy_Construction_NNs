[
   {
      "page_content": "(trainX, trainy), (testX, testy) = load_data()\ntrainX = np.float32(trainX) / 255.",
      "metadata": {
         "section_header": "Global Variables",
         "type": "python global"
      }
   },
   {
      "page_content": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom tqdm import tqdm\nfrom matplotlib import pyplot as plt\nfrom keras._tf_keras.keras.datasets.mnist import load_data\ndef get_minibatch(batch_size, device):\n\tindices = torch.randperm(trainX.shape[0])[:batch_size]\n\treturn torch.tensor(trainX[indices], dtype=torch.float).reshape(batch_size, -1).to(\n\t\tdevice), torch.nn.functional.one_hot(torch.tensor(trainy[indices], dtype=torch.long), num_classes=10).to(\n\t\tdevice).type(torch.float)\ndef sample_noise(size, device, dim=100):\n\treturn torch.rand((size, dim), device=device)\nclass Generator(nn.Module):\n\tdef __init__(self, latent_dim=100, context_dim=10, output_dim=28 * 28):\n\t\tsuper(Generator, self).__init__()\n\t\tself.hidden1_z = nn.Sequential(nn.Linear(latent_dim, 200), nn.Dropout(p=0.5), nn.ReLU(), )\n\t\tself.hidden1_context = nn.Sequential(nn.Linear(context_dim, 1000), nn.Dropout(p=0.5), nn.ReLU(), )\n\t\tself.hidden2 = nn.Sequential(nn.Linear(1200, 1200), nn.Dropout(p=0.5), nn.ReLU(), )\n\t\tself.out_layer = nn.Sequential(nn.Linear(1200, output_dim), nn.Sigmoid(), )\n\tdef forward(self, noise, context):\n\t\th = torch.cat((self.hidden1_z(noise), self.hidden1_context(context)), dim=1)\n\t\th = self.hidden2(h)\n\t\treturn self.out_layer(h)\nclass Discriminator(nn.Module):\n\tdef __init__(self, input_dim=28 * 28, context_dim=10):\n\t\tsuper(Discriminator, self).__init__()\n\t\tself.hidden1_x = nn.Sequential(nn.Linear(input_dim, 240), nn.Dropout(p=0.5), nn.LeakyReLU(), )\n\t\tself.hidden1_context = nn.Sequential(nn.Linear(context_dim, 50), nn.Dropout(p=0.5), nn.LeakyReLU(), )\n\t\tself.hidden2 = nn.Sequential(nn.Linear(290, 240), nn.Dropout(p=0.5), nn.LeakyReLU(), )\n\t\tself.out_layer = nn.Sequential(nn.Linear(240, 1), nn.Sigmoid(), )\n\tdef forward(self, x, context):\n\t\th = torch.cat((self.hidden1_x(x), self.hidden1_context(context)), dim=1)\n\t\th = self.hidden2(h)\n\t\treturn self.out_layer(h)\ndef train(generator, discriminator, generator_optimizer, discriminator_optimizer, schedulers, nb_epochs, k=1,\n\t\t  batch_size=100):\n\ttraining_loss = {'generative': [], 'discriminator': []}\n\tfor epoch in tqdm(range(nb_epochs)):\n\t\t### Train the disciminator\n\t\tfor _ in range(k):\n\t\t\t# Sample a minibatch of m noise samples\n\t\t\tz = sample_noise(batch_size, device)\n\t\t\t# Sample a minibatch of m examples from the data generating distribution\n\t\t\tx, label = get_minibatch(batch_size, device)\n\t\t\t# Update the discriminator by ascending its stochastic gradient\n\t\t\tf_loss = torch.nn.BCELoss()(discriminator(generator(z, label), label).reshape(batch_size),\n\t\t\t\t\t\t\t\t\t\ttorch.zeros(batch_size, device=device))\n\t\t\tr_loss = torch.nn.BCELoss()(discriminator(x, label).reshape(batch_size),\n\t\t\t\t\t\t\t\t\t\ttorch.ones(batch_size, device=device))\n\t\t\tloss = (r_loss + f_loss) / 2\n\t\t\tdiscriminator_optimizer.zero_grad()\n\t\t\tloss.backward()\n\t\t\tdiscriminator_optimizer.step()\n\t\t\ttraining_loss['discriminator'].append(loss.item())\n\t\t### Train the generator\n\t\t# Sample a minibatch of m noise samples\n\t\tz = sample_noise(batch_size, device)\n\t\t_, label = get_minibatch(batch_size, device)\n\t\t# Update the generator by descending its stochastic gradient\n\t\tloss = torch.nn.BCELoss()(discriminator(generator(z, label), label).reshape(batch_size),\n\t\t\t\t\t\t\t\t  torch.ones(batch_size, device=device))\n\t\tgenerator_optimizer.zero_grad()\n\t\tloss.backward()\n\t\tgenerator_optimizer.step()\n\t\ttraining_loss['generative'].append(loss.item())\n\t\tfor scheduler in schedulers:\n\t\t\tscheduler.step()\n\treturn training_loss\nif __name__ == \"__main__\":\n\tdevice = 'cuda:0'\n\tdiscriminator = Discriminator().to(device)\n\tgenerator = Generator().to(device)\n\toptimizer_d = optim.SGD(discriminator.parameters(), lr=0.1, momentum=0.5)\n\toptimizer_g = optim.SGD(generator.parameters(), lr=0.1, momentum=0.5)\n\tschedulers = [torch.optim.lr_scheduler.ExponentialLR(optimizer_d, 1 / 1.00004),\n\t\t\t\t  torch.optim.lr_scheduler.ExponentialLR(optimizer_g, 1 / 1.00004)]\n\tloss = train(generator, discriminator, optimizer_g, optimizer_d, schedulers, 287828, batch_size=100)\n\tplt.figure(figsize=(12, 12))\n\tNB_IMAGES = 10\n\tfor i in range(10):\n\t\tz = sample_noise(NB_IMAGES, device)\n\t\tcontext = torch.nn.functional.one_hot(torch.ones(NB_IMAGES, dtype=torch.long) * i, num_classes=10).to(\n\t\t\tdevice).type(torch.float)\n\t\tx = generator(z, context)\n\t\tfor j in range(NB_IMAGES):\n\t\t\tplt.subplot(10, 10, 10 * i + 1 + j)\n\t\t\tplt.axis('off')\n\t\t\tplt.imshow(x[j].data.cpu().numpy().reshape(28, 28), cmap='gray')\n\tplt.savefig('Imgs/cgan.png')",
      "metadata": {
         "section_header": "Global Other",
         "type": "python global"
      }
   },
   {
      "page_content": "def get_minibatch(batch_size, device):\n\tindices = torch.randperm(trainX.shape[0])[:batch_size]\n\treturn torch.tensor(trainX[indices], dtype=torch.float).reshape(batch_size, -1).to(\n\t\tdevice), torch.nn.functional.one_hot(torch.tensor(trainy[indices], dtype=torch.long), num_classes=10).to(\n\t\tdevice).type(torch.float)",
      "metadata": {
         "section_header": "get_minibatch",
         "type": "python function"
      }
   },
   {
      "page_content": "def sample_noise(size, device, dim=100):\n\treturn torch.rand((size, dim), device=device)",
      "metadata": {
         "section_header": "sample_noise",
         "type": "python function"
      }
   },
   {
      "page_content": "Functions: __init__, forward",
      "metadata": {
         "section_header": "Generator",
         "type": "python class"
      }
   },
   {
      "page_content": "\tdef __init__(self, latent_dim=100, context_dim=10, output_dim=28 * 28):\n\t\tsuper(Generator, self).__init__()\n\t\tself.hidden1_z = nn.Sequential(nn.Linear(latent_dim, 200), nn.Dropout(p=0.5), nn.ReLU(), )\n\t\tself.hidden1_context = nn.Sequential(nn.Linear(context_dim, 1000), nn.Dropout(p=0.5), nn.ReLU(), )\n\t\tself.hidden2 = nn.Sequential(nn.Linear(1200, 1200), nn.Dropout(p=0.5), nn.ReLU(), )\n\t\tself.out_layer = nn.Sequential(nn.Linear(1200, output_dim), nn.Sigmoid(), )",
      "metadata": {
         "section_header": "__init__",
         "type": "python function"
      }
   },
   {
      "page_content": "\tdef forward(self, noise, context):\n\t\th = torch.cat((self.hidden1_z(noise), self.hidden1_context(context)), dim=1)\n\t\th = self.hidden2(h)\n\t\treturn self.out_layer(h)",
      "metadata": {
         "section_header": "forward",
         "type": "python function"
      }
   },
   {
      "page_content": "Functions: __init__, forward",
      "metadata": {
         "section_header": "Discriminator",
         "type": "python class"
      }
   },
   {
      "page_content": "\tdef __init__(self, input_dim=28 * 28, context_dim=10):\n\t\tsuper(Discriminator, self).__init__()\n\t\tself.hidden1_x = nn.Sequential(nn.Linear(input_dim, 240), nn.Dropout(p=0.5), nn.LeakyReLU(), )\n\t\tself.hidden1_context = nn.Sequential(nn.Linear(context_dim, 50), nn.Dropout(p=0.5), nn.LeakyReLU(), )\n\t\tself.hidden2 = nn.Sequential(nn.Linear(290, 240), nn.Dropout(p=0.5), nn.LeakyReLU(), )\n\t\tself.out_layer = nn.Sequential(nn.Linear(240, 1), nn.Sigmoid(), )",
      "metadata": {
         "section_header": "__init__",
         "type": "python function"
      }
   },
   {
      "page_content": "\tdef forward(self, x, context):\n\t\th = torch.cat((self.hidden1_x(x), self.hidden1_context(context)), dim=1)\n\t\th = self.hidden2(h)\n\t\treturn self.out_layer(h)",
      "metadata": {
         "section_header": "forward",
         "type": "python function"
      }
   },
   {
      "page_content": "def train(generator, discriminator, generator_optimizer, discriminator_optimizer, schedulers, nb_epochs, k=1,\n\t\t  batch_size=100):\n\ttraining_loss = {'generative': [], 'discriminator': []}\n\tfor epoch in tqdm(range(nb_epochs)):\n\t\t### Train the disciminator\n\t\tfor _ in range(k):\n\t\t\t# Sample a minibatch of m noise samples\n\t\t\tz = sample_noise(batch_size, device)\n\t\t\t# Sample a minibatch of m examples from the data generating distribution\n\t\t\tx, label = get_minibatch(batch_size, device)\n\t\t\t# Update the discriminator by ascending its stochastic gradient\n\t\t\tf_loss = torch.nn.BCELoss()(discriminator(generator(z, label), label).reshape(batch_size),\n\t\t\t\t\t\t\t\t\t\ttorch.zeros(batch_size, device=device))\n\t\t\tr_loss = torch.nn.BCELoss()(discriminator(x, label).reshape(batch_size),\n\t\t\t\t\t\t\t\t\t\ttorch.ones(batch_size, device=device))\n\t\t\tloss = (r_loss + f_loss) / 2\n\t\t\tdiscriminator_optimizer.zero_grad()\n\t\t\tloss.backward()\n\t\t\tdiscriminator_optimizer.step()\n\t\t\ttraining_loss['discriminator'].append(loss.item())\n\t\t### Train the generator\n\t\t# Sample a minibatch of m noise samples\n\t\tz = sample_noise(batch_size, device)\n\t\t_, label = get_minibatch(batch_size, device)\n\t\t# Update the generator by descending its stochastic gradient\n\t\tloss = torch.nn.BCELoss()(discriminator(generator(z, label), label).reshape(batch_size),\n\t\t\t\t\t\t\t\t  torch.ones(batch_size, device=device))\n\t\tgenerator_optimizer.zero_grad()\n\t\tloss.backward()\n\t\tgenerator_optimizer.step()\n\t\ttraining_loss['generative'].append(loss.item())\n\t\tfor scheduler in schedulers:\n\t\t\tscheduler.step()\n\treturn training_loss",
      "metadata": {
         "section_header": "train",
         "type": "python function"
      }
   }
]