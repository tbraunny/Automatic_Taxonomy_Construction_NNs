[
    {
        "page_content": "## Gao Huang ∗ Cornell University",
        "metadata": {
            "section_header": "Gao Huang ∗ Cornell University",
            "title": "Densely Connected Convolutional Networks",
            "type": "paper"
        }
    },
    {
        "page_content": "## Zhuang Liu ∗ Tsinghua University\n\nLaurens van der Maaten Facebook AI Research gh349@cornell.edu\n\nliuzhuang13@mails.tsinghua.edu.cn",
        "metadata": {
            "section_header": "Zhuang Liu ∗ Tsinghua University",
            "title": "Densely Connected Convolutional Networks",
            "type": "paper"
        }
    },
    {
        "page_content": "## Kilian Q. Weinberger Cornell University\n\nkqw4@cornell.edu",
        "metadata": {
            "section_header": "Kilian Q. Weinberger Cornell University",
            "title": "Densely Connected Convolutional Networks",
            "type": "paper"
        }
    },
    {
        "page_content": "## Abstract\n\nRecent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections-one between each layer and its subsequent layer-our network has L L ( +1) 2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet .",
        "metadata": {
            "section_header": "Abstract",
            "title": "Densely Connected Convolutional Networks",
            "type": "paper"
        }
    },
    {
        "page_content": "## 1. Introduction\n\nConvolutional neural networks (CNNs) have become the dominant machine learning approach for visual object recognition. Although they were originally introduced over 20 years ago [18], improvements in computer hardware and network structure have enabled the training of truly deep CNNs only recently. The original LeNet5 [19] consisted of 5 layers, VGG featured 19 [29], and only last year Highway\n\n∗ Authors contributed equally\n\nFigure 1: A 5-layer dense block with a growth rate of k = 4 . Each layer takes all preceding feature-maps as input.\n\n<!-- image -->\n\nNetworks [34] and Residual Networks (ResNets) [11] have surpassed the 100-layer barrier.\n\nAs CNNs become increasingly deep, a new research problem emerges: as information about the input or gradient passes through many layers, it can vanish and 'wash out' by the time it reaches the end (or beginning) of the network. Many recent publications address this or related problems. ResNets [11] and Highway Networks [34] bypass signal from one layer to the next via identity connections. Stochastic depth [13] shortens ResNets by randomly dropping layers during training to allow better information and gradient flow. FractalNets [17] repeatedly combine several parallel layer sequences with different number of convolutional blocks to obtain a large nominal depth, while maintaining many short paths in the network. Although these different approaches vary in network topology and training procedure, they all share a key characteristic: they create short paths from early layers to later layers.\n\nlvdmaaten@fb.com\n\nIn this paper, we propose an architecture that distills this insight into a simple connectivity pattern: to ensure maximum information flow between layers in the network, we connect all layers (with matching feature-map sizes) directly with each other. To preserve the feed-forward nature, each layer obtains additional inputs from all preceding layers and passes on its own feature-maps to all subsequent layers. Figure 1 illustrates this layout schematically. Crucially, in contrast to ResNets, we never combine features through summation before they are passed into a layer; instead, we combine features by concatenating them. Hence, the glyph[lscript] th layer has glyph[lscript] inputs, consisting of the feature-maps of all preceding convolutional blocks. Its own feature-maps are passed on to all L -glyph[lscript] subsequent layers. This introduces L L ( +1) 2 connections in an L -layer network, instead of just L , as in traditional architectures. Because of its dense connectivity pattern, we refer to our approach as Dense Convolutional Network (DenseNet) .\n\nA possibly counter-intuitive effect of this dense connectivity pattern is that it requires fewer parameters than traditional convolutional networks, as there is no need to relearn redundant feature-maps. Traditional feed-forward architectures can be viewed as algorithms with a state, which is passed on from layer to layer. Each layer reads the state from its preceding layer and writes to the subsequent layer. It changes the state but also passes on information that needs to be preserved. ResNets [11] make this information preservation explicit through additive identity transformations. Recent variations of ResNets [13] show that many layers contribute very little and can in fact be randomly dropped during training. This makes the state of ResNets similar to (unrolled) recurrent neural networks [21], but the number of parameters of ResNets is substantially larger because each layer has its own weights. Our proposed DenseNet architecture explicitly differentiates between information that is added to the network and information that is preserved. DenseNet layers are very narrow ( e.g. , 12 filters per layer), adding only a small set of feature-maps to the 'collective knowledge' of the network and keep the remaining featuremaps unchanged-and the final classifier makes a decision based on all feature-maps in the network.\n\nBesides better parameter efficiency, one big advantage of DenseNets is their improved flow of information and gradients throughout the network, which makes them easy to train. Each layer has direct access to the gradients from the loss function and the original input signal, leading to an implicit deep supervision [20]. This helps training of deeper network architectures. Further, we also observe that dense connections have a regularizing effect, which reduces overfitting on tasks with smaller training set sizes.\n\nWe evaluate DenseNets on four highly competitive benchmark datasets (CIFAR-10, CIFAR-100, SVHN, and ImageNet). Our models tend to require much fewer param- eters than existing algorithms with comparable accuracy. Further, we significantly outperform the current state-ofthe-art results on most of the benchmark tasks.",
        "metadata": {
            "section_header": "Introduction",
            "title": "Densely Connected Convolutional Networks",
            "type": "paper"
        }
    },
    {
        "page_content": "## 3. DenseNets\n\nConsider a single image x 0 that is passed through a convolutional network. The network comprises L layers, each of which implements a non-linear transformation H glyph[lscript] ( ) · , where glyph[lscript] indexes the layer. H glyph[lscript] ( ) · can be a composite function of operations such as Batch Normalization (BN) [14], rectified linear units (ReLU) [6], Pooling [19], or Convolution (Conv). We denote the output of the glyph[lscript] th layer as x glyph[lscript] .\n\nResNets. Traditional convolutional feed-forward networks connect the output of the glyph[lscript] th layer as input to the ( glyph[lscript] + 1) th layer [16], which gives rise to the following layer transition: x glyph[lscript] = H glyph[lscript] ( x glyph[lscript] -1 ) . ResNets [11] add a skip-connection that bypasses the non-linear transformations with an identity function:\n\n<!-- formula-not-decoded -->\n\nAn advantage of ResNets is that the gradient can flow directly through the identity function from later layers to the earlier layers. However, the identity function and the output of H glyph[lscript] are combined by summation, which may impede the information flow in the network.\n\nDense connectivity. To further improve the information flow between layers we propose a different connectivity pattern: we introduce direct connections from any layer to all subsequent layers. Figure 1 illustrates the layout of the resulting DenseNet schematically. Consequently, the glyph[lscript] th layer receives the feature-maps of all preceding layers, x 0 , . . . , x glyph[lscript] -1 , as input:\n\n<!-- formula-not-decoded -->\n\nwhere [ x 0 , x 1 , . . . , x glyph[lscript] -1 ] refers to the concatenation of the feature-maps produced in layers 0 , . . . , glyph[lscript] -1 . Because of its dense connectivity we refer to this network architecture as Dense Convolutional Network (DenseNet) . For ease of implementation, we concatenate the multiple inputs of H glyph[lscript] ( ) · in eq. (2) into a single tensor.\n\nComposite function. Motivated by [12], we define H glyph[lscript] ( ) · as a composite function of three consecutive operations: batch normalization (BN) [14], followed by a rectified linear unit (ReLU) [6] and a 3 × 3 convolution (Conv).\n\nPooling layers. The concatenation operation used in Eq. (2) is not viable when the size of feature-maps changes. However, an essential part of convolutional networks is down-sampling layers that change the size of feature-maps. To facilitate down-sampling in our architecture we divide the network into multiple densely connected dense blocks ; see Figure 2. We refer to layers between blocks as transition layers , which do convolution and pooling. The transition layers used in our experiments consist of a batch normalization layer and an 1 × 1 convolutional layer followed by a 2 × 2 average pooling layer.\n\nGrowth rate. If each function H glyph[lscript] produces k featuremaps, it follows that the glyph[lscript] th layer has k 0 + k × ( glyph[lscript] -1) input feature-maps, where k 0 is the number of channels in the input layer. An important difference between DenseNet and existing network architectures is that DenseNet can have very narrow layers, e.g. , k = 12 . We refer to the hyperparameter k as the growth rate of the network. We show in Section 4 that a relatively small growth rate is sufficient to\n\nTable 1: DenseNet architectures for ImageNet. The growth rate for all the networks is k = 32 . Note that each 'conv' layer shown in the table corresponds the sequence BN-ReLU-Conv.\n\n| Layers           | Output Size                    | DenseNet-121                   | DenseNet-121                   | DenseNet-169                   | DenseNet-169                   | DenseNet-201                   | DenseNet-201                   | DenseNet-264                   | DenseNet-264                   |\n|------------------|--------------------------------|--------------------------------|--------------------------------|--------------------------------|--------------------------------|--------------------------------|--------------------------------|--------------------------------|--------------------------------|\n| Convolution      | 112 × 112                      | 7 × 7 conv, stride 2           | 7 × 7 conv, stride 2           | 7 × 7 conv, stride 2           | 7 × 7 conv, stride 2           | 7 × 7 conv, stride 2           | 7 × 7 conv, stride 2           | 7 × 7 conv, stride 2           | 7 × 7 conv, stride 2           |\n| Pooling          | 56 × 56                        | 3 × 3 max pool, stride 2       | 3 × 3 max pool, stride 2       | 3 × 3 max pool, stride 2       | 3 × 3 max pool, stride 2       | 3 × 3 max pool, stride 2       | 3 × 3 max pool, stride 2       | 3 × 3 max pool, stride 2       | 3 × 3 max pool, stride 2       |\n| Dense Block (1)  | 56 × 56                        | [ 1 × 1 conv 3 × 3 conv        | × 6                            | [ 1 × 1 conv 3 × 3 conv        | × 6                            | [ 1 × 1 conv 3 × 3 conv        | × 6                            | [ 1 × 1 conv 3 × 3 conv        | ] × 6                          |\n| Transition Layer | 56 × 56                        | 1 × 1 conv                     | 1 × 1 conv                     | 1 × 1 conv                     | 1 × 1 conv                     | 1 × 1 conv                     | 1 × 1 conv                     | 1 × 1 conv                     | 1 × 1 conv                     |\n| (1)              | 28 × 28                        | 2 × 2 average pool, stride 2   | 2 × 2 average pool, stride 2   | 2 × 2 average pool, stride 2   | 2 × 2 average pool, stride 2   | 2 × 2 average pool, stride 2   | 2 × 2 average pool, stride 2   | 2 × 2 average pool, stride 2   | 2 × 2 average pool, stride 2   |\n| Dense Block (2)  | 28 × 28                        | [ 1 × 1 conv 3 × 3 conv        | × 12                           | [ 1 × 1 conv 3 × 3 conv        | × 12                           | [ 1 × 1 conv 3 × 3 conv        | × 12                           | [ 1 × 1 conv 3 × 3 conv        | ] × 12                         |\n| Transition Layer | 28 × 28                        | 1 × 1 conv                     | 1 × 1 conv                     | 1 × 1 conv                     | 1 × 1 conv                     | 1 × 1 conv                     | 1 × 1 conv                     | 1 × 1 conv                     | 1 × 1 conv                     |\n| (2)              | 14 × 14                        | 2 × 2 average pool, stride 2   | 2 × 2 average pool, stride 2   | 2 × 2 average pool, stride 2   | 2 × 2 average pool, stride 2   | 2 × 2 average pool, stride 2   | 2 × 2 average pool, stride 2   | 2 × 2 average pool, stride 2   | 2 × 2 average pool, stride 2   |\n| Dense Block (3)  | 14 × 14                        | [ 1 × 1 conv 3 × 3 conv        | × 24                           | [ 1 × 1 conv 3 × 3 conv        | × 32                           | [ 1 × 1 conv 3 × 3 conv        | × 48                           | [ 1 × 1 conv 3 × 3 conv        | ] × 64                         |\n| Transition Layer | 14 × 14                        | 1 × 1 conv                     | 1 × 1 conv                     | 1 × 1 conv                     | 1 × 1 conv                     | 1 × 1 conv                     | 1 × 1 conv                     | 1 × 1 conv                     | 1 × 1 conv                     |\n| (3)              | 7 × 7                          | 2 × 2 average pool, stride 2   | 2 × 2 average pool, stride 2   | 2 × 2 average pool, stride 2   | 2 × 2 average pool, stride 2   | 2 × 2 average pool, stride 2   | 2 × 2 average pool, stride 2   | 2 × 2 average pool, stride 2   | 2 × 2 average pool, stride 2   |\n| Dense Block (4)  | 7 × 7                          | [ 1 × 1 conv 3 × 3 conv        | × 16                           | [ 1 × 1 conv 3 × 3 conv        | × 32                           | [ 1 × 1 conv 3 × 3 conv        | × 32                           | [ 1 × 1 conv 3 × 3 conv        | ] × 48                         |\n| Classification   | 1 × 1                          | 7 × 7 global average pool      | 7 × 7 global average pool      | 7 × 7 global average pool      | 7 × 7 global average pool      | 7 × 7 global average pool      | 7 × 7 global average pool      | 7 × 7 global average pool      | 7 × 7 global average pool      |\n| Layer            | 1000D fully-connected, softmax | 1000D fully-connected, softmax | 1000D fully-connected, softmax | 1000D fully-connected, softmax | 1000D fully-connected, softmax | 1000D fully-connected, softmax | 1000D fully-connected, softmax | 1000D fully-connected, softmax | 1000D fully-connected, softmax |\n\nobtain state-of-the-art results on the datasets that we tested on. One explanation for this is that each layer has access to all the preceding feature-maps in its block and, therefore, to the network's 'collective knowledge'. One can view the feature-maps as the global state of the network. Each layer adds k feature-maps of its own to this state. The growth rate regulates how much new information each layer contributes to the global state. The global state, once written, can be accessed from everywhere within the network and, unlike in traditional network architectures, there is no need to replicate it from layer to layer.\n\nBottleneck layers. Although each layer only produces k output feature-maps, it typically has many more inputs. It has been noted in [37, 11] that a 1 × 1 convolution can be introduced as bottleneck layer before each 3 × 3 convolution to reduce the number of input feature-maps, and thus to improve computational efficiency. We find this design especially effective for DenseNet and we refer to our network with such a bottleneck layer, i.e. , to the BN-ReLU-Conv(1 × 1)-BN-ReLU-Conv(3 × 3) version of H glyph[lscript] , as DenseNet-B. In our experiments, we let each 1 × 1 convolution produce 4 k feature-maps.\n\nImplementation Details. On all datasets except ImageNet, the DenseNet used in our experiments has three dense blocks that each has an equal number of layers. Before entering the first dense block, a convolution with 16 (or twice the growth rate for DenseNet-BC) output channels is performed on the input images. For convolutional layers with kernel size 3 × 3, each side of the inputs is zero-padded by one pixel to keep the feature-map size fixed. We use 1 × 1 convolution followed by 2 × 2 average pooling as transition layers between two contiguous dense blocks. At the end of the last dense block, a global average pooling is performed and then a softmax classifier is attached. The feature-map sizes in the three dense blocks are 32 × 32, 16 × 16, and 8 × 8, respectively. We experiment with the basic DenseNet structure with configurations { L = 40 , k = 12 } , { L = 100 , k = 12 } and { L = 100 , k = 24 } . For DenseNetBC, the networks with configurations { L = 100 , k = 12 } , { L =250 , k =24 } and { L =190 , k =40 } are evaluated.\n\nCompression. To further improve model compactness, we can reduce the number of feature-maps at transition layers. If a dense block contains m feature-maps, we let the following transition layer generate glyph[floorleft] θm glyph[floorright] output featuremaps, where 0 &lt;θ ≤ 1 is referred to as the compression factor. When θ =1 , the number of feature-maps across transition layers remains unchanged. We refer the DenseNet with θ&lt; 1 as DenseNet-C, and we set θ = 0 5 . in our experiment. When both the bottleneck and transition layers with θ &lt; 1 are used, we refer to our model as DenseNet-BC.\n\nIn our experiments on ImageNet, we use a DenseNet-BC structure with 4 dense blocks on 224 × 224 input images. The initial convolution layer comprises 2 k convolutions of size 7 × 7 with stride 2; the number of feature-maps in all other layers also follow from setting k . The exact network configurations we used on ImageNet are shown in Table 1.",
        "metadata": {
            "section_header": "DenseNets",
            "title": "Densely Connected Convolutional Networks",
            "type": "paper"
        }
    },
    {
        "page_content": "## 4. Experiments\n\nWeempirically demonstrate DenseNet's effectiveness on several benchmark datasets and compare with state-of-theart architectures, especially with ResNet and its variants.\n\nTable 2: Error rates (%) on CIFAR and SVHN datasets. k denotes network's growth rate. Results that surpass all competing methods are bold and the overall best results are blue . '+' indicates standard data augmentation (translation and/or mirroring). ∗ indicates results run by ourselves. All the results of DenseNets without data augmentation (C10, C100, SVHN) are obtained using Dropout. DenseNets achieve lower error rates while using fewer parameters than ResNet. Without data augmentation, DenseNet performs better by a large margin.\n\n| Method                            | Depth    | Params     | C10             | C10+   | C100          | C100+       | SVHN   |\n|-----------------------------------|----------|------------|-----------------|--------|---------------|-------------|--------|\n| Network in Network [22]           | -        | -          | 10.41           | 8.81   | 35.68         | -           | 2.35   |\n| All-CNN [32]                      | -        | -          | 9.08            | 7.25   | -             | 33.71       | -      |\n| Deeply Supervised Net [20]        | -        | -          | 9.69            | 7.97   | -             | 34.57       | 1.92   |\n| Highway Network [34]              | -        | -          | -               | 7.72   | -             | 32.39       | -      |\n| FractalNet [17]                   | 21       | 38.6M      | 10.18           | 5.22   | 35.34         | 23.30       | 2.01   |\n| with Dropout/Drop-path            | 21       | 38.6M      | 7.33            | 4.60   | 28.20         | 23.73       | 1.87   |\n| ResNet [11]                       | 110      | 1.7M       | -               | 6.61   | -             | -           | -      |\n| ResNet (reported by [13])         | 110      | 1.7M       | 13.63           | 6.41   | 44.74         | 27.22       | 2.01   |\n| ResNet with Stochastic Depth [13] | 110      | 1.7M       | 11.66           | 5.23   | 37.80         | 24.58       | 1.75   |\n| Wide ResNet [42]                  | 16       | 11.0M      | -               | 4.81   | -             | 22.07       | -      |\n| with Dropout                      | 16       | 2.7M       | -               | -      | -             | -           | 1.64   |\n| ResNet (pre-activation) [12]      | 164 1001 | 1.7M 10.2M | 11.26 ∗ 10.56 ∗ | 4.62   | 35.58 33.47 ∗ | 24.33 22.71 | - -    |\n| ( k = 12)                         |          |            |                 | 5.46   | ∗             |             |        |\n| DenseNet 12)                      |          | 1.0M       | 7.00            | 5.24   |               |             |        |\n| DenseNet ( k =                    | 40       |            |                 |        | 27.55         | 24.42       | 1.79   |\n|                                   | 100      | 7.0M       | 5.77            | 4.10   | 23.79         | 20.20       | 1.67   |\n| DenseNet ( k = 24)                | 100      | 27.2M      | 5.83            | 3.74   | 23.42         | 19.25       | 1.59   |\n| DenseNet-BC ( k = 12)             | 100      | 0.8M       | 5.92            | 4.51   | 24.15         | 22.27       | 1.76   |\n| DenseNet-BC ( k = 24)             | 250      | 15.3M      | 5.19            | 3.62   | 19.64         | 17.60       | 1.74   |\n| DenseNet-BC ( k = 40)             | 190      | 25.6M      | -               | 3.46   | -             | 17.18       | -      |",
        "metadata": {
            "section_header": "Experiments",
            "title": "Densely Connected Convolutional Networks",
            "type": "paper"
        }
    },
    {
        "page_content": "## 4.1. Datasets\n\nCIFAR. The two CIFAR datasets [15] consist of colored natural images with 32 × 32 pixels. CIFAR-10 (C10) consists of images drawn from 10 and CIFAR-100 (C100) from 100 classes. The training and test sets contain 50,000 and 10,000 images respectively, and we hold out 5,000 training images as a validation set. We adopt a standard data augmentation scheme (mirroring/shifting) that is widely used for these two datasets [11, 13, 17, 22, 28, 20, 32, 34]. We denote this data augmentation scheme by a '+' mark at the end of the dataset name ( e.g. , C10+). For preprocessing, we normalize the data using the channel means and standard deviations. For the final run we use all 50,000 training images and report the final test error at the end of training.\n\nSVHN. The Street View House Numbers (SVHN) dataset [24] contains 32 × 32 colored digit images. There are 73,257 images in the training set, 26,032 images in the test set, and 531,131 images for additional training. Following common practice [7, 13, 20, 22, 30] we use all the training data without any data augmentation, and a validation set with 6,000 images is split from the training set. We select the model with the lowest validation error during training and report the test error. We follow [42] and divide the pixel values by 255 so they are in the [0 , 1] range.\n\nImageNet. The ILSVRC 2012 classification dataset [2] consists 1.2 million images for training, and 50,000 for validation, from 1 000 , classes. We adopt the same data augmentation scheme for training images as in [8, 11, 12], and apply a single-crop or 10-crop with size 224 × 224 at test time. Following [11, 12, 13], we report classification errors on the validation set.",
        "metadata": {
            "section_header": "Datasets",
            "title": "Densely Connected Convolutional Networks",
            "type": "paper"
        }
    },
    {
        "page_content": "## 4.2. Training\n\nAll the networks are trained using stochastic gradient descent (SGD). On CIFAR and SVHN we train using batch size 64 for 300 and 40 epochs, respectively. The initial learning rate is set to 0.1, and is divided by 10 at 50% and 75% of the total number of training epochs. On ImageNet, we train models for 90 epochs with a batch size of 256. The learning rate is set to 0.1 initially, and is lowered by 10 times at epoch 30 and 60. Note that a naive implementation of DenseNet may contain memory inefficiencies. To reduce the memory consumption on GPUs, please refer to our technical report on the memory-efficient implementation of DenseNets [26].\n\nFollowing [8], we use a weight decay of 10 -4 and a Nesterov momentum [35] of 0.9 without dampening. We adopt the weight initialization introduced by [10]. For the three datasets without data augmentation, i.e. , C10, C100\n\nTable 3: The top-1 and top-5 error rates on the ImageNet validation set, with single-crop / 10crop testing.\n\n| Model        | top-1         | top-5       |\n|--------------|---------------|-------------|\n| DenseNet-121 | 25.02 / 23.61 | 7.71 / 6.66 |\n| DenseNet-169 | 23.80 / 22.08 | 6.85 / 5.92 |\n| DenseNet-201 | 22.58 / 21.46 | 6.34 / 5.54 |\n| DenseNet-264 | 22.15 / 20.80 | 6.12 / 5.29 |\n\n<!-- image -->\n\n<!-- image -->\n\nFigure 3: Comparison of the DenseNets and ResNets top-1 error rates (single-crop testing) on the ImageNet validation dataset as a function of learned parameters ( left ) and FLOPs during test-time ( right ).\n\nand SVHN, we add a dropout layer [33] after each convolutional layer (except the first one) and set the dropout rate to 0.2. The test errors were only evaluated once for each task and model setting.",
        "metadata": {
            "section_header": "Training",
            "title": "Densely Connected Convolutional Networks",
            "type": "paper"
        }
    },
    {
        "page_content": "## 4.3. Classification Results on CIFAR and SVHN\n\nWe train DenseNets with different depths, L , and growth rates, k . The main results on CIFAR and SVHN are shown in Table 2. To highlight general trends, we mark all results that outperform the existing state-of-the-art in boldface and the overall best result in blue .\n\nAccuracy. Possibly the most noticeable trend may originate from the bottom row of Table 2, which shows that DenseNet-BC with L = 190 and k = 40 outperforms the existing state-of-the-art consistently on all the CIFAR datasets. Its error rates of 3.46% on C10+ and 17.18% on C100+ are significantly lower than the error rates achieved by wide ResNet architecture [42]. Our best results on C10 and C100 (without data augmentation) are even more encouraging: both are close to 30% lower than FractalNet with drop-path regularization [17]. On SVHN, with dropout, the DenseNet with L = 100 and k = 24 also surpasses the current best result achieved by wide ResNet. However, the 250-layer DenseNet-BC doesn't further improve the performance over its shorter counterpart. This may be explained by that SVHN is a relatively easy task, and extremely deep models may overfit to the training set.\n\nParameter Efficiency. The results in Table 2 indicate that DenseNets utilize parameters more efficiently than alternative architectures (in particular, ResNets). The DenseNetBC with bottleneck structure and dimension reduction at transition layers is particularly parameter-efficient. For example, our 250-layer model only has 15.3M parameters, but it consistently outperforms other models such as FractalNet and Wide ResNets that have more than 30M parameters. We also highlight that DenseNet-BC with L =100 and k =12 achieves comparable performance ( e.g. , 4.51% vs 4.62% error on C10+, 22.27% vs 22.71% error on C100+) as the 1001-layer pre-activation ResNet using 90% fewer parameters. Figure 4 (right panel) shows the training loss and test errors of these two networks on C10+. The 1001-layer deep ResNet converges to a lower training loss value but a similar test error. We analyze this effect in more detail below.\n\nCapacity. Without compression or bottleneck layers, there is a general trend that DenseNets perform better as L and k increase. We attribute this primarily to the corresponding growth in model capacity. This is best demonstrated by the column of C10+ and C100+. On C10+, the error drops from 5.24% to 4.10% and finally to 3.74% as the number of parameters increases from 1.0M, over 7.0M to 27.2M. On C100+, we observe a similar trend. This suggests that DenseNets can utilize the increased representational power of bigger and deeper models. It also indicates that they do not suffer from overfitting or the optimization difficulties of residual networks [11].\n\nOverfitting. One positive side-effect of the more efficient use of parameters is a tendency of DenseNets to be less prone to overfitting. We observe that on the datasets without data augmentation, the improvements of DenseNet architectures over prior work are particularly pronounced. On C10, the improvement denotes a 29% relative reduction in error from 7.33% to 5.19%. On C100, the reduction is about 30% from 28.20% to 19.64%. In our experiments, we observed potential overfitting in a single setting: on C10, a 4 × growth of parameters produced by increasing k =12 to k =24 lead to a modest increase in error from 5.77% to 5.83%. The DenseNet-BC bottleneck and compression layers appear to be an effective way to counter this trend.",
        "metadata": {
            "section_header": "Classification Results on CIFAR and SVHN",
            "title": "Densely Connected Convolutional Networks",
            "type": "paper"
        }
    },
    {
        "page_content": "## 4.4. Classification Results on ImageNet\n\nWe evaluate DenseNet-BC with different depths and growth rates on the ImageNet classification task, and compare it with state-of-the-art ResNet architectures. To ensure a fair comparison between the two architectures, we eliminate all other factors such as differences in data preprocessing and optimization settings by adopting the publicly available Torch implementation for ResNet by [8] 1 .\n\n1 https://github.com/facebook/fb.resnet.torch\n\n<!-- image -->\n\n<!-- image -->\n\ntest error (%)\n\n×\n\nFigure 4: Left: Comparison of the parameter efficiency on C10+ between DenseNet variations. Middle: Comparison of the parameter efficiency between DenseNet-BC and (pre-activation) ResNets. DenseNet-BC requires about 1/3 of the parameters as ResNet to achieve comparable accuracy. Right: Training and testing curves of the 1001-layer pre-activation ResNet [12] with more than 10M parameters and a 100-layer DenseNet with only 0.8M parameters.\n\n<!-- image -->\n\nWe simply replace the ResNet model with the DenseNetBC network, and keep all the experiment settings exactly the same as those used for ResNet.\n\nWe report the single-crop and 10-crop validation errors of DenseNets on ImageNet in Table 3. Figure 3 shows the single-crop top-1 validation errors of DenseNets and ResNets as a function of the number of parameters (left) and FLOPs (right). The results presented in the figure reveal that DenseNets perform on par with the state-of-the-art ResNets, whilst requiring significantly fewer parameters and computation to achieve comparable performance. For example, a DenseNet-201 with 20M parameters model yields similar validation error as a 101-layer ResNet with more than 40M parameters. Similar trends can be observed from the right panel, which plots the validation error as a function of the number of FLOPs: a DenseNet that requires as much computation as a ResNet-50 performs on par with a ResNet-101, which requires twice as much computation.\n\nIt is worth noting that our experimental setup implies that we use hyperparameter settings that are optimized for ResNets but not for DenseNets. It is conceivable that more extensive hyper-parameter searches may further improve the performance of DenseNet on ImageNet.",
        "metadata": {
            "section_header": "Classification Results on ImageNet",
            "title": "Densely Connected Convolutional Networks",
            "type": "paper"
        }
    },
    {
        "page_content": "## 5. Discussion\n\nSuperficially, DenseNets are quite similar to ResNets: Eq. (2) differs from Eq. (1) only in that the inputs to H glyph[lscript] ( ) · are concatenated instead of summed. However, the implications of this seemingly small modification lead to substantially different behaviors of the two network architectures.\n\nModel compactness. As a direct consequence of the input concatenation, the feature-maps learned by any of the DenseNet layers can be accessed by all subsequent layers. This encourages feature reuse throughout the network, and leads to more compact models.\n\nThe left two plots in Figure 4 show the result of an experiment that aims to compare the parameter efficiency of all variants of DenseNets (left) and also a comparable\n\nResNet architecture (middle). We train multiple small networks with varying depths on C10+ and plot their test accuracies as a function of network parameters. In comparison with other popular network architectures, such as AlexNet [16] or VGG-net [29], ResNets with pre-activation use fewer parameters while typically achieving better results [12]. Hence, we compare DenseNet ( k = 12 ) against this architecture. The training setting for DenseNet is kept the same as in the previous section.\n\nThe graph shows that DenseNet-BC is consistently the most parameter efficient variant of DenseNet. Further, to achieve the same level of accuracy, DenseNet-BC only requires around 1/3 of the parameters of ResNets (middle plot). This result is in line with the results on ImageNet we presented in Figure 3. The right plot in Figure 4 shows that a DenseNet-BC with only 0.8M trainable parameters is able to achieve comparable accuracy as the 1001-layer (pre-activation) ResNet [12] with 10.2M parameters.\n\nImplicit Deep Supervision. One explanation for the improved accuracy of dense convolutional networks may be that individual layers receive additional supervision from the loss function through the shorter connections. One can interpret DenseNets to perform a kind of 'deep supervision'. The benefits of deep supervision have previously been shown in deeply-supervised nets (DSN; [20]), which have classifiers attached to every hidden layer, enforcing the intermediate layers to learn discriminative features.\n\nDenseNets perform a similar deep supervision in an implicit fashion: a single classifier on top of the network provides direct supervision to all layers through at most two or three transition layers. However, the loss function and gradient of DenseNets are substantially less complicated, as the same loss function is shared between all layers.\n\nStochastic vs. deterministic connection. There is an interesting connection between dense convolutional networks and stochastic depth regularization of residual networks [13]. In stochastic depth, layers in residual networks are randomly dropped, which creates direct connections be- tween the surrounding layers. As the pooling layers are never dropped, the network results in a similar connectivity pattern as DenseNet: there is a small probability for any two layers, between the same pooling layers, to be directly connected-if all intermediate layers are randomly dropped. Although the methods are ultimately quite different, the DenseNet interpretation of stochastic depth may provide insights into the success of this regularizer.\n\nFeature Reuse. By design, DenseNets allow layers access to feature-maps from all of its preceding layers (although sometimes through transition layers). We conduct an experiment to investigate if a trained network takes advantage of this opportunity. We first train a DenseNet on C10+ with L = 40 and k = 12 . For each convolutional layer glyph[lscript] within a block, we compute the average (absolute) weight assigned to connections with layer s . Figure 5 shows a heat-map for all three dense blocks. The average absolute weight serves as a surrogate for the dependency of a convolutional layer on its preceding layers. A red dot in position ( glyph[lscript], s ) indicates that the layer glyph[lscript] makes, on average, strong use of feature-maps produced s -layers before. Several observations can be made from the plot:\n\n- 1. All layers spread their weights over many inputs within the same block. This indicates that features extracted by very early layers are, indeed, directly used by deep layers throughout the same dense block.\n- 2. The weights of the transition layers also spread their weight across all layers within the preceding dense block, indicating information flow from the first to the last layers of the DenseNet through few indirections.\n- 3. The layers within the second and third dense block consistently assign the least weight to the outputs of the transition layer (the top row of the triangles), indicating that the transition layer outputs many redundant features (with low weight on average). This is in keeping with the strong results of DenseNet-BC where exactly these outputs are compressed.\n- 4. Although the final classification layer, shown on the very right, also uses weights across the entire dense block, there seems to be a concentration towards final feature-maps, suggesting that there may be some more high-level features produced late in the network.",
        "metadata": {
            "section_header": "Discussion",
            "title": "Densely Connected Convolutional Networks",
            "type": "paper"
        }
    },
    {
        "page_content": "## 6. Conclusion\n\nWe proposed a new convolutional network architecture, which we refer to as Dense Convolutional Network (DenseNet). It introduces direct connections between any two layers with the same feature-map size. We showed that DenseNets scale naturally to hundreds of layers, while exhibiting no optimization difficulties. In our experiments,\n\nFigure 5: The average absolute filter weights of convolutional layers in a trained DenseNet. The color of pixel ( s, glyph[lscript] ) encodes the average L 1 norm (normalized by number of input feature-maps) of the weights connecting convolutional layer s to glyph[lscript] within a dense block. Three columns highlighted by black rectangles correspond to two transition layers and the classification layer. The first row encodes weights connected to the input layer of the dense block.\n\n<!-- image -->\n\nDenseNets tend to yield consistent improvement in accuracy with growing number of parameters, without any signs of performance degradation or overfitting. Under multiple settings, it achieved state-of-the-art results across several highly competitive datasets. Moreover, DenseNets require substantially fewer parameters and less computation to achieve state-of-the-art performances. Because we adopted hyperparameter settings optimized for residual networks in our study, we believe that further gains in accuracy of DenseNets may be obtained by more detailed tuning of hyperparameters and learning rate schedules.\n\nWhilst following a simple connectivity rule, DenseNets naturally integrate the properties of identity mappings, deep supervision, and diversified depth. They allow feature reuse throughout the networks and can consequently learn more compact and, according to our experiments, more accurate models. Because of their compact internal representations and reduced feature redundancy, DenseNets may be good feature extractors for various computer vision tasks that build on convolutional features, e.g. , [4, 5]. We plan to study such feature transfer with DenseNets in future work.\n\nAcknowledgements. The authors are supported in part by the NSF III-1618134, III-1526012, IIS-1149882, the Office of Naval Research Grant N00014-17-1-2175 and the Bill and Melinda Gates foundation. GH is supported by the International Postdoctoral Exchange Fellowship Program of China Postdoctoral Council (No.20150015). ZL is supported by the National Basic Research Program of China Grants 2011CBA00300, 2011CBA00301, the NSFC 61361136003. We also thank Daniel Sedra, Geoff Pleiss and Yu Sun for many insightful discussions.",
        "metadata": {
            "section_header": "Conclusion",
            "title": "Densely Connected Convolutional Networks",
            "type": "paper"
        }
    }
]