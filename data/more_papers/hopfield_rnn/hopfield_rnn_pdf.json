[
    {
        "page_content": "## ABSTRACT\n\nWe introduce a modern Hopfield network with continuous states and a corresponding update rule. The new Hopfield network can store exponentially (with the dimension of the associative space) many patterns, retrieves the pattern with one update, and has exponentially small retrieval errors. It has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. The new update rule is equivalent to the attention mechanism used in transformers. This equivalence enables a characterization of the heads of transformer models. These heads perform in the first layers preferably global averaging and in higher layers partial averaging via metastable states. The new modern Hopfield network can be integrated into deep learning architectures as layers to allow the storage of and access to raw input data, intermediate results, or learned prototypes. These Hopfield layers enable new ways of deep learning, beyond fully-connected, convolutional, or recurrent networks, and provide pooling, memory, association, and attention mechanisms. We demonstrate the broad applicability of the Hopfield layers across various domains. Hopfield layers improved state-of-the-art on three out of four considered multiple instance learning problems as well as on immune repertoire classification with several hundreds of thousands of instances. On the UCI benchmark collections of small classification tasks, where deep learning methods typically struggle, Hopfield layers yielded a new state-ofthe-art when compared to different machine learning methods. Finally, Hopfield layers achieved state-of-the-art on two drug design datasets. The implementation is available at: https://github.com/ml-jku/hopfield-layers",
        "metadata": {
            "section_header": "ABSTRACT",
            "title": "HOPFIELD NETWORKS IS ALL YOU NEED",
            "type": "paper"
        }
    },
    {
        "page_content": "## 1 INTRODUCTION\n\nThe deep learning community has been looking for alternatives to recurrent neural networks (RNNs) for storing information. For example, linear memory networks use a linear autoencoder for sequences as a memory (Carta et al., 2020). Additional memories for RNNs like holographic reduced representations (Danihelka et al., 2016), tensor product representations (Schlag &amp; Schmidhuber, 2018; Schlag et al., 2019) and classical associative memories (extended to fast weight approaches) (Schmidhuber, 1992; Ba et al., 2016a;b; Zhang &amp; Zhou, 2017; Schlag et al., 2021) have been suggested. Most approaches to new memories are based on attention. The neural Turing machine (NTM) is equipped with an external memory and an attention process (Graves et al., 2014). Memory networks (Weston et al., 2014) use an arg max attention by first mapping a query and patterns into a space and then retrieving the pattern with the largest dot product. End to end memory networks (EMN) make this attention scheme differentiable by replacing arg max through a softmax (Sukhbaatar et al., 2015a;b). EMN with dot products became very popular and implement a key-value attention (Daniluk et al., 2017) for self-attention. An enhancement of EMN is the transformer (Vaswani et al., 2017a;b) and its\n\nextensions (Dehghani et al., 2018). The transformer has had a great impact on the natural language processing (NLP) community, in particular via the BERT models (Devlin et al., 2018; 2019).\n\nContribution of this work: (i) introducing novel deep learning layers that are equipped with a memory via modern Hopfield networks, (ii) introducing a novel energy function and a novel update rule for continuous modern Hopfield networks that are differentiable and typically retrieve patterns after one update. Differentiability is required for gradient descent parameter updates and retrieval with one update is compatible with activating the layers of deep networks.\n\nWe suggest using modern Hopfield networks to store information or learned prototypes in different layers of neural networks. Binary Hopfield networks were introduced as associative memories that can store and retrieve patterns (Hopfield, 1982). A query pattern can retrieve the pattern to which it is most similar or an average over similar patterns. Hopfield networks seem to be an ancient technique, however, new energy functions improved their properties. The stability of spurious states or metastable states was sensibly reduced (Barra et al., 2018). The largest and most impactful successes are reported on increasing the storage capacity of Hopfield networks. In a d -dimensional space, the standard Hopfield model can store d uncorrelated patterns without errors but only Cd/ log( d ) random patterns with C &lt; 1 2 / for a fixed stable pattern or C &lt; 1 / 4 if all patterns are stable (McEliece et al., 1987). The same bound holds for nonlinear learning rules (Mazza, 1997). Using tricks-of-trade and allowing small retrieval errors, the storage capacity is about 0 138 . d (Crisanti et al., 1986; Hertz et al., 1991; Torres et al., 2002). If the learning rule is not related to the Hebb rule, then up to d patterns can be stored (Abu-Mostafa &amp; StJacques, 1985). For Hopfield networks with non-zero diagonal matrices, the storage can be increased to Cd log( d ) (Folli et al., 2017). In contrast to the storage capacity, the number of energy minima (spurious states, stable states) of Hopfield networks is exponential in d (Tanaka &amp; Edwards, 1980; Bruck &amp; Roychowdhury, 1990; Wainrib &amp; Touboul, 2013).\n\nThe standard binary Hopfield network has an energy function that can be expressed as the sum of interaction functions F with F x ( ) = x 2 . Modern Hopfield networks, also called 'dense associative memory' (DAM) models, use an energy function with interaction functions of the form F x ( ) = x n and, thereby, achieve a storage capacity proportional to d n -1 (Krotov &amp; Hopfield, 2016; 2018). The energy function of modern Hopfield networks makes them robust against adversarial attacks (Krotov &amp; Hopfield, 2018). Modern binary Hopfield networks with energy functions based on interaction functions of the form F x ( ) = exp( x ) even lead to storage capacity of 2 d/ 2 , where all stored binary patterns are fixed points but the radius of attraction vanishes (Demircigil et al., 2017). However, in order to integrate Hopfield networks into deep learning architectures, it is necessary to make them differentiable, that is, we require continuous Hopfield networks (Hopfield, 1984; Koiran, 1994).\n\nTherefore, we generalize the energy function of Demircigil et al. (2017) that builds on exponential interaction functions to continuous patterns and states and obtain a new modern Hopfield network. We also propose a new update rule which ensures global convergence to stationary points of the energy (local minima or saddle points). We prove that our new modern Hopfield network typically retrieves patterns in one update step ( glyph[epsilon1] -close to the fixed point) with an exponentially low error and has a storage capacity proportional to c d -1 4 (reasonable settings for c = 1 37 . and c = 3 15 . are given in Theorem 3). The retrieval of patterns with one update is important to integrate Hopfield networks in deep learning architectures, where layers are activated only once. Surprisingly, our new update rule is also the key-value attention as used in transformer and BERT models (see Fig. 1). Our modern Hopfield networks can be integrated as a new layer in deep learning architectures for pooling, memory, prototype learning, and attention. We test these new layers on different benchmark datasets and tasks like immune repertoire classification.\n\nFigure 1: We generalize the energy of binary modern Hopfield networks to continuous states while keeping fast convergence and storage capacity properties. We also propose a new update rule that minimizes the energy. The new update rule is the attention mechanism of the transformer. Formulae are modified to express softmax as row vector. ' = '-sign means 'keeps the properties'.\n\n<!-- image -->",
        "metadata": {
            "section_header": "INTRODUCTION",
            "title": "HOPFIELD NETWORKS IS ALL YOU NEED",
            "type": "paper"
        }
    },
    {
        "page_content": "## 2 MODERN HOPFIELD NETS WITH CONTINUOUS STATES\n\nNew energy function for continuous state Hopfield networks. In order to integrate modern Hopfield networks into deep learning architectures, we have to make them continuous. To allow for continuous states, we propose a new energy function that is a modification of the energy of modern Hopfield networks (Demircigil et al., 2017). We also propose a new update rule which can be proven to converge to stationary points of the energy (local minima or saddle points).\n\nWe have N stored (key) patterns x i ∈ R d represented by the matrix X = ( x 1 , . . . , x N ) with the largest pattern M = max i ‖ x i ‖ . The state (query) pattern is ξ ∈ R d . For exponential interaction functions, we need the log-sum-exp function ( lse ) for 0 &lt; β\n\n<!-- formula-not-decoded -->\n\nwhich is convex (see appendix Eq. (461), and Lemma A22). The energy function E of the modern Hopfield networks for binary patterns x i and a binary state pattern ξ is E = -∑ N i =1 F ( ξ T x i ) (Krotov &amp; Hopfield, 2016). Here, F x ( ) = x n is the interaction function, where n = 2 gives the classical Hopfield network. The storage capacity is proportional to d n -1 (Krotov &amp; Hopfield, 2016). This model was generalized by Demircigil et al. (2017) to exponential interaction functions F x ( ) = exp( x ) which gives the energy E = -exp(lse(1 , X ξ T )) . This energy leads to an exponential storage capacity of N = 2 d/ 2 for binary patterns. Furthermore, with a single update, the fixed point is recovered with high probability for random patterns. However, still this modern Hopfield network has binary states.\n\nWe generalize this energy function to continuous-valued patterns while keeping the properties of the modern Hopfield networks like the exponential storage capacity and the extremely fast convergence (see Fig. 1). For the new energy we take the logarithm of the negative energy of modern Hopfield networks and add a quadratic term of the current state. The quadratic term ensures that the norm of the state vector ξ remains finite and the energy is bounded. Classical Hopfield networks do not require to bound the norm of their state vector, since it is binary and has fixed length. We define the novel energy function E as\n\n<!-- formula-not-decoded -->\n\nWe have 0 glyph[lessorequalslant] E glyph[lessorequalslant] 2 M 2 (see appendix Lemma A1). Using p = softmax( β X ξ T ) , we define a novel update rule (see Fig. 1):\n\n<!-- formula-not-decoded -->\n\nThe next theorem states that the update rule Eq. (3) converges globally. The proof uses the ConcaveConvex Procedure (CCCP) (Yuille &amp; Rangarajan, 2002; 2003), which is equivalent to Legendre minimization (Rangarajan et al., 1996; 1999) algorithms (Yuille &amp; Rangarajan, 2003).\n\nTheorem 1. The update rule Eq. (3) converges globally: For ξ t +1 = f ( ξ t ) , the energy E( ξ t ) → E( ξ ∗ ) for t →∞ and a fixed point ξ ∗ .\n\nProof. The update rule in Eq. (3) is the CCCP for minimizing the energy E , which is the sum of the convex 1 2 / ξ T ξ and concave -lse (see details in appendix Theorem 1). Theorem 2 in Yuille &amp; Rangarajan (2002) yields the global convergence property. Also, in Theorem 2 in Sriperumbudur &amp; Lanckriet (2009) the global convergence of CCCP is proven via a rigorous analysis using Zangwill's global convergence theory of iterative algorithms.\n\nThe global convergence theorem only assures that for the energy E( ξ t ) → E( ξ ∗ ) for t →∞ but not ξ t → ξ ∗ . The next theorem strengthens Zangwill's global convergence theorem (Meyer, 1976) and gives convergence results similar to those known for expectation maximization (Wu, 1983).\n\nTheorem 2. For the iteration Eq. (3) we have E( ξ t ) → E( ξ ∗ ) = E ∗ as t → ∞ , for some stationary point ξ ∗ . Furthermore, ∥ ∥ ξ t +1 -ξ t ∥ ∥ → 0 and either { ξ t } ∞ t =0 converges or, in the other case, the set of limit points of { ξ t } ∞ t =0 is a connected and compact subset of L (E ) ∗ , where L ( a ) = { ξ ∈ L | E( ) = ξ a } and L is the set of stationary points of the iteration Eq. (3) . If L (E ) ∗ is finite, then any sequence { ξ t } ∞ t =0 generated by the iteration Eq. (3) converges to some ξ ∗ ∈ L (E ) ∗ .\n\nFor a proof, see appendix Theorem 2. Therefore, all the limit points of any sequence generated by the iteration Eq. (3) are stationary points (local minima or saddle points) of the energy function E . Either the iteration converges or, otherwise, the set of limit points is a connected and compact set.\n\nThe next theorem gives the results on the storage capacity of our new continuous state modern Hopfield network. We first define what we mean by storing and retrieving patterns using a modern Hopfield network with continuous states.\n\nglyph[negationslash]\n\nDefinition 1 (Pattern Stored and Retrieved) . We assume that around every pattern x i a sphere S i is given. We say x i is stored if there is a single fixed point x ∗ i ∈ S i to which all points ξ ∈ S i converge, and S i ∩ S j = ∅ for i = j . We say x i is retrieved for a given glyph[epsilon1] if iteration (update rule) Eq. (3) gives a point ˜ x i that is at least glyph[epsilon1] -close to the single fixed point x ∗ i ∈ S i . The retrieval error is ‖ ˜ x i -x i ‖ .\n\nAs with classical Hopfield networks, we consider patterns on the sphere, i.e. patterns with a fixed norm. For randomly chosen patterns, the number of patterns that can be stored is exponential in the dimension d of the space of the patterns ( x i ∈ R d ).\n\nTheorem 3. We assume a failure probability 0 &lt; p glyph[lessorequalslant] 1 and randomly chosen patterns on the sphere with radius M := K √ d -1 . We define a := 2 d -1 (1 + ln(2 βK p d 2 ( -1))) , b := 2 K β 2 5 , and c := b W 0 (exp( a +ln( )) b , where W 0 is the upper branch of the Lambert W function (Olver et al., 2010,\n\n(4.13)), and ensure c ≥ ( 2 √ p ) 4 d -1 . Then with probability 1 -p , the number of random patterns that can be stored is\n\n<!-- formula-not-decoded -->\n\nTherefore it is proven for c ≥ 3 1546 . with β = 1 , K = 3 , d = 20 and p = 0 001 . ( a +ln( ) b &gt; 1 27 . ) and proven for c ≥ 1 3718 . with β = 1 , K = 1 , d = 75 , and p = 0 001 . ( a +ln( ) b &lt; -0 94 . ).\n\nFor a proof, see appendix Theorem A5.\n\nThe next theorem states that the update rule typically retrieves patterns after one update. Retrieval of a pattern x i for fixed point x ∗ i and query ξ is defined via an glyph[epsilon1] by ‖ f ( ξ ) -x ∗ i ‖ &lt; glyph[epsilon1] , that is, the update is glyph[epsilon1] -close to the fixed point. Retrieval with one update is crucial to integrate modern Hopfield networks into deep learning architectures, where layers are activated only once. First we need the concept of separation of a pattern. For pattern x i we define its separation ∆ i to other patterns by:\n\nglyph[negationslash]\n\nglyph[negationslash]\n\n<!-- formula-not-decoded -->\n\nThe update rule retrieves patterns with one update for well separated patterns, that is, patterns with large ∆ i .\n\nTheorem 4. With query ξ , after one update the distance of the new point f ( ξ ) to the fixed point x ∗ i is exponentially small in the separation ∆ i . The precise bounds using the Jacobian J = ∂f ( ξ ) ∂ ξ and its value J m in the mean value theorem are:\n\n<!-- formula-not-decoded -->\n\nFor given glyph[epsilon1] and sufficient large ∆ i , we have ‖ f ( ξ ) -x ∗ i ‖ &lt; glyph[epsilon1] , that is, retrieval with one update.\n\nSee proof in appendix Theorem A8.\n\nAt the same time, the retrieval error decreases exponentially with the separation ∆ i .\n\nTheorem 5 (Exponentially Small Retrieval Error) . The retrieval error ‖ f ( ξ ) -x i ‖ of pattern x i is bounded by\n\n<!-- formula-not-decoded -->\n\nand for\n\n‖\n\nx\n\ni\n\n-\n\nx\n\n∗\n\ni\n\n‖\n\nglyph[lessorequalslant]\n\n2\n\nβ M\n\ntogether with\n\n‖\n\nx\n\ni\n\n- ‖\n\nξ\n\nglyph[lessorequalslant]\n\n2\n\nβ M\n\nby\n\n<!-- formula-not-decoded -->\n\nSee proof in appendix Theorem A9.\n\n1\n\n1\n\nMetastable states and one global fixed point. So far, we considered patterns x i that are well separated and the iteration converges to a fixed point which is near a pattern x i . If no pattern x i is well separated from the others, then the iteration converges to a global fixed point close to the arithmetic mean of the vectors. In this case the softmax vector p is close to uniform, that is, p i = 1 /N . If some vectors are similar to each other and well separated from all other vectors, then a metastable state near the similar vectors exists. Iterations that start near the metastable state converge to this metastable state, also if initialized by one of the similar patterns. For convergence proofs to one global fixed point and to metastable states see appendix Lemma A7 and Lemma A12, respectively.\n\nHopfield update rule is attention of the transformer. The Hopfield network update rule is the attention mechanism used in transformer and BERT models (see Fig. 1). To see this, we assume N stored (key) patterns y i and S state (query) patterns r i that are mapped to the Hopfield space of dimension d k . We set x i = W y ξ T K i , i = W r T Q i , and multiply the result of our update rule with W V . The matrices Y = ( y 1 , . . . , y N ) T and R = ( r 1 , . . . , r S ) T combine the y i and r i as row vectors. We define the matrices X T = K = Y W K , Ξ T = Q = RW Q , and V = Y W K W V = X W T V , where W K ∈ R d y × d k , W Q ∈ R d r × d k , W V ∈ R d k × d v . If β = 1 / √ d k and softmax ∈ R N is changed to a row vector, we obtain for the update rule Eq. (3) multiplied by W V :\n\n<!-- formula-not-decoded -->\n\nThe left part of Eq. (10) is the transformer attention. In the transformer self-attention R = Y , and W W K V replaced by just W V . Besides the attention mechanism, Hopfield networks allow for other functionalities in deep network architectures, which we introduce via specific layers in the next section. The right part of Eq. (10) serves to explain these specific layers.",
        "metadata": {
            "section_header": "MODERN HOPFIELD NETS WITH CONTINUOUS STATES",
            "title": "HOPFIELD NETWORKS IS ALL YOU NEED",
            "type": "paper"
        }
    },
    {
        "page_content": "## 3 NEW HOPFIELD LAYERS FOR DEEP LEARNING\n\nModern Hopfield networks with continuous states can be integrated into deep learning architectures, because they are continuous and differentiable with respect to their parameters. Furthermore, they typically retrieve patterns with one update, which is conform to deep learning layers that are activated only once. For these two reasons, modern Hopfield networks can serve as specialized layers in deep networks to equip them with memories. Below, we introduce three types of Hopfield layers: Hopfield , HopfieldPooling , and HopfieldLayer . Possible applications of Hopfield layers in deep network architectures comprise:\n\n- · multiple instance learning (MIL) (Dietterich et al., 1997),\n- · processing of and learning with point sets (Qi et al., 2017a;b; Xu et al., 2018),\n- · set-based and permutation invariant learning (Guttenberg et al., 2016; Ravanbakhsh et al., 2016; Zaheer et al., 2017; Korshunova et al., 2018; Ilse et al., 2018; Zhai et al., 2020),\n- · attention-based learning (Vaswani et al., 2017a),\n- · deep learning with associative memories (Graves et al., 2014; Weston et al., 2014; Ba et al., 2016a;b; Schlag &amp; Schmidhuber, 2018; Schlag et al., 2019),\n- · natural language processing (Devlin et al., 2018; 2019),\n- · sequence analysis and time series prediction (Hochreiter, 1991; Hochreiter &amp; Schmidhuber, 1997; Cho et al., 2014), and\n- · storing and retrieving reference data, e.g. the training data, outliers, high error data points, prototypes or cluster centers, support vectors &amp; border cases.\n\nHopfield network layers can substitute existing layers like pooling layers, permutation equivariant layers (Guttenberg et al., 2016; Ravanbakhsh et al., 2016), GRU (Cho et al., 2014) &amp; LSTM (Hochreiter, 1991; Hochreiter &amp; Schmidhuber, 1997) layers, and attention layers (Vaswani et al., 2017a;b; Bahdanau et al., 2014).\n\nTypes of neural networks. We consider two types of feedforward neural networks: (I) Neural networks that propagate an activation vector from the input layer to the output layer. Examples are fully-connected or convolutional neural networks. (II) Neural networks that propagate a set of vectors from the input layer to the output layer, where each layer applies the same operation to each element of the set and the output layer may summarize the set via a vector. An example is the transformer. Recurrent neural networks are networks of type (I), which are iteratively applied to a set or a sequence, where intermediate results are stored in a memory and can be reused. Modern Hopfield networks can be integrated into both types of neural network architectures and enable to equip each of their layers with associative memories. See Fig. 2.\n\nTypes of new Hopfield layers. We introduce three types of Hopfield layers: Hopfield , HopfieldPooling , and HopfieldLayer . The continuous modern Hopfield network results in a plethora of new deep learning architectures, since we can (a) propagate sets or single vectors, (b) propagate queries,\n\nFigure 2: Left: A standard deep network with layers ( glyph[squaresolid] ) propagates either a vector or a set of vectors from the input to the output. Right: A deep network, where layers ( glyph[squaresolid] ) are equipped with associative memories via Hopfield layers ( glyph[squaresolid] ).\n\n<!-- image -->\n\nstored patterns, or both, (c) learn static queries or stored patterns, (d) fill the memory by training sets, prototypes, or external data. Next, we provide three useful types of Hopfield layers. The implementation is available at: https://github.com/ml-jku/hopfield-layers\n\n(1) Layer Hopfield for networks that propagate sets of vectors via state (query) patterns R and stored (key) patterns Y . The layer Hopfield is the realization of formula (10). The memory of the Hopfield layer can be filled with sets from the input or previous layers , see Fig. 3. The memory may be filled with a reference set, which is covered by providing the reference set as additional input. Thus, the layer Hopfield allows the association of two sets. A prominent example of a layer that performs such association is the transformer attention mechanism, which associates keys and queries, e.g. two point sets that have to be compared. This layer allows for different kinds of sequence-to-sequence learning, point set operations, and retrieval-based methods. The layer Hopfield with skip connections in a ResNet architecture is identical to the popular transformer and BERT models. In the experiments, we analyzed these Hopfield layers in transformer architectures. In our experiments in which we compare machine learning methods on small datasets of the UCI benchmark collection the layer Hopfield is also used.\n\nFigure 3: The layer Hopfield allows the association of two sets R ( glyph[squaresolid] ) and Y ( glyph[squaresolid] ). It can be integrated into deep networks that propagate sets of vectors. The Hopfield memory is filled with a set from either the input or previous layers. The output is a set of vectors Z ( glyph[squaresolid] ).\n\n<!-- image -->\n\n(2) Layer HopfieldPooling for networks that propagate patterns via the stored (key) patterns Y . This layer performs a pooling or summarization of sets Y obtained from queries in previous layers or the input. The memory of the HopfieldPooling layer is filled with sets from the input or previous layers . The HopfieldPooling layer uses the queries to search for patterns in the memory, the stored set. If more patterns are similar to a particular search pattern (query), then the result is an average over these patterns. The state (query) patterns of each layer are static and can be learned. Multiple queries supply a set to the next layer, where each query corresponds to one element of the set. Thus, the layer HopfieldPooling enables fixed pattern search, pooling operations, and memories like LSTMs or GRUs. The static pattern functionality is typically needed if particular patterns must be identified in the data.\n\nA single HopfieldPooling layer allows for multiple instance learning. Static state (query)\n\npatterns together with position encoding in the keys allows for performing pooling operations. The position encoding can be two-dimensional, where standard convolutional filters can be constructed as in convolutional neural networks (CNNs). The HopfieldPooling layer can substitute pooling, averaging, LSTM, and permutation equivariant layers. See Fig. 4. The layer HopfieldPooling is used for experiments with multiple instance learning tasks, e.g. for immune repertoire classification in the experiments.\n\nFigure 4: The layer HopfieldPooling enables pooling or summarization of sets, which are obtained from the input or from previous layers. The input Y ( glyph[squaresolid] ) can be either a set or a sequence. The query patterns of each layer are static and can be learned. The output is a set of vectors Z ( glyph[squaresolid] ), where the number of vectors equals the number of query patterns. The layer HopfieldPooling can realize multiple instance learning.\n\n<!-- image -->\n\n(3) Layer HopfieldLayer for networks that propagate a vector or a set of vectors via state (query) patterns R . The queries R can be input vectors or queries that are computed from the output of previous layers. The memory of the HopfieldLayer layer is filled with a fixed set , which can be the training set, a reference set, prototype set, or a learned set (a learned matrix). The stored (key) patterns are static and can be learned. If the training set is stored in the memory, then each layer constructs a new set of queries based on the query results of previous layers. The stored patterns can be initialized by the training set or a reference set and then learned, in which case they deviate from the training set. The stored patterns can be interpreted as weights from the state (query) to hidden neurons that have a softmax activation function (Krotov &amp; Hopfield, 2020). The layer HopfieldLayer can substitute a fully connected layer, see Fig. 5. A single HopfieldLayer layer also allows for approaches similar to support vector machines (SVMs), approaches similar to k -nearest neighbor, approaches similar to learning vector quantization, and pattern search. For classification, the raw data y i = ( z i , t i ) can be the concatenation of input z i and target t i . In this case, the matrices W K and W V can be designed such that inside the softmax the input z i is used and outside the softmax the target t i . Thus, the softmax provides a weighted average of the target vectors based on the similarity between the query and the inputs. Also SVM models, k -nearest neighbor, and learning vector quantization can be considered as weighted averages of the targets. The encoder-decoder attention layer of the transformers are a HopfieldLayer layer, where the memory is filled with the encoder output set. In our experiments with the drug design benchmark datasets, the layer HopfieldLayer has been applied and compared to other machine learning methods.\n\nFigure 5: The layer HopfieldLayer enables multiple queries of the training set, a reference set, prototype set, or a learned set (a learned matrix). The queries for each layer are computed from the results of previous layers. The input is a set of vectors R ( glyph[squaresolid] ). The output is also a set of vectors Z ( glyph[squaresolid] ), where the number of output vectors equals the number of input vectors. The layer HopfieldLayer can realize SVM models, k -nearest neighbor, and LVQ.\n\n<!-- image -->\n\nAdditional functionality of new Hopfield layers. The insights about energy, convergence, and storage properties provide all new Hopfield layers with additional functionalities: i) multiple updates\n\nto control how precise fixed points are found without additional parameters needed. ii) variable β to determine the kind of fixed points such as the size of metastable states. The variable β controls over how many patterns is averaged. As observed in the experiments, the variable is relevant in combination with the learning rate to steer the learning dynamics. The parameter β governs the fixed point dynamics and can be learned, too. iii) controlling the storage capacity via the dimension of the associative space. The storage capacity can be relevant for tasks with a huge number of instances as in the immune repertoire classification experiment. iv) pattern normalization controls, like the layernorm, the fixed point dynamics by the norm and shift of the patterns. For more details see appendix, Section A.6.",
        "metadata": {
            "section_header": "NEW HOPFIELD LAYERS FOR DEEP LEARNING",
            "title": "HOPFIELD NETWORKS IS ALL YOU NEED",
            "type": "paper"
        }
    },
    {
        "page_content": "## 4 EXPERIMENTS\n\nWe show that our proposed Hopfield layers can be applied successfully to a wide range of tasks. The tasks are from natural language processing, contain multiple instance learning problems, a collection of small classification tasks, and drug design problems.\n\nAnalysis of transformer and BERT models. Transformer and BERT models can be implemented by the layer Hopfield . The kind of fixed point of the Hopfield net is determined by how the pattern x i is separated from others patterns. (a) a global fixed point : no separation of a pattern from the others, (b) a fixed point close to a single pattern : pattern is separated from other patterns, (c) metastable state : some patterns are similar to each other and well separated from all other vectors. We observed that the attention heads of transformer and BERT models are predominantly in metastable states, which are categorized into four classes: (I) averaging over a very large number of patterns (very large metastable state or fixed point (a)), (II) averaging over a large number of patterns (large metastable state), (III) averaging over a medium number of patterns (medium metastable state), (IV) averaging over a small number of patterns (small metastable state or fixed point (c)). For analyzing the metastable states, we calculated the minimal number k of softmax values required to sum up to 0 90 . . Hence, k indicates the size of a metastable state. To determine in which of the four classes a head is mainly operating, we computed the distribution of k across sequences. Concretely, for N tokens and for ¯ k as the median of the distribution, a head is classified as operating in class (I) if 1 / 2 N &lt; k ¯ , as operating in class (II) if 1 / 8 N &lt; k ¯ glyph[lessorequalslant] 1 / N 2 , as operating in class (III) if 1 / 32 N &lt; k ¯ glyph[lessorequalslant] 1 / 8 N , and as operating in class (IV) if ¯ k glyph[lessorequalslant] 1 32 / N . We analyzed pre-trained BERT models from Hugging Face Inc. (Wolf et al., 2019) according to these operating classes. In Fig. A.3 in the appendix the distribution of the pre-trained bert-base-cased model is depicted (for other models see appendix Section A.5.1.4). Operating classes (II) (large metastable states) and (IV) (small metastable states) are often observed in the middle layers. Operating class (I) (averaging over a very large number of patterns) is abundant in lower layers. Similar observations have been reported in other studies (Toneva &amp; Wehbe, 2019a;b; Tay et al., 2020). Operating class (III) (medium metastable states) is predominant in the last layers.\n\nMultiple Instance Learning Datasets. For multiple instance learning (MIL) (Dietterich et al., 1997), we integrate our new Hopfield network via the layer HopfieldPooling into deep learning architectures. Recently, deep learning methods have been applied to MIL problems (Ilse et al., 2018), but still the performance on many datasets lacks improvement. Thus, MIL datasets still pose an interesting challenge, in which Hopfield layers equipped with memory are a promising approach.\n\n· Immune Repertoire Classification. The first MIL task is immune repertoire classification, where a deep learning architecture with HopfieldPooling (DeepRC) was used (Widrich et al., 2020a;b). Immune repertoire classification (Emerson et al., 2017) typically requires to extract few patterns from a large set of sequences, the repertoire, that are indicative for the respective immune status. The datasets contain ≈ 300,000 instances per immune repertoire, which represents one of the largest multiple instance learning experiments ever conducted (Carbonneau et al., 2018). Most MIL methods fail due the large number of instances. This experiment comprises real-world and simulated datasets. Simulated datasets are generated by implanting sequence motifs (Akbar et al., 2019; Weber et al., 2020) with low frequency into simulated or experimentally-observed immune receptor sequences. The performance of DeepRC was compared with other machine learning methods: (i) known motif, (ii) SVM using k -mers and MinMax or Jaccard kernel, (iii) K -Nearest Neighbor (KNN) with k -mers, (iv) logistic regression with k -mers, (v) burden test with k -mers, and (vi) logistic multiple\n\nTable 1: Results for MIL datasets Tiger, Fox, Elephant, and UCSB Breast Cancer in terms of AUC. Results for all methods except the first are taken from either a (Küçüka¸ scı &amp; Baydo˘ gan, 2018) or b (Carbonneau et al., 2016), depending on which reports the higher AUC.\n\n| Method                                         | tiger            | fox              | elephant         | UCSB             |\n|------------------------------------------------|------------------|------------------|------------------|------------------|\n| Hopfield (ours)                                | 91 3 . ± 0 5 .   | 64 05 . ± 0 4 .  | 94 9 . ± 0 3 .   | 89 5 . ± 0 8 .   |\n| Path encoding (Küçüka¸ scı &amp; Baydo˘ gan, 2018) | 91 0 . ± 1 0 . a | 71 2 . ± 1 4 . a | 94 4 . ± 0 7 . a | 88 0 . ± 2 2 . a |\n| MInD (Cheplygina et al., 2016)                 | 85 3 . ± 1 1 . a | 70 4 . ± 1 6 . a | 93 6 . ± 0 9 . a | 83 1 . ± 2 7 . a |\n| MILES (Chen et al., 2006)                      | 87 2 . ± 1 7 . b | 73 8 . ± 1 6 . a | 92 7 . ± 0 7 . a | 83 3 . ± 2 6 . a |\n| APR (Dietterich et al., 1997)                  | 77 8 . ± 0 7 . b | 54 1 . ± 0 9 . b | 55 0 . ± 1 0 . b | -                |\n| Citation-kNN (Wang, 2000)                      | 85 5 . ± 0 9 . b | 63 5 . ± 1 5 . b | 89 6 . ± 0 9 . b | 70 6 . ± 3 2 . a |\n| DD (Maron &amp; Lozano-Pérez, 1998)                | 84 1 . b         | 63 1 . b         | 90 7 . b         | -                |\n\ninstance learning (lMIL). On the real-world dataset DeepRC achieved an AUC of 0 832 . ± 0 022 . , followed by the SVM with MinMax kernel (AUC 0 825 . ± 0 022 . ) and the burden test with an AUC of 0 699 . ± 0 041 . . Across datasets, DeepRC outperformed all competing methods with respect to average AUC (Widrich et al., 2020a;b).\n\n· MIL benchmark datasets. We apply Hopfield layers to further MIL datasets (Ilse et al., 2018; Küçüka¸ scı &amp; Baydo˘ gan, 2018; Cheplygina et al., 2016): Elephant, Fox and Tiger for image annotation (Andrews et al., 2003). These datasets consist of color images from the Corel dataset that have been preprocessed and segmented. An image consists of a set of segments (or blobs), each characterized by color, texture and shape descriptors. The datasets have 100 positive and 100 negative example images. The latter have been randomly drawn from a pool of photos of other animals. Elephant comprises 1,391 instances and 230 features, Fox 1,320 instances and 230 features, and Tiger has 1,220 instances and 230 features. Furthermore, we use the UCSB breast cancer classification (Kandemir et al., 2014) dataset, which consists of 2,002 instances across 58 input objects. An instance represents a patch of a histopathological image of cancerous or normal tissue. The layer HopfieldPooling is used, which allows for computing a per-input-object representation by extracting an average of instances that are indicative for one of the two classes. The input to the layer HopfieldPooling is a set of embedded instances Y . A trainable but fixed state (query) pattern Q is used for averaging over class-indicative instances. This averaging enables a compression of variable-sized bags to a fixedsized representation to discriminate the bags. More details in appendix Sec. A.5.2. Our approach has set a new state-of-the-art and has outperformed other methods (Küçüka¸ scı &amp; Baydo˘an, 2018; g Carbonneau et al., 2016) on the datasets Tiger, Elephant and UCSB Breast Cancer (see Table 1).\n\nUCI Benchmark Collection. So far deep learning struggled with small datasets. However, Hopfield networks are promising for handling small datasets, since they can store the training data points or their representations to perform similarity-based, nearest neighbor, or learning vector quantization methods. Therefore, we test the Hopfield layer Hopfield on the small datasets of the UC Irvine (UCI) Machine Learning Repository that have been used to benchmark supervised learning methods (Fernández-Delgado et al., 2014; Wainberg et al., 2016; Khan et al., 2018) and also feed-forward neural networks (Klambauer et al., 2017a; Wu et al., 2018), where our Hopfield networks could exploit their memory. The whole 121 datasets in the collection vary strongly with respect to their size, number of features, and difficulties (Fernández-Delgado et al., 2014), such that they have been divided into 75 'small datasets' with less than 1,000 samples and 45 'large datasets' with more than or equal to 1,000 samples in Klambauer et al. (2017a).\n\nOn the 75 small datasets, Random Forests (RFs) and Support Vector Machines (SVM) are highly accurate, whereas on the large datasets, deep learning methods and neural networks are in the lead (Klambauer et al., 2017a;b; Wu et al., 2018). We applied a modern Hopfield network via the layer HopfieldLayer , where a selfnormalizing net (SNN) maps the input vector to Y and R . The output Z of HopfieldLayer enters a softmax output. We compared our modern Hopfield networks against deep learning\n\nTable 2: Results on 75 small datasets of the UCI benchmarks given as difference to average rank.\n\n| Method          | avg. rank diff.   | p -value     |\n|-----------------|-------------------|--------------|\n| Hopfield (ours) | - 3 92 .          | -            |\n| SVM             | - 3 23 .          | 0 15 .       |\n| SNN             | - 2 85 .          | 0 10 .       |\n| RandomForest    | - 2 79 .          | 0 05 .       |\n| . . .           | . . .             | . . .        |\n| Stacking        | 8 73 .            | 1 2 . e - 11 |\n\nmethods (e.g. SNNs, resnet), RFs, SVMs, boosting, bagging, and many other machine learning methods of Fernández-Delgado et al. (2014). Since for each method, multiple variants and implementations had been included, we used method groups and representatives as defined by Klambauer et al. (2017a). For each dataset, a ranking of the methods was calculated which is presented in Table 2. We found that Hopfield networks outperform all other methods on the small datasets, setting a new state-of-the-art for 10 datasets. The difference is significant except for the first three runner-up methods (Wilcoxon signed rank test). See appendix Section A.5.3 for details.\n\nDrug Design Benchmark Datasets. We test the Hopfield layer HopfieldLayer , on four drug design datasets. These datasets represent four main areas of modeling tasks in drug design, concretely to develop accurate models for predicting a) new anti-virals (HIV) by the Drug Therapeutics Program (DTP) AIDS Antiviral Screen, b) new protein inhibitors, concretely human β -secretase (BACE) inhibitors by Subramanian et al. (2016), c) metabolic effects as blood-brain barrier permeability (BBBP) (Martins et al., 2012) and d) side effects of a chemical compound from the Side Effect Resource (SIDER) Kuhn et al. (2016). We applied the Hopfield layer HopfieldLayer , where the training data is used as stored patterns Y , the input vector as state pattern R , and the corresponding training label to project the output of the Hopfield layer Y W V . Our architecture with HopfieldLayer has reached state-of-the-art for predicting side effects on SIDER 0 672 . ± 0 019 . as well as for predicting β -secretase BACE 0 902 . ± 0 023 . . For details, see Table A.5 in the appendix.\n\nConclusion. We have introduced a modern Hopfield network with continuous states and the corresponding new update rule. This network can store exponentially many patterns, retrieves patterns with one update, and has exponentially small retrieval errors. We analyzed the attention heads of BERT models. The new modern Hopfield networks have been integrated into deep learning architectures as layers to allow the storage of and access to raw input data, intermediate results, or learned prototypes. These Hopfield layers enable new ways of deep learning, beyond fully-connected, convolutional, or recurrent networks, and provide pooling, memory, association, and attention mechanisms. Hopfield layers that equip neural network layers with memories improved state-of-the-art in three out of four considered multiple instance learning problems and on immune repertoire classification, and on two drug design dataset. They yielded the best results among different machine learning methods on the UCI benchmark collections of small classification tasks.",
        "metadata": {
            "section_header": "EXPERIMENTS",
            "title": "HOPFIELD NETWORKS IS ALL YOU NEED",
            "type": "paper"
        }
    },
    {
        "page_content": "## ACKNOWLEDGMENTS\n\nThe ELLIS Unit Linz, the LIT AI Lab and the Institute for Machine Learning are supported by the Land Oberösterreich, LIT grants DeepToxGen (LIT-2017-3-YOU-003), and AI-SNN (LIT2018-6-YOU-214), the Medical Cognitive Computing Center (MC3), Janssen Pharmaceutica, UCB Biopharma, Merck Group, Audi.JKU Deep Learning Center, Audi Electronic Venture GmbH, TGW, Primal, S3AI (FFG-872172), Silicon Austria Labs (SAL), Anyline, FILL, EnliteAI, Google Brain, ZF Friedrichshafen AG, Robert Bosch GmbH, TÜV Austria, DCS, and the NVIDIA Corporation. IARAI is supported by Here Technologies.",
        "metadata": {
            "section_header": "ACKNOWLEDGMENTS",
            "title": "HOPFIELD NETWORKS IS ALL YOU NEED",
            "type": "paper"
        }
    },
    {
        "page_content": "## A APPENDIX\n\nThis appendix consists of six sections (A.1-A.6). Section A.1 introduces the new modern Hopfield network with continuous states and its update rule. Furthermore, Section A.1 provides a thorough and profound theoretical analysis of this new Hopfield network. Section A.2 provides the mathematical background for Section A.1. Section A.3 reviews binary Modern Hopfield Networks of Krotov &amp; Hopfield. Section A.4 shows that the Hopfield update rule is the attention mechanism of the transformer. Section A.5 gives details on the experiments. Section A.6 describes the PyTorch implementation of layers based on the new Hopfield networks and how to use them.",
        "metadata": {
            "section_header": "A APPENDIX",
            "title": "HOPFIELD NETWORKS IS ALL YOU NEED",
            "type": "paper"
        }
    },
    {
        "page_content": "## CONTENTS OF THE APPENDIX\n\n| A.1              | Continuous State Modern Hopfield Networks (A New Concept)                                                                                                          | . . . . . . . .                                                                                                                                                    | 12   |\n|------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------|------|\n|                  | A.1.1                                                                                                                                                              | Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                                       | 12   |\n|                  | A.1.2                                                                                                                                                              | New Energy Function . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                                            | 13   |\n|                  | A.1.3                                                                                                                                                              | New Update Rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                                          | 15   |\n|                  | A.1.4                                                                                                                                                              | Global Convergence of the Update Rule . . . . . . . . . . . . . . . . .                                                                                            | 16   |\n|                  | A.1.5                                                                                                                                                              | Local Convergence of the Update Rule: Fixed Point Iteration . . . . . .                                                                                            | 19   |\n|                  | A.1.6                                                                                                                                                              | Properties of Fixed Points Near Stored Pattern . . . . . . . . . . . . .                                                                                           | 44   |\n|                  | A.1.7                                                                                                                                                              | Learning Associations . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                                          | 57   |\n|                  | A.1.8                                                                                                                                                              | Infinite Many Patterns and Forgetting Patterns . . . . . . . . . . . . . .                                                                                         | 60   |\n|                  | A.1.9                                                                                                                                                              | Number of Spurious States . . . . . . . . . . . . . . . . . . . . . . . .                                                                                          | 61   |\n| A.2              | Properties of Softmax, Log-Sum-Exponential, Legendre Transform, Lambert W Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . | Properties of Softmax, Log-Sum-Exponential, Legendre Transform, Lambert W Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . | 62   |\n| A.3              | Modern Hopfield Networks: Binary States (Krotov and Hopfield) . . .                                                                                                | . . . . .                                                                                                                                                          | 70   |\n|                  | A.3.1                                                                                                                                                              | Modern Hopfield Networks: Introduction . . . . . . . . . . . . . . . .                                                                                             | 70   |\n|                  | A.3.2                                                                                                                                                              | Energy and Update Rule for Binary Modern Hopfield Networks . . . .                                                                                                 | 71   |\n| A.4              | Hopfield Update Rule is Attention of The Transformer . . . . . . . . . . .                                                                                         | . .                                                                                                                                                                | 73   |\n| A.5              | Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                                      | . . .                                                                                                                                                              | 73   |\n|                  | A.5.1                                                                                                                                                              | Experiment 1: Attention in Transformers described by Hopfield dynamics                                                                                             | 73   |\n|                  | A.5.2                                                                                                                                                              | Experiment 2: Multiple Instance Learning Datasets. . . . . . . . . . .                                                                                             | 78   |\n|                  | A.5.3                                                                                                                                                              | Experiment 3: Classification on Small UCI Benchmark Datasets . . . .                                                                                               | 81   |\n|                  | A.5.4                                                                                                                                                              | Experiment 4: Drug Design Benchmark Datasets . . . . . . . . . . . .                                                                                               | 82   |\n| A.6              | PyTorch Implementation of Hopfield Layers . . . . .                                                                                                                | . . . . . . . . . . . . . .                                                                                                                                        | 83   |\n|                  | A.6.1                                                                                                                                                              | Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                                       | 83   |\n|                  | A.6.2                                                                                                                                                              | Functionality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                                        | 84   |\n|                  | A.6.3                                                                                                                                                              | Usage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                                        | 86   |\n| LIST OF THEOREMS | LIST OF THEOREMS                                                                                                                                                   | LIST OF THEOREMS                                                                                                                                                   |      |\n| A1               | Theorem (Global Convergence (Zangwill): Energy) . .                                                                                                                | . . . . . . . . . . . . .                                                                                                                                          | 16   |\n| A2               | Theorem (Global Convergence: Stationary Points) . .                                                                                                                | . . . . . . . . . . . . . .                                                                                                                                        | 18   |\n| A3               | Theorem (Storage Capacity (M=2): Placed Patterns) .                                                                                                                | . . . . . . . . . . . . .                                                                                                                                          | 46   |\n\n| A4                                                            | Theorem (Storage Capacity (M=5): Placed Patterns) . . . . . . . . . . . .           | 47   |\n|---------------------------------------------------------------|-------------------------------------------------------------------------------------|------|\n| A5                                                            | Theorem (Storage Capacity (Main): Random Patterns) . . . . . . . . . . .            | 49   |\n| A6                                                            | Theorem (Storage Capacity (d computed): Random Patterns) . . . . . . . .            | 52   |\n| A7                                                            | Theorem (Storage Capacity (expected separation): Random Patterns) . . . .           | 55   |\n| A8                                                            | Theorem (Pattern Retrieval with One Update) . . . . . . . . . . . . . . . .         | 56   |\n| A9                                                            | Theorem (Exponentially Small Retrieval Error) . . . . . . . . . . . . . . .         | 57   |\n| A10                                                           | Theorem (Storage Capacity for Binary Modern Hopfield Nets (Demircigil et al. 2017)) | 72   |\n| LIST OF DEFINITIONS                                           | LIST OF DEFINITIONS                                                                 |      |\n| A1                                                            | Definition (Softmax) . . . . . . . . . . . . . . . . . . . . . . . . . . . . .      | 62   |\n| A2                                                            | Definition (Log-Sum-Exp Function) . . . . . . . . . . . . . . . . . . . . .         | 62   |\n| A3                                                            | Definition (Convex Conjugate) . . . . . . . . . . . . . . . . . . . . . . . .       | 66   |\n| A4                                                            | Definition (Legendre Transform) . . . . . . . . . . . . . . . . . . . . . . .       | 66   |\n| A5                                                            | Definition (Epi-Sum) . . . . . . . . . . . . . . . . . . . . . . . . . . . . .      | 66   |\n| A6                                                            | Definition (Lambert Function) . . . . . . . . . . . . . . . . . . . . . . . .       | 69   |\n| LIST OF FIGURES                                               | LIST OF FIGURES                                                                     |      |\n| A.1                                                           | The three cases of fixed points . . . . . . . . . . . . . . . . . . . . . . . .     | 19   |\n| A.2                                                           | From binary Hopfield network to transformer . . . . . . . . . . . . . . . .         | 73   |\n| A.4                                                           | Ridge plots of the distribution of counts . . . . . . . . . . . . . . . . . . .     | 76   |\n| A.5                                                           | Change of count density during training . . . . . . . . . . . . . . . . . . .       | 77   |\n| A.6                                                           | Attentions of a Gaussian averaging heads . . . . . . . . . . . . . . . . . .        | 78   |\n| A.7                                                           | A flowchart of the Hopfield layer . . . . . . . . . . . . . . . . . . . . . . .     | 87   |\n| LIST OF TABLES                                                | LIST OF TABLES                                                                      |      |\n| A.1                                                           | Results of immune repertoire classification across all datasets . . . . . . . .     | 79   |\n| A.2                                                           | Hyperparameter selection for MIL datasets . . . . . . . . . . . . . . . . .         | 80   |\n| A.3                                                           | Hyperparameter selection for small UCI benchmark datasets . . . . . . . .           | 82   |\n| A.4                                                           | Hyperparameter selection for drug design datasets . . . . . . . . . . . . . .       | 82   |\n| A.5                                                           | Results on drug design benchmark datasets . . . . . . . . . . . . . . . . .         | 83   |\n| A.1 CONTINUOUS STATE MODERN HOPFIELD NETWORKS (A NEW CONCEPT) | A.1 CONTINUOUS STATE MODERN HOPFIELD NETWORKS (A NEW CONCEPT)                       |      |",
        "metadata": {
            "section_header": "CONTENTS OF THE APPENDIX",
            "title": "HOPFIELD NETWORKS IS ALL YOU NEED",
            "type": "paper"
        }
    },
    {
        "page_content": "## A.1.1 INTRODUCTION\n\nIn Section A.1 our new modern Hopfield network is introduced. In Subsection A.1.2 we present the new energy function. Then in Subsection A.1.3, our new update rule is introduced. In Subsection A.1.4, we show that this update rule ensures global convergence. We show that all the limit points of any sequence generated by the update rule are the stationary points (local minima or saddle points) of the energy function. In Section A.1.5, we consider the local convergence of the update rule and see that patterns are retrieved with one update. In Subsection A.1.6, we consider the properties of the fixed points that are associated with the stored patterns. In Subsection A.1.6.1, we show that exponentially many patterns can be stored. The main result is given in Theorem A5: For random\n\npatterns on a sphere we can store and retrieve exponentially (in the dimension of the Hopfield space) many patterns. Subsection A.1.6.2 reports that patterns are typically retrieved with one update step and that the retrieval error is exponentially small.\n\nIn Subsection A.1.7, we consider how associations for the new Hopfield networks can be learned. In Subsection A.1.7.2, we analyze if the association is learned directly by a bilinear form. In Subsection A.1.7.3, we analyze if stored patterns and query patterns are mapped to the space of the Hopfield network. Therefore, we treat the architecture of the transformer and BERT. In Subsection A.1.8, we introduce a temporal component into the new Hopfield network that leads to a forgetting behavior. The forgetting allows us to treat infinite memory capacity in Subsection A.1.8.1. In Subsection A.1.8.2, we consider the controlled forgetting behavior.\n\nIn Section A.2, we provide the mathematical background that is needed for our proofs. In particular we give lemmas on properties of the softmax, the log-sum-exponential, the Legendre transform, and the Lambert W function.\n\nIn Section A.3, we review the new Hopfield network as introduced by Krotov and Hopfield in 2016. However in contrast to our new Hopfield network, the Hopfield network of Krotov and Hopfield is binary, that is, a network with binary states. In Subsection A.3.1, we give an introduction to neural networks equipped with associative memories and new Hopfield networks. In Subsection A.3.1.1, we discuss neural networks that are enhanced by an additional external memory and by attention mechanisms. In Subsection A.3.1.2, we give an overview over the modern Hopfield networks. Finally, in Subsection A.3.2, we present the energy function and the update rule for the modern, binary Hopfield networks.",
        "metadata": {
            "section_header": "A.1.1 INTRODUCTION",
            "title": "HOPFIELD NETWORKS IS ALL YOU NEED",
            "type": "paper"
        }
    },
    {
        "page_content": "## A.1.2 NEW ENERGY FUNCTION\n\nWe have patterns x 1 , . . . , x N that are represented by the matrix\n\n<!-- formula-not-decoded -->\n\nThe largest norm of a pattern is\n\n<!-- formula-not-decoded -->",
        "metadata": {
            "section_header": "A.1.2 NEW ENERGY FUNCTION",
            "title": "HOPFIELD NETWORKS IS ALL YOU NEED",
            "type": "paper"
        }
    },
    {
        "page_content": "## The query or state of the Hopfield network is ξ .\n\nThe energy function E in the new type of Hopfield models of Krotov and Hopfield is E = -∑ N i =1 F ( ξ T x i ) for binary patterns x i and binary state ξ with interaction function F x ( ) = x n , where n = 2 gives classical Hopfield model (Krotov &amp; Hopfield, 2016). The storage capacity is proportional to d n -1 (Krotov &amp; Hopfield, 2016). This model was generalized by Demircigil et al. (Demircigil et al., 2017) to exponential interaction functions F x ( ) = exp( x ) , which gives the energy E = -exp(lse(1 , X ξ T )) . This energy leads to an exponential storage capacity of N = 2 d/ 2 for binary patterns. Furthermore, with a single update the fixed point is recovered with high probability. See more details in Section A.3.\n\nIn contrast to the these binary modern Hopfield networks, we focus on modern Hopfield networks with continuous state s that can store continuous patterns . We generalize the energy of Demircigil et al. (Demircigil et al., 2017) to continuous states while keeping the lse properties which ensure high storage capacity and fast convergence. Our new energy E for a continuous query or state ξ is defined\n\nas\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nFirst let us collect and prove some properties of E . The next lemma gives bounds on the energy E . Lemma A1. The energy E is larger than zero:\n\n<!-- formula-not-decoded -->\n\nFor ξ in the simplex defined by the patterns, the energy E is upper bounded by:\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nProof. We start by deriving the lower bound of zero. The pattern most similar to query or state ξ is x ξ :\n\n<!-- formula-not-decoded -->\n\nWe obtain\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nThe energy is zero and, therefore, the bound attained, if all x i are equal, that is, x i = x for all i and ξ = x .\n\nFor deriving upper bounds on the energy E , we require the the query ξ to be in the simplex defined by the patterns, that is,\n\n<!-- formula-not-decoded -->\n\nThe first upper bound is.\n\n<!-- formula-not-decoded -->\n\nFor the first inequality we applied Lemma A19 to -lse( β, X ξ T ) with z = p giving\n\n<!-- formula-not-decoded -->\n\nas the term involving the logarithm is non-positive.\n\nNext we derive the second upper bound, for which we need the mean m x of the patterns\n\n<!-- formula-not-decoded -->\n\nWe obtain\n\n<!-- formula-not-decoded -->\n\nwhere for the first inequality we again applied Lemma A19 with z = (1 /N,.. . , 1 /N ) and β -1 ∑ i 1 /N ln(1 /N ) = -β -1 ln( N ) . This inequality also follows from Jensen's inequality. The second inequality uses the Cauchy-Schwarz inequality. The last inequality uses\n\n<!-- formula-not-decoded -->\n\nand\n\n<!-- formula-not-decoded -->",
        "metadata": {
            "section_header": "The query or state of the Hopfield network is ξ .",
            "title": "HOPFIELD NETWORKS IS ALL YOU NEED",
            "type": "paper"
        }
    },
    {
        "page_content": "## A.1.3 NEW UPDATE RULE\n\nWe now introduce an update rule for minimizing the energy function E . The new update rule is\n\n<!-- formula-not-decoded -->\n\nwhere we used\n\n<!-- formula-not-decoded -->\n\nThe new state ξ new is in the simplex defined by the patterns, no matter what the previous state ξ was. For comparison, the synchronous update rule for the classical Hopfield network with threshold zero is\n\n<!-- formula-not-decoded -->\n\nTherefore, instead of using the vector X ξ T as in the classical Hopfield network, its softmax version softmax( β X ξ T ) is used.\n\nIn the next section (Section A.1.4) we show that the update rule Eq. (28) ensures global convergence. We show that all the limit points of any sequence generated by the update rule are the stationary points (local minima or saddle points) of the energy function E . In Section A.1.5 we consider the local convergence of the update rule Eq. (28) and see that patterns are retrieved with one update.",
        "metadata": {
            "section_header": "A.1.3 NEW UPDATE RULE",
            "title": "HOPFIELD NETWORKS IS ALL YOU NEED",
            "type": "paper"
        }
    },
    {
        "page_content": "## A.1.4 GLOBAL CONVERGENCE OF THE UPDATE RULE\n\nWe are interested in the global convergence , that is, convergence from each initial point, of the iteration\n\n<!-- formula-not-decoded -->\n\nwhere we used\n\n<!-- formula-not-decoded -->\n\nWe defined the energy function\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nWe will show that the update rule in Eq. (31) is the Concave-Convex Procedure (CCCP) for minimizing the energy E . The CCCP is proven to converge globally.\n\nTheorem A1 (Global Convergence (Zangwill): Energy) . The update rule Eq. (31) converges globally: For ξ t +1 = f ( ξ t ) , the energy E( ξ t ) → E( ξ ∗ ) for t →∞ and a fixed point ξ ∗ .\n\nProof. The Concave-Convex Procedure (CCCP) (Yuille &amp; Rangarajan, 2002; 2003) minimizes a function that is the sum of a concave function and a convex function. CCCP is equivalent to Legendre minimization (Rangarajan et al., 1996; 1999) algorithms (Yuille &amp; Rangarajan, 2003). The Jacobian of the softmax is positive semi-definite according to Lemma A22. The Jacobian of the softmax is the Hessian of the lse , therefore lse is a convex and -lse a concave function. Therefore, the energy function E( ξ ) is the sum of the convex function E ( 1 ξ ) = 1 / 2 ξ T ξ + C 1 and the concave function E ( 2 ξ ) = -lse :\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nwhere C 1 does not depend on ξ .\n\nThe Concave-Convex Procedure (CCCP) (Yuille &amp; Rangarajan, 2002; 2003) applied to E is\n\n<!-- formula-not-decoded -->\n\nwhich is\n\n<!-- formula-not-decoded -->\n\nThe resulting update rule is\n\n<!-- formula-not-decoded -->\n\nusing\n\n<!-- formula-not-decoded -->\n\nThis is the update rule in Eq. (31).\n\nTheorem 2 in Yuille &amp; Rangarajan (2002) and Theorem 2 in Yuille &amp; Rangarajan (2003) state that the update rule Eq. (31) is guaranteed to monotonically decrease the energy E as a function of time. See also Theorem 2 in Sriperumbudur &amp; Lanckriet (2009).\n\nAlthough the objective converges in all cases, it does not necessarily converge to a local minimum (Lipp &amp; Boyd, 2016).\n\nHowever the convergence proof of CCCP in Yuille &amp; Rangarajan (2002; 2003) was not as rigorous as required. In Sriperumbudur &amp; Lanckriet (2009) a rigorous analysis of the convergence of CCCP is performed using Zangwill's global convergence theory of iterative algorithms.\n\nIn Sriperumbudur &amp; Lanckriet (2009) the minimization problem\n\n<!-- formula-not-decoded -->\n\nis considered with E 1 convex, -E 2 convex, c component-wise convex function, and d an affine function. The CCCP algorithm solves this minimization problem by linearization of the concave part and is defined in Sriperumbudur &amp; Lanckriet (2009) as\n\n<!-- formula-not-decoded -->\n\nWe define the upper bound E C on the energy:\n\n<!-- formula-not-decoded -->\n\nE C is equal to the energy E( ξ t ) for ξ = ξ t :\n\n<!-- formula-not-decoded -->\n\nSince -E 2 is convex, the first order characterization of convexity holds (Eq. 3.2 in Boyd &amp; Vandenberghe (2009)):\n\n<!-- formula-not-decoded -->\n\nthat is\n\n<!-- formula-not-decoded -->\n\nglyph[negationslash]\n\nTherefore, for ξ = ξ t the function E C is an upper bound on the energy:\n\n<!-- formula-not-decoded -->\n\nwhere C 2 does not depend on ξ . Since we do not have constraints, ξ t +1 is defined as\n\n<!-- formula-not-decoded -->\n\nhence E C ( ξ t +1 , ξ t ) glyph[lessorequalslant] E C ( ξ t , ξ t ) . Combining the inequalities gives:\n\n<!-- formula-not-decoded -->\n\nSince we do not have constraints, ξ t +1 is the minimum of\n\n<!-- formula-not-decoded -->\n\nas a function of ξ .\n\nFor a minimum not at the border, the derivative has to be the zero vector\n\n<!-- formula-not-decoded -->\n\nand the Hessian must be positive semi-definite\n\n<!-- formula-not-decoded -->\n\nThe Hessian is strict positive definite everywhere, therefore the optimization problem is strict convex (if the domain is convex) and there exist only one minimum, which is a global minimum. E C can even be written as a quadratic form:\n\n<!-- formula-not-decoded -->\n\nwhere C 3 does not depend on ξ .\n\nTherefore, the minimum is\n\n<!-- formula-not-decoded -->\n\nif it is in the domain as we assume.\n\nUsing M = max i ‖ x i ‖ , ξ t +1 is in the sphere S = { x | ‖ x ‖ glyph[lessorequalslant] M } which is a convex and compact set. Hence, if ξ 0 ∈ S , then the iteration is a mapping from S to S . Therefore, the point-set-map defined by the iteration Eq. (55) is uniformly compact on S according to Remark 7 in Sriperumbudur &amp;Lanckriet (2009). Theorem 2 and Theorem 4 in (Sriperumbudur &amp; Lanckriet, 2009) states that all the limit points of the iteration Eq. (55) are stationary points. These theorems follow from Zangwill's global convergence theorem: Convergence Theorem A, page 91 in Zangwill (1969) and page 3 in Wu (1983).\n\nThe global convergence theorem only assures that for the sequence ξ t +1 = f ( ξ t ) and a function Φ we have Φ( ξ t ) → Φ( ξ ∗ ) for t → ∞ but not ξ t → ξ ∗ . However, if f is strictly monotone with respect to Φ , then we can strengthen Zangwill's global convergence theorem (Meyer, 1976). We set Φ = E and show E( ξ t +1 ) &lt; E( ξ t ) if ξ t is not a stationary point of E , that is, f is strictly monotone with respect to E . The following theorem is similar to the convergence results for the expectation maximization (EM) algorithm in Wu (1983) which are given in theorems 1 to 6 in Wu (1983). The following theorem is also very similar to Theorem 8 in Sriperumbudur &amp; Lanckriet (2009).\n\nTheorem A2 (Global Convergence: Stationary Points) . For the iteration Eq. (55) we have E( ξ t ) → E( ξ ∗ ) = E ∗ as t →∞ , for some stationary point ξ ∗ . Furthermore ∥ ∥ ξ t +1 -ξ t ∥ ∥ → 0 and either { ξ t } ∞ t =0 converges or, in the other case, the set of limit points of { ξ t } ∞ t =0 is a connected and compact subset of L (E ) ∗ , where L ( a ) = { ξ ∈ L | E( ) = ξ a } and L is the set of stationary points of the iteration Eq. (55) . If L (E ) ∗ is finite, then any sequence { ξ t } ∞ t =0 generated by the iteration Eq. (55) converges to some ξ ∗ ∈ L (E ) ∗ .\n\nglyph[negationslash]\n\nProof. We have E( ξ t ) = E ( 1 ξ t ) + E 2 ( ξ t ) . The gradient ∇ ξ E ( 2 ξ t ) = -∇ ξ lse( β, X ξ T ) is continuous. Therefore, Eq. (51) has minimum in the sphere S , which is a convex and compact set. If ξ t +1 = ξ t , then ξ t was not the minimum of Eq. (48) as the derivative at ξ t is not equal to zero. Eq. (53) shows that the optimization problem Eq. (48) is strict convex, hence it has only one minimum, which is a global minimum. Eq. (54) shows that the optimization problem Eq. (48) is even a quadratic form. Therefore, we have\n\n<!-- formula-not-decoded -->\n\nTherefore, the point-set-map defined by the iteration Eq. (55) (for definitions see (Sriperumbudur &amp;Lanckriet, 2009)) is strictly monotonic with respect to E . Therefore, we can apply Theorem 3 in Sriperumbudur &amp; Lanckriet (2009) or Theorem 3.1 and Corollary 3.2 in Meyer (1976), which give the statements of the theorem.\n\nWe showed global convergence of the iteration Eq. (31). We have shown that all the limit points of any sequence generated by the iteration Eq. (31) are the stationary points (critical points; local minima or saddle points) of the energy function E . Local maxima as stationary points are only possible if the iterations exactly hits a local maximum. However, convergence to a local maximum without being there is not possible because Eq. (56) ensures a strict decrease of the energy E . Therefore, almost sure local maxima are not obtained as stationary points. Either the iteration converges or, in the second case, the set of limit points is a connected and compact set. But what happens if ξ 0 is in an glyph[epsilon1] -neighborhood around a local minimum ξ ∗ ? Will the iteration Eq. (31) converge to ξ ∗ ? What is the rate of convergence? These questions are about local convergence which will be treated in detail in next section.",
        "metadata": {
            "section_header": "A.1.4 GLOBAL CONVERGENCE OF THE UPDATE RULE",
            "title": "HOPFIELD NETWORKS IS ALL YOU NEED",
            "type": "paper"
        }
    },
    {
        "page_content": "## A.1.5 LOCAL CONVERGENCE OF THE UPDATE RULE: FIXED POINT ITERATION\n\nFor the proof of local convergence to a fixed point we will apply Banach fixed point theorem. For the rate of convergence we will rely on properties of a contraction mapping.",
        "metadata": {
            "section_header": "A.1.5 LOCAL CONVERGENCE OF THE UPDATE RULE: FIXED POINT ITERATION",
            "title": "HOPFIELD NETWORKS IS ALL YOU NEED",
            "type": "paper"
        }
    },
    {
        "page_content": "## A.1.5.1 General Bound on the Jacobian of the Iteration. We consider the iteration\n\n<!-- formula-not-decoded -->\n\nusing\n\n<!-- formula-not-decoded -->\n\nThe Jacobian J is symmetric and has the following form:\n\n<!-- formula-not-decoded -->\n\nwhere J s is Jacobian of the softmax.\n\nTo analyze the local convergence of the iteration, we distinguish between the following three cases (see also Fig. A.1). Here we only provide an informal discussion to give the reader some intuition. A rigorous formulation of the results can be found in the corresponding subsections.\n\n- a) If the patterns x i are not well separated, the iteration goes to a fixed point close to the arithmetic mean of the vectors. In this case p is close to p i = 1 /N .\n- b) If the patterns x i are well separated, then the iteration goes to the pattern to which the initial ξ is similar. If the initial ξ is similar to a vector x i then it will converge to a vector close to x i and p will converge to a vector close to e i .\n- c) If some vectors are similar to each other but well separated from all other vectors, then a so called metastable state between the similar vectors exists. Iterations that start near the metastable state converge to this metastable state.\n\nFigure A.1: The three cases of fixed points. a) Stored patterns (fixed point is single pattern) : patterns are stored if they are well separated. Each pattern x i has a single fixed point x ∗ i close to it. In the sphere S i , pattern x i is the only pattern and x ∗ i the only fixed point. b) Metastable state (fixed point is average of similar patterns) : x i and x j are similar to each other and not well separated. The fixed point m ∗ x is a metastable state that is close to the mean m x of the similar patterns. c) Global fixed point (fixed point is average of all patterns) : no pattern is well separated from the others. A single global fixed point m ∗ x exists that is close to the arithmetic mean m x of all patterns. We begin with a bound on the Jacobian of the iteration, thereby heavily relying on the Jacobian of the softmax from Lemma A24.\n\n<!-- image -->\n\n<!-- image -->\n\nfixed point\n\npattern\n\naverage pattern\n\nLemma A2. For N patterns X = ( x 1 , . . . , x N ) , p = softmax( β X ξ T ) , M = max i ‖ x i ‖ , and m = max i p i (1 -p i ) , the spectral norm of the Jacobian J of the fixed point iteration is bounded:\n\n<!-- formula-not-decoded -->\n\nIf\n\np\n\nmax\n\n= max\n\ni\n\np\n\ni\n\n≥\n\n1\n\n-\n\nglyph[epsilon1]\n\n,\n\nthen for the spectral norm of the Jacobian holds\n\n<!-- formula-not-decoded -->\n\n<!-- image -->\n\nProof. With\n\n<!-- formula-not-decoded -->\n\nthe symmetric Jacobian J is\n\n<!-- formula-not-decoded -->\n\nwhere J s is Jacobian of the softmax.\n\nWith m = max i p i (1 -p i ) , Eq. (476) from Lemma A24 is\n\n<!-- formula-not-decoded -->\n\nUsing this bound on ‖ J s ‖ 2 , we obtain\n\n<!-- formula-not-decoded -->\n\nThe spectral norm ‖ ‖ . 2 is bounded by the Frobenius norm ‖ ‖ . F which can be expressed by the norm squared of its column vectors:\n\n<!-- formula-not-decoded -->\n\nTherefore, we obtain the first statement of the lemma:\n\n<!-- formula-not-decoded -->\n\nWith p max = max i p i ≥ 1 -glyph[epsilon1] Eq. (480) in Lemma A24 is\n\n<!-- formula-not-decoded -->\n\nUsing this inequality, we obtain the second statement of the lemma:\n\n<!-- formula-not-decoded -->\n\nWe now define the 'separation' ∆ i of a pattern x i from data X = ( x 1 , . . . , x N ) here, since it has an important role for the convergence properties of the iteration.\n\nDefinition 2 (Separation of Patterns) . We define ∆ i , i.e. the separation of pattern x i from data X = ( x 1 , . . . , x N ) as:\n\nglyph[negationslash]\n\nglyph[negationslash]\n\n<!-- formula-not-decoded -->\n\nThe pattern is separated from the other data if 0 &lt; ∆ i . Using the parallelogram identity, ∆ i can also be expressed as\n\nglyph[negationslash]\n\n<!-- formula-not-decoded -->\n\nglyph[negationslash]\n\nglyph[negationslash]\n\nFor ‖ x i ‖ = ‖ x j ‖ we have ∆ = 1 2min i / j,j = i ‖ x i -x j ‖ 2 .\n\nglyph[negationslash]\n\nAnalog we say for a query ξ and data X = ( x 1 , . . . , x N ) , that x i is least separated from ξ while being separated from other x j with j = i if\n\nglyph[negationslash]\n\nglyph[negationslash]\n\n<!-- formula-not-decoded -->\n\nglyph[negationslash]\n\nglyph[negationslash]\n\n<!-- formula-not-decoded -->\n\nNext we consider the case where the iteration has only one stable fixed point.\n\nA.1.5.2 One Stable State: Fixed Point Near the Mean of the Patterns. We start with the case where no pattern is well separated from the others.\n\n· Global fixed point near the global mean: Analysis using the data center.\n\nWe revisit the bound on the Jacobian of the iteration by utilizing properties of pattern distributions. We begin with a probabilistic interpretation where we consider p i as the probability of selecting the vector x i . Consequently, we define expectations as E [ p f ( x )] = ∑ N i =1 p f i ( x i ) . In this setting the matrix\n\n<!-- formula-not-decoded -->\n\nis the covariance matrix of data X when its vectors are selected according to the probability p :\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\ntherefore we have\n\n<!-- formula-not-decoded -->\n\nThe largest eigenvalue of the covariance matrix (equal to the largest singular value) is the variance in the direction of the eigenvector associated with the largest eigenvalue.\n\nWe define:\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nm x is the arithmetic mean (the center) of the patterns. m max is the maximal distance of the patterns to the center m x .\n\nThe variance of the patterns is\n\n<!-- formula-not-decoded -->\n\nThe maximal distance to the center m max allows the derivation of a bound on the norm of the Jacobian.\n\nNext lemma gives a condition for a global fixed point.\n\nLemma A3. The following bound on the norm ‖ J ‖ 2 of the Jacobian of the fixed point iteration f holds independent of p or the query ξ .\n\n<!-- formula-not-decoded -->\n\nFor β m 2 max &lt; 1 there exists a unique fixed point (global fixed point) of iteration f in each compact set.\n\nProof. In order to bound the variance we compute the vector a that minimizes\n\n<!-- formula-not-decoded -->\n\nThe solution to\n\nis\n\n<!-- formula-not-decoded -->\n\nThe Hessian of f is positive definite since\n\n<!-- formula-not-decoded -->\n\nand f is a convex function. Hence, the mean\n\n<!-- formula-not-decoded -->\n\nminimizes ∑ N i =1 p i ‖ x i -a ‖ 2 . Therefore, we have\n\n<!-- formula-not-decoded -->\n\nLet us quickly recall that the spectral norm of an outer product of two vectors is the product of the Euclidean norms of the vectors:\n\n<!-- formula-not-decoded -->\n\nsince bb T has eigenvector b / ‖ b ‖ with eigenvalue ‖ b ‖ 2 and otherwise zero eigenvalues.\n\nWe now bound the variance of the patterns:\n\n<!-- formula-not-decoded -->\n\nThe bound of the lemma on ‖ J ‖ 2 follows from Eq. (78).\n\nFor ‖ J ‖ 2 glyph[lessorequalslant] β m 2 max &lt; 1 we have a contraction mapping on each compact set. Banach fixed point theorem says there is a unique fixed point in the compact set.\n\nNow let us further investigate the tightness of the bound on ‖ Var p [ x ] ‖ 2 via ‖ x i -¯ x ‖ 2 : we consider the trace, which is the sum ∑ d k =1 e k of the w.l.o.g. ordered nonnegative eigenvalues e k of Var p [ x ] The spectral norm is equal to the largest eigenvalue e 1 , which is equal to the largest singular value, as we have positive semidefinite matrices. We obtain:\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nTherefore, the tightness of the bound depends on eigenvalues which are not the largest. Hence variations which are not along the largest variation weaken the bound.\n\nNext we investigate the location of fixed points which existence is ensured by the global convergence stated in Theorem A2. For N patterns X = ( x 1 , . . . , x N ) , we consider the iteration\n\n<!-- formula-not-decoded -->\n\nusing\n\n<!-- formula-not-decoded -->\n\nξ new is in the simplex of the patterns, that is, ξ new = ∑ i p i x i with ∑ i p i = 1 and 0 glyph[lessorequalslant] p i . Hence, after one update ξ is in the simplex of the pattern and stays there. If the center m x is the zero vector m x = 0 , that is, the data is centered, then the mean is a fixed point of the iteration. For ξ = m x = 0 we have\n\n<!-- formula-not-decoded -->\n\nand\n\n<!-- formula-not-decoded -->\n\nIn particular normalization methods like batch normalization would promote the mean as a fixed point.\n\nWeconsider the differences of dot products for x i : x x T i i -x x T i j = x T i ( x i -x j ) , for fixed point m ∗ x : ( m ∗ x ) T x i -( m ∗ x ) T x j = ( m ∗ x ) T ( x i -x j ) , and for the center m x : m x T x i -m x T x j = m x T x ( i -x j ) . Using the Cauchy-Schwarz inequality, we get\n\n<!-- formula-not-decoded -->\n\nThis inequality gives:\n\n<!-- formula-not-decoded -->\n\nwhere we used ‖ ξ -0 ‖ glyph[lessorequalslant] ‖ ξ -m x ‖ + ‖ m x -0 ‖ , ‖ ξ -m x ‖ = ‖ ∑ i p i x i -m x ‖ glyph[lessorequalslant] ∑ i p i ‖ x i -m x ‖ glyph[lessorequalslant] m max , and M = max i ‖ x i ‖ . In particular\n\n<!-- formula-not-decoded -->\n\nLet i = arg max j ξ T x j , therefore the maximal softmax component is i . For the maximal softmax component i we have:\n\n<!-- formula-not-decoded -->\n\nglyph[negationslash]\n\n<!-- formula-not-decoded -->\n\nglyph[negationslash]\n\nAnalogously we obtain for i = arg max j m x T x j , a bound on the maximal softmax component i if the center is put into the iteration:\n\n<!-- formula-not-decoded -->\n\nAnalog we obtain a bound for i = arg max ( j m ∗ x ) T x j on the maximal softmax component i of the fixed point:\n\n<!-- formula-not-decoded -->\n\nThe two important terms are m max , the variance or spread of the data and ‖ m x ‖ , which tells how well the data is centered. For a contraction mapping we already required βm 2 max &lt; 1 , therefore the first term in the exponent is 2 βm 2 max &lt; 2 . The second term 2 βm max ‖ m x ‖ is small if the data is centered.\n\n· Global fixed point near the global mean: Analysis using softmax values.\n\nIf ξ T x i ≈ ξ T x j for all i and j , then p i ≈ 1 /N and we have m = max i p i (1 -p i ) &lt; 1 /N . For M glyph[lessorequalslant] 1 / √ 2 β we obtain from Lemma A2:\n\n<!-- formula-not-decoded -->\n\nThe local fixed point is m ∗ x ≈ m x = (1 /N ) ∑ N i =1 x i with p i ≈ 1 /N .\n\nWe now treat this case more formally. First we discuss conditions that ensure that the iteration is a contraction mapping. We consider the iteration Eq. (57) in the variable p :\n\n<!-- formula-not-decoded -->\n\nThe Jacobian is\n\nwith\n\n<!-- formula-not-decoded -->\n\nThe version of the mean value theorem in Lemma A32 states for J m = ∫ 1 0 J( λ p ) d λ = X X T J m s with the symmetric matrix J m s = ∫ 1 0 J ( s λ p ) d λ :\n\n<!-- formula-not-decoded -->\n\nWith m = max i p i (1 -p i ) , Eq. (476) from Lemma A24 is\n\n<!-- formula-not-decoded -->\n\nFirst observe that λp i (1 -λp i ) glyph[lessorequalslant] p i (1 -p i ) for p i glyph[lessorequalslant] 0 5 . and λ ∈ [0 , 1] , since p i (1 -p i ) -λp i (1 -λp i ) = (1 -λ p ) i (1 -(1 + λ p ) i ) ≥ 0 . For max i p i glyph[lessorequalslant] 0 5 . this observation leads to the following bound for J m s :\n\n<!-- formula-not-decoded -->\n\nEq. (479) in Lemma A24 states that every J s is bounded by 1 / β 2 , therefore also the mean:\n\n<!-- formula-not-decoded -->\n\nSince m = max i p i (1 -p i ) &lt; max i p i = p max , the previous bounds can be combined as follows:\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nConsequently,\n\n<!-- formula-not-decoded -->\n\nwhere we used Eq. (170). ∥ ∥ X X T ∥ ∥ 2 = ∥ ∥ XX T ∥ ∥ 2 , therefore ∥ ∥ X X T ∥ ∥ 2 is N times the maximal second moment of the data squared.\n\nObviously, g ( p ) is a contraction mapping in compact sets, where\n\n<!-- formula-not-decoded -->\n\nS is the sphere around the origin 0 with radius one. For\n\n<!-- formula-not-decoded -->\n\nwe have ‖ p ‖ glyph[lessorequalslant] ‖ p ‖ 1 = 1 and ‖ p new ‖ glyph[lessorequalslant] ‖ p new ‖ 1 = 1 . Therefore, g maps points from S into S . g is a contraction mapping for\n\n<!-- formula-not-decoded -->\n\nAccording to Banach fixed point theorem g has a fixed point in the sphere S .\n\nHölder's inequality gives:\n\n<!-- formula-not-decoded -->\n\nAlternatively:\n\n<!-- formula-not-decoded -->\n\nLet now S be the sphere around the origin 0 with radius 1 / √ N + √ p max and let ‖ J m ( p ) ‖ 2 glyph[lessorequalslant] c &lt; 1 for p ∈ S . The old p is in the sphere S ( p ∈ S ) since p max &lt; √ p max for p max &lt; 1 . We have\n\n<!-- formula-not-decoded -->\n\nTherefore, g is a mapping from S into S and a contraction mapping. According to Banach fixed point theorem, a fixed point exists in S .\n\nFor the 1-norm, we use Lemma A24 and ‖ p ‖ 1 = 1 to obtain from Eq. (115):\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nwhere m = max i p i (1 -p i ) , M 1 = ‖ X ‖ 1 = max i ‖ x i ‖ 1 , M = max i ‖ x i ‖ , ‖ X ‖ ∞ = ∥ ∥ X T ∥ ∥ 1 = max i ∥ ∥ [ X T ] i ∥ ∥ 1 (maximal absolute row sum norm), and M ∞ = max i ‖ x i ‖ ∞ . Let us quickly mention some auxiliary estimates related to X X T :\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nwhere the first inequaltiy is from Hölder's inequality. We used\n\n<!-- formula-not-decoded -->\n\nwhere the first inequality is from Hölder's inequality (here the same as the Cauchy-Schwarz inequality). See proof of Lemma A24 for the 1-norm bound on J s . Everything else follows from the fact that the 1-norm is sub-multiplicative as induced matrix norm.\n\nWe consider the minimal ‖ p ‖ .\n\n<!-- formula-not-decoded -->\n\nThe solution to this minimization problem is p = (1 /N ) 1 . Therefore, we have 1 / √ N glyph[lessorequalslant] ‖ p ‖ and 1 /N glyph[lessorequalslant] ‖ p ‖ 2 Using Eq. (119) we obtain\n\n<!-- formula-not-decoded -->\n\nMoreover\n\n<!-- formula-not-decoded -->\n\nsince p new ∈ S and p ∈ S .\n\nFor the fixed point, we have\n\n<!-- formula-not-decoded -->\n\nand hence\n\n<!-- formula-not-decoded -->\n\nTherefore, for small ‖ J m ‖ 2 we have p ∗ ≈ (1 /N ) 1 .\n\nA.1.5.3 Many Stable States: Fixed Points Near Stored Patterns. We move on to the next case, where the patterns x i are well separated. In this case the iteration goes to the pattern to which the initial ξ is most similar. If the initial ξ is similar to a vector x i then it will converge to x i and p will be e i . The main ingredients are again Banach's Theorem and estimates on the Jacobian norm.",
        "metadata": {
            "section_header": "A.1.5.1 General Bound on the Jacobian of the Iteration. We consider the iteration",
            "title": "HOPFIELD NETWORKS IS ALL YOU NEED",
            "type": "paper"
        }
    },
    {
        "page_content": "## · Proof of a fixed point by Banach Fixed Point Theorem.\n\n→ Mapped Vectors Stay in a Compact Environment. We show that if x i is sufficient dissimilar to other x j then there is an compact environment of x i (a sphere) where the fixed point iteration maps this environment into itself. The idea of the proof is to define a sphere around x i for which points from the sphere are mapped by f into the sphere.\n\nWe first need following lemma which bounds the distance ‖ x i -f ( ξ ) ‖ , where x i is the pattern that is least separated from ξ but separated from other patterns.\n\nglyph[negationslash]\n\nLemma A4. For a query ξ and data X = ( x 1 , . . . , x N ) , there exists a x i that is least separated from ξ while being separated from other x j with j = i :\n\nglyph[negationslash]\n\nglyph[negationslash]\n\n<!-- formula-not-decoded -->\n\nglyph[negationslash]\n\nglyph[negationslash]\n\n<!-- formula-not-decoded -->\n\nFor x i , the following holds:\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nglyph[epsilon1]\n\n= (\n\nN\n\n-\n\n1) exp(\n\n-\n\nβ c\n\n)\n\n.\n\n(134)\n\nwhere\n\nProof. For the softmax component i we have:\n\nglyph[negationslash]\n\nglyph[negationslash]\n\n<!-- formula-not-decoded -->\n\nglyph[negationslash]\n\nFor softmax components k = i we have\n\nglyph[negationslash]\n\n<!-- formula-not-decoded -->\n\nThe iteration f can be written as\n\n<!-- formula-not-decoded -->\n\nWe now can bound ‖ x i -f ( ξ ) ‖ :\n\nglyph[negationslash]\n\n<!-- formula-not-decoded -->\n\nglyph[negationslash]\n\nWe define ∆ i , i.e. the separation of pattern x i from data X = ( x 1 , . . . , x N ) as:\n\nglyph[negationslash]\n\nglyph[negationslash]\n\n<!-- formula-not-decoded -->\n\nThe pattern is separated from the other data if 0 &lt; ∆ i . Using the parallelogram identity, ∆ i can also be expressed as\n\nglyph[negationslash]\n\n<!-- formula-not-decoded -->\n\nglyph[negationslash]\n\nglyph[negationslash]\n\nFor ‖ x i ‖ = ‖ x j ‖ we have ∆ = 1 2min i / j,j = i ‖ x i -x j ‖ 2 .\n\nNext we define the sphere where we want to apply Banach fixed point theorem.\n\nDefinition 3 (Sphere S i ) . The sphere S i is defined as\n\n<!-- formula-not-decoded -->\n\nLemma A5. With ξ given, if the assumptions\n\nglyph[negationslash]",
        "metadata": {
            "section_header": "Proof of a fixed point by Banach Fixed Point Theorem.",
            "title": "HOPFIELD NETWORKS IS ALL YOU NEED",
            "type": "paper"
        }
    },
    {
        "page_content": "## A1: ξ is inside sphere: ξ ∈ S i ,\n\nA2: data point x i is well separated from the other data:\n\n<!-- formula-not-decoded -->\n\nhold, then f ( ξ ) is inside the sphere: f ( ξ ) ∈ S i . Therefore, with assumption (A2), f is a mapping from S i into S i .\n\nProof. We need the separation ˜ ∆ i of ξ from the data.\n\nglyph[negationslash]\n\n<!-- formula-not-decoded -->\n\nUsing the Cauchy-Schwarz inequality, we obtain for 1 glyph[lessorequalslant] j glyph[lessorequalslant] N :\n\n<!-- formula-not-decoded -->\n\nWe have the lower bound\n\nglyph[negationslash]\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nwhere we used the assumption (A1) of the lemma.\n\nFrom the proof in Lemma A4 we have\n\n<!-- formula-not-decoded -->\n\nLemma A4 states that\n\n<!-- formula-not-decoded -->\n\nWe have\n\n<!-- formula-not-decoded -->\n\nwhere we used assumption (A2) of the lemma. Therefore, f ( ξ ) is a mapping from the sphere S i into the sphere S i : If ξ ∈ S i then f ( ξ ) ∈ S i .\n\n· Contraction mapping.\n\nFor applying Banach fixed point theorem we need to show that f is contraction in the compact environment S i .\n\nLemma A6. Assume that\n\nA1:\n\n<!-- formula-not-decoded -->\n\nthen f is a contraction mapping in S i .\n\nglyph[negationslash]\n\nProof. The version of the mean value theorem Lemma A32 states for J m = ∫ 1 0 J( λ ξ +(1 -λ ) x i ) d λ :\n\n<!-- formula-not-decoded -->\n\nTherefore\n\n<!-- formula-not-decoded -->\n\nWe define ˜ = ξ λ ξ +(1 -λ ) x i for some λ ∈ [0 , 1] . From the proof in Lemma A4 we have\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nglyph[negationslash]\n\n<!-- formula-not-decoded -->\n\nFirst we compute an upper bound on ˜ glyph[epsilon1] . We need the separation ˜ ∆ i of ξ from the data. Using the Cauchy-Schwarz inequality, we obtain for 1 glyph[lessorequalslant] j glyph[lessorequalslant] N :\n\n<!-- formula-not-decoded -->\n\nWe have the lower bound on ˜ ∆ i :\n\nglyph[negationslash]\n\n<!-- formula-not-decoded -->\n\nglyph[negationslash]\n\nwhere we used ∥ ∥ ∥ ˜ ξ -x i ∥ ∥ ∥ = λ ‖ ξ -x i ‖ glyph[lessorequalslant] ‖ ξ -x i ‖ . From the definition of ˜ glyph[epsilon1] in Eq. (152) we have\n\n<!-- formula-not-decoded -->\n\nwhere we used ξ ∈ S i , therefore ‖ ξ -x i ‖ glyph[lessorequalslant] 1 β N M .\n\nNext we compute an lower bound on ˜ glyph[epsilon1] . We start with an upper on ˜ ∆ i :\n\nglyph[negationslash]\n\n<!-- formula-not-decoded -->\n\nglyph[negationslash]\n\nwhere we used ∥ ∥ ∥ ˜ ξ -x i ∥ ∥ ∥ = λ ‖ ξ -x i ‖ glyph[lessorequalslant] ‖ ξ -x i ‖ . From the definition of ˜ glyph[epsilon1] in Eq. (152) we have\n\n<!-- formula-not-decoded -->\n\nwhere we used ξ ∈ S i , therefore ‖ ξ -x i ‖ glyph[lessorequalslant] 1 β N M .\n\nglyph[negationslash]\n\nNow we bound the Jacobian. We can assume ˜ glyph[epsilon1] glyph[lessorequalslant] 0 5 . otherwise (1 -˜) glyph[epsilon1] glyph[lessorequalslant] 0 5 . in the following. From the proof of Lemma A24 we know for p max ( ˜ ) ξ ≥ 1 -˜ glyph[epsilon1] , then p i ( ˜ ) ξ glyph[lessorequalslant] ˜ glyph[epsilon1] for p i ( ˜ ) ξ = p max ( ˜ ) ξ .\n\nTherefore, p i ( ˜ )(1 ξ -p i ( ˜ )) ξ glyph[lessorequalslant] m glyph[lessorequalslant] ˜(1 glyph[epsilon1] -˜) glyph[epsilon1] for all i . Next we use the derived upper and lower bound on ˜ glyph[epsilon1] in previous Eq. (61) in Lemma A2:\n\n<!-- formula-not-decoded -->\n\nThe bound Eq. (160) holds for the mean J m , too, since it averages over J( ˜ ) ξ :\n\n<!-- formula-not-decoded -->\n\nThe assumption of the lemma is\n\n<!-- formula-not-decoded -->\n\nThis is\n\n<!-- formula-not-decoded -->\n\nTherefore, the spectral norm ‖ J ‖ 2 can be bounded by:\n\n<!-- formula-not-decoded -->\n\nTherefore, f is a contraction mapping in S i .\n\n· Banach Fixed Point Theorem. Now we have all ingredients to apply Banach fixed point theorem.",
        "metadata": {
            "section_header": "A1: ξ is inside sphere: ξ ∈ S i ,",
            "title": "HOPFIELD NETWORKS IS ALL YOU NEED",
            "type": "paper"
        }
    },
    {
        "page_content": "## Lemma A7. Assume that\n\nA1:\n\n<!-- formula-not-decoded -->\n\nthen f has a fixed point in S i .\n\nProof. We use Banach fixed point theorem: Lemma A5 says that f maps from S i into S i . Lemma A6 says that f is a contraction mapping in S i .\n\n· Contraction mapping with a fixed point.\n\nWe have shown that a fixed point exists. We want to know how fast the iteration converges to the fixed point. Let x ∗ i be the fixed point of the iteration f in the sphere S i . Using the mean value theorem 1\n\nLemma A32, we have with J m = ∫ 0 J( λ ξ +(1 -λ ) x ∗ i ) d λ :\n\n<!-- formula-not-decoded -->\n\nAccording to Lemma A24, if p max = max i p i ≥ 1 -glyph[epsilon1] for all ˜ = x λ ξ +(1 -λ ) x ∗ i , then the spectral norm of the Jacobian is bounded by\n\n<!-- formula-not-decoded -->\n\nThe norm of Jacobian at ˜ x is bounded\n\n<!-- formula-not-decoded -->\n\nWe used that the spectral norm ‖ ‖ . 2 is bounded by the Frobenius norm ‖ ‖ . F which can be expressed by the norm squared of its column vectors:\n\n<!-- formula-not-decoded -->\n\nTherefore\n\n<!-- formula-not-decoded -->\n\nThe norm of Jacobian of the fixed point iteration is bounded\n\n<!-- formula-not-decoded -->\n\nThe separation of pattern x i from data X = ( x 1 , . . . , x N ) is\n\nglyph[negationslash]\n\nglyph[negationslash]\n\n<!-- formula-not-decoded -->\n\nWe need the separation ˜ ∆ i of ˜ = x λ ξ +(1 -λ ) x ∗ i from the data:\n\nglyph[negationslash]\n\n<!-- formula-not-decoded -->\n\nWe compute a lower bound on ∆ i . Using the Cauchy-Schwarz inequality, we obtain for 1 glyph[lessorequalslant] j glyph[lessorequalslant] N :\n\n˜ ∣ ∣ ˜ x x T j -x x T i j ∣ ∣ glyph[lessorequalslant] ‖ ˜ x -x i ‖ ‖ x j ‖ glyph[lessorequalslant] ‖ ˜ x -x i ‖ M . (174)\n\nWe have the lower bound\n\nglyph[negationslash]\n\n<!-- formula-not-decoded -->\n\nglyph[negationslash]\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nSince\n\nwe have\n\n<!-- formula-not-decoded -->\n\nFor the softmax component i we have:\n\nglyph[negationslash]\n\n<!-- formula-not-decoded -->\n\nglyph[negationslash]\n\n<!-- formula-not-decoded -->\n\nTherefore\n\n<!-- formula-not-decoded -->\n\nWe can bound the spectral norm of the Jacobian, which upper bounds the Lipschitz constant:\n\n<!-- formula-not-decoded -->\n\nFor a contraction mapping we require\n\n<!-- formula-not-decoded -->\n\nwhich can be ensured by\n\n<!-- formula-not-decoded -->\n\nSolving this inequality for ∆ i gives\n\n<!-- formula-not-decoded -->\n\nIn an environment around x ∗ i in which Eq. (183) holds, f is a contraction mapping and every point converges under the iteration f to x ∗ i when the iteration stays in the environment. After every iteration the mapped point f ( ξ ) is closer to the fixed point x ∗ i than the original point x i :\n\n<!-- formula-not-decoded -->\n\nUsing\n\n‖ f ( ξ ) -x ∗ i ‖ glyph[lessorequalslant] ‖ J m ‖ 2 ‖ ξ -x ∗ i ‖ glyph[lessorequalslant] ‖ J m ‖ 2 ‖ ξ -f ( ξ ) ‖ + ‖ J m ‖ 2 ‖ f ( ξ ) -x ∗ i ‖ , (185)\n\nwe obtain\n\n<!-- formula-not-decoded -->\n\nFor large ∆ i the iteration is close to the fixed point even after one update. This has been confirmed in several experiments.\n\nA.1.5.4 Metastable States: Fixed Points Near Mean of Similar Patterns. The proof concept is the same as for a single pattern but now for the arithmetic mean of similar patterns.\n\n· Bound on the Jacobian.\n\nThe Jacobian of the fixed point iteration is\n\n<!-- formula-not-decoded -->\n\nIf we consider p i as the probability of selecting the vector x i , then we can define expectations as E [ p f ( x )] = ∑ N i =1 p f i ( x i ) . In this setting the matrix\n\n<!-- formula-not-decoded -->\n\nis the covariance matrix of data X when its vectors are selected according to the probability p :\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\ntherefore we have\n\n<!-- formula-not-decoded -->\n\nWe now elaborate more on this interpretation as variance. Specifically the singular values of J (or in other words: the covariance) should be reasonably small. The singular values are the key to ensure convergence of the iteration Eq. (57). Next we present some thoughts.\n\n- 1. It's clear that the largest eigenvalue of the covariance matrix (equal to the largest singular value) is the variance in the direction of the eigenvector associated with the largest eigenvalue.\n- 2. Furthermore the variance goes to zero as one p i goes to one, since only one pattern is chosen and there is no variance.\n- 3. The variance is reasonable small if all patterns are chosen with equal probability.\n- 4. The variance is small if few similar patterns are chosen with high probability. If the patterns are sufficient similar, then the spectral norm of the covariance matrix is smaller than one.\n\nThe first three issues have already been adressed. Now we focus on the last one in greater detail. We assume that the first l patterns are much more probable (and similar to one another) than the other patterns. Therefore, we define:\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nM is an upper bound on the Euclidean norm of the patterns, which are vectors. glyph[epsilon1] is an upper bound on the probability γ of not choosing one of the first l patterns, while 1 -glyph[epsilon1] is a lower bound the probability (1 -γ ) of choosing one of the first l patterns. m x is the arithmetic mean (the center) of the first l patterns. m max is the maximal distance of the patterns to the center m x . ˜ p is the probability p normalized for the first l patterns.\n\nThe variance of the first l patterns is\n\n<!-- formula-not-decoded -->\n\nLemma A8. With the definitions in Eq. (193) to Eq. (200) , the following bounds on the norm ‖ J ‖ 2 of the Jacobian of the fixed point iteration hold. The γ -bound for ‖ J ‖ 2 is\n\n<!-- formula-not-decoded -->\n\nand the glyph[epsilon1] -bound for ‖ J ‖ 2 is:\n\n<!-- formula-not-decoded -->\n\nProof. The variance Var [ ˜ p x 1: l ] can be expressed as:\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nTherefore, we have\n\n<!-- formula-not-decoded -->\n\nWe now can reformulate the Jacobian J :\n\n<!-- formula-not-decoded -->\n\nThe spectral norm of an outer product of two vectors is the product of the Euclidean norms of the vectors:\n\n<!-- formula-not-decoded -->\n\nsince bb T has eigenvector b / ‖ b ‖ with eigenvalue ‖ b ‖ 2 and otherwise zero eigenvalues.\n\nWe now bound the norms of some matrices and vectors:\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nIn order to bound the variance of the first l patterns, we compute the vector a that minimizes\n\n<!-- formula-not-decoded -->\n\nThe solution to\n\nis\n\n<!-- formula-not-decoded -->\n\nThe Hessian of f is positive definite since\n\n<!-- formula-not-decoded -->\n\nand f is a convex function. Hence, the mean\n\n<!-- formula-not-decoded -->\n\nminimizes ∑ N i =1 p i ‖ x i -a ‖ 2 . Therefore, we have\n\n<!-- formula-not-decoded -->\n\nWe now bound the variance on the first l patterns:\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nWe obtain for the spectral norm of J :\n\n<!-- formula-not-decoded -->\n\nCombining the previous two estimates immediately leads to Eq. (201).\n\nThe function h x ( ) = x 2(2 -x ) has the derivative h ' ( x ) = 4(1 -x ) . Therefore, h x ( ) is monotone increasing for x &lt; 1 . For 0 glyph[lessorequalslant] γ glyph[lessorequalslant] glyph[epsilon1] &lt; 1 , we can immediately deduce that γ 2(2 -γ ) glyph[lessorequalslant] glyph[epsilon1] 2(2 -glyph[epsilon1] ) . Since glyph[epsilon1] is larger than γ , we obtain the following glyph[epsilon1] -bound for ‖ J ‖ 2 :\n\n<!-- formula-not-decoded -->\n\nWe revisit the bound on (1 -γ ) Var ˜ p [ x 1: l ] . The trace ∑ d k =1 e k is the sum of the eigenvalues e k . The spectral norm is equal to the largest eigenvalue e 1 , that is, the largest singular value. We obtain:\n\n<!-- formula-not-decoded -->\n\nTherefore, the tightness of the bound depends on eigenvalues which are not the largest. That is variations which are not along the strongest variation weaken the bound.\n\nWithout restricting the generality, we assume that the first l patterns are much more probable (and similar to one another) than the other patterns. Therefore, we define:\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nM is an upper bound on the Euclidean norm of the patterns, which are vectors. glyph[epsilon1] is an upper bound on the probability γ of not choosing one of the first l patterns, while 1 -glyph[epsilon1] is a lower bound the probability (1 -γ ) of choosing one of the first l patterns. m x is the arithmetic mean (the center) of the first l patterns. m max is the maximal distance of the patterns to the center m x . ˜ p is the probability p normalized for the first l patterns.\n\n· Mapped vectors stay in a compact environment. We show that if m x is sufficient dissimilar to other x j with l &lt; j then there is an compact environment of m x (a sphere) where the fixed point iteration maps this environment into itself. The idea of the proof is to define a sphere around m x for which the points from the sphere are mapped by f into the sphere.\n\nWe first need following lemma which bounds the distance ‖ m x -f ( ξ ) ‖ of a ξ which is close to m x .\n\nLemma A9. For a query ξ and data X = ( x 1 , . . . , x N ) , we define\n\n<!-- formula-not-decoded -->\n\nThe following holds:\n\n<!-- formula-not-decoded -->\n\nwhere\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nProof. Let s = arg max j,j glyph[lessorequalslant] l ξ T x j , therefore ξ T m x = 1 l ∑ l i =1 ξ T x i glyph[lessorequalslant] 1 l ∑ l i =1 ξ T x s = ξ T x s . For softmax components j with l &lt; j we have\n\nglyph[negationslash]\n\n<!-- formula-not-decoded -->\n\nsince ξ T x s -ξ T x j ≥ ξ T m x -ξ T x j for each j with l &lt; j , therefore ξ T x s -ξ T x j ≥ c\n\nThe iteration f can be written as\n\n<!-- formula-not-decoded -->\n\nWe set p i = [softmax( β X ξ T )] i , therefore ∑ l i =1 p i = 1 -γ ≥ 1 -glyph[epsilon1] and ∑ N i = +1 l p i = γ glyph[lessorequalslant] glyph[epsilon1] . Therefore\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nIt follows that\n\n<!-- formula-not-decoded -->\n\nWe now can bound ‖ m x -f ( ξ ) ‖ :\n\n<!-- formula-not-decoded -->\n\nwhere we applied Eq. (233) in the penultimate inequality. This is the statement of the lemma.\n\nThe separation of the center (the arithmetic mean) m x of the first l from data X = ( x l +1 , . . . , x N ) is ∆ m , defined as\n\n<!-- formula-not-decoded -->\n\nThe center is separated from the other data x j with l &lt; j if 0 &lt; ∆ m . By the same arguments as in Eq. (140), ∆ m can also be expressed as\n\n<!-- formula-not-decoded -->\n\nFor ‖ m x ‖ = ‖ x j ‖ we have ∆ m = 1 2min / j,l&lt;j ‖ m x -x j ‖ 2 .\n\nNext we define the sphere where we want to apply Banach fixed point theorem.\n\nDefinition 4 (Sphere S m ) . The sphere S m is defined as\n\n<!-- formula-not-decoded -->\n\nLemma A10. With ξ given, if the assumptions\n\nA1: ξ is inside sphere: ξ ∈ S m ,\n\nA2: the center m x is well separated from other data x j with l &lt; j :\n\n<!-- formula-not-decoded -->\n\nA3: the distance m max of similar patterns to the center is sufficient small:\n\n<!-- formula-not-decoded -->\n\nhold, then f ( ξ ) ∈ S m . Therefore, under conditions (A2) and (A3), f is a mapping from S m into S m .\n\nProof. We need the separation ˜ ∆ m of ξ from the rest of the data, which is the last N -l data points X = ( x l +1 , . . . , x N ) .\n\n<!-- formula-not-decoded -->\n\nUsing the Cauchy-Schwarz inequality, we obtain for l +1 glyph[lessorequalslant] j glyph[lessorequalslant] N :\n\n<!-- formula-not-decoded -->\n\nWe have the lower bound\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nwhere we used the assumption (A1) of the lemma.\n\nFrom the proof in Lemma A9 we have\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nLemma A9 states that\n\n<!-- formula-not-decoded -->\n\nTherefore, we have\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nwhere we used assumption (A2) of the lemma. Therefore, f ( ξ ) is a mapping from the sphere S m into the sphere S m .\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n· Contraction mapping.\n\nFor applying Banach fixed point theorem we need to show that f is contraction in the compact environment S m .",
        "metadata": {
            "section_header": "Lemma A7. Assume that",
            "title": "HOPFIELD NETWORKS IS ALL YOU NEED",
            "type": "paper"
        }
    },
    {
        "page_content": "## Lemma A11. Assume that\n\nA1:\n\n<!-- formula-not-decoded -->\n\nand\n\nA2:\n\n<!-- formula-not-decoded -->\n\nthen f is a contraction mapping in S m .\n\nProof. The version of the mean value theorem Lemma A32 states for the symmetric J m = ∫ 1 0 J( λ ξ + (1 -λ ) m x ) d λ :\n\n<!-- formula-not-decoded -->\n\nIn complete analogy to Lemma A6, we get:\n\n<!-- formula-not-decoded -->\n\nWe define ˜ = ξ λ ξ +(1 -λ ) m x for some λ ∈ [0 , 1] . We need the separation ˜ ∆ m of ˜ ξ from the rest of the data, which is the last N -l data points X = ( x l +1 , . . . , x N ) .\n\n<!-- formula-not-decoded -->\n\nFrom the proof in Lemma A9 we have\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nWe first compute an upper bound on ˜ glyph[epsilon1] . Using the Cauchy-Schwarz inequality, we obtain for l +1 glyph[lessorequalslant] j glyph[lessorequalslant] N :\n\n<!-- formula-not-decoded -->\n\nWe have the lower bound on ˜ ∆ m :\n\n<!-- formula-not-decoded -->\n\nwhere we used ∥ ∥ ˜ ξ -m x ∥ ∥ = λ ‖ ξ -m x ‖ glyph[lessorequalslant] ‖ ξ -m x ‖ . We obtain the upper bound on ˜ glyph[epsilon1]\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nwhere we used that in the sphere S i holds:\n\n<!-- formula-not-decoded -->\n\ntherefore\n\n<!-- formula-not-decoded -->\n\nNext we compute a lower bound on ˜ glyph[epsilon1] and to this end start with the upper bound on ˜ ∆ m using the same arguments as in Eq. (158) in combination with Eq. (266).\n\n<!-- formula-not-decoded -->\n\nwhere we used ∥ ∥ ∥ ˜ ξ -m x ∥ ∥ ∥ = λ ‖ ξ -m x ‖ glyph[lessorequalslant] ‖ ξ -m x ‖ . We obtain the lower bound on ˜ glyph[epsilon1] :\n\n<!-- formula-not-decoded -->\n\nwhere we used that in the sphere S i holds:\n\n<!-- formula-not-decoded -->\n\ntherefore\n\n<!-- formula-not-decoded -->\n\nFrom Lemma A8 we have\n\n<!-- formula-not-decoded -->\n\nThe bound Eq. (271) holds for the mean J m , too, since it averages over J( ˜ ) ξ :\n\n<!-- formula-not-decoded -->\n\nThe assumption of the lemma is\n\n<!-- formula-not-decoded -->\n\nTherefore, we have\n\n<!-- formula-not-decoded -->\n\nTherefore, the spectral norm ‖ J m ‖ 2 can be bounded by:\n\n<!-- formula-not-decoded -->\n\nFor the last but one inequality we used 2 M glyph[lessorequalslant] max { m max , 2 M } .\n\nTherefore, f is a contraction mapping in S m .\n\n· Banach Fixed Point Theorem. Now we have all ingredients to apply Banach fixed point theorem.",
        "metadata": {
            "section_header": "Lemma A11. Assume that",
            "title": "HOPFIELD NETWORKS IS ALL YOU NEED",
            "type": "paper"
        }
    },
    {
        "page_content": "## Lemma A12. Assume that\n\nA1:\n\n<!-- formula-not-decoded -->\n\nand\n\nA2:\n\n<!-- formula-not-decoded -->\n\nthen f has a fixed point in S m .\n\nProof. We use Banach fixed point theorem: Lemma A10 says that f maps from the compact set S m into the same compact set S m . Lemma A11 says that f is a contraction mapping in S m .",
        "metadata": {
            "section_header": "Lemma A12. Assume that",
            "title": "HOPFIELD NETWORKS IS ALL YOU NEED",
            "type": "paper"
        }
    },
    {
        "page_content": "## · Contraction mapping with a fixed point.\n\nWe assume that the first l patterns are much more probable (and similar to one another) than the other patterns. Therefore, we define:\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nM is an upper bound on the Euclidean norm of the patterns, which are vectors. glyph[epsilon1] is an upper bound on the probability γ of not choosing one of the first l patterns, while 1 -glyph[epsilon1] is a lower bound the probability (1 -γ ) of choosing one of the first l patterns. m x is the arithmetic mean (the center) of the first l patterns. m max is the maximal distance of the patterns to the center m x . ˜ p is the probability p normalized for the first l patterns.\n\nThe variance of the first l patterns is\n\n<!-- formula-not-decoded -->\n\nWe have shown that a fixed point exists. We want to know how fast the iteration converges to the fixed point. Let m ∗ x be the fixed point of the iteration f in the sphere S m . Using the mean value theorem Lemma A32, we have with J m = ∫ 1 0 J( λ ξ +(1 -λ ) m ∗ x ) d λ :\n\n<!-- formula-not-decoded -->\n\nAccording to Lemma A8 the following bounds on the norm ‖ J ‖ 2 of the Jacobian of the fixed point iteration hold. The γ -bound for ‖ J ‖ 2 is\n\n<!-- formula-not-decoded -->\n\nwhile the glyph[epsilon1] -bound for ‖ J ‖ 2 is:\n\n<!-- formula-not-decoded -->\n\nFrom the last condition we require for a contraction mapping:\n\n<!-- formula-not-decoded -->\n\nWe want to see how large glyph[epsilon1] is. The separation of center m x from data X = ( x l +1 , . . . , x N ) is\n\n<!-- formula-not-decoded -->\n\nWe need the separation ˜ ∆ m of ˜ = x λ ξ +(1 -λ ) m ∗ x from the data.\n\n<!-- formula-not-decoded -->\n\nWe compute a lower bound on ˜ ∆ m . Using the Cauchy-Schwarz inequality, we obtain for 1 glyph[lessorequalslant] j glyph[lessorequalslant] N :\n\n<!-- formula-not-decoded -->\n\nWe have the lower bound\n\n<!-- formula-not-decoded -->\n\nSince\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->",
        "metadata": {
            "section_header": "Contraction mapping with a fixed point.",
            "title": "HOPFIELD NETWORKS IS ALL YOU NEED",
            "type": "paper"
        }
    },
    {
        "page_content": "## A.1.6 PROPERTIES OF FIXED POINTS NEAR STORED PATTERN\n\nIn Subsection A.1.5.3 many stable states that are fixed points near the stored patterns are considered. We now consider this case. In the fist subsection we investigate the storage capacity if all patterns are sufficiently separated so that metastable states do not appear. In the next subsection we look into the updates required and error when retrieving the stored patterns. For metastable states we can do the same analyses if each metastable state is treated as one state like one pattern.\n\nWe see a trade-off that is known from classical Hopfield networks and for modern Hopfield networks. Small separation ∆ i of the pattern x i from the other patterns gives high storage capacity. However the convergence speed is lower and the retrieval error higher. In contrast, large separation ∆ i of the pattern x i from the other pattern allows the retrieval of patterns with one update step and exponentially low error.\n\nwe have\n\nA.1.6.1 Exponentially Many Patterns can be Stored. From Subsection A.1.5.3 need some definitions. We assume to have N patterns, the separation of pattern x i from the other patterns { x 1 , . . . , x i -1 , x i +1 , . . . , x N } is ∆ i , defined as\n\nglyph[negationslash]\n\nglyph[negationslash]\n\n<!-- formula-not-decoded -->\n\nThe pattern is separated from the other data if 0 &lt; ∆ i . The separation ∆ i can also be expressed as\n\nglyph[negationslash]\n\n<!-- formula-not-decoded -->\n\nglyph[negationslash]\n\nglyph[negationslash]\n\nFor ‖ x i ‖ = ‖ x j ‖ we have ∆ = 1 2min i / j,j = i ‖ x i -x j ‖ 2 . The sphere S i with center x i is defined as\n\n<!-- formula-not-decoded -->\n\nThe maximal length of a pattern is M = max i ‖ x i ‖ .\n\nWe next define what we mean with storing and retrieving a pattern.\n\nglyph[negationslash]\n\nDefinition 5 (Pattern Stored and Retrieved) . We assume that around every pattern x i a sphere S i is given. We say x i is stored if there is a single fixed point x ∗ i ∈ S i to which all points ξ ∈ S i converge, and S i ∩ S j = ∅ for i = j . We say x i is retrieved for a given glyph[epsilon1] if iteration (update rule) Eq. (92) gives a point ˜ x i that is at least glyph[epsilon1] -close to the single fixed point x ∗ i ∈ S i . The retrieval error is ‖ ˜ x i -x i ‖ .\n\nThe sphere S i around pattern x i can be any a sphere and do not have the specific sphere defined in Def. 3.\n\nFor a query\n\nξ\n\n∈\n\nS\n\ni\n\nto converge to a fixed point\n\nx\n\n∗\n\ni\n\n∈\n\nS\n\ni\n\nwe required for the application of Banach\n\nfixed point theorem and for ensuring a contraction mapping the following inequality:\n\n<!-- formula-not-decoded -->\n\nThis is the assumption in Lemma A7 to ensure a fixed point in sphere S i . Since replacing ( N -1) N by N 2 gives\n\n<!-- formula-not-decoded -->\n\nthe inequality follows from following master inequality\n\n<!-- formula-not-decoded -->\n\nglyph[negationslash]\n\nglyph[negationslash]\n\nIf we assume that S i ∩ S j = ∅ with i = j , then the triangle inequality with a point from the intersection gives\n\n<!-- formula-not-decoded -->\n\nTherefore, we have using the Cauchy-Schwarz inequality:\n\n<!-- formula-not-decoded -->\n\nThe last inequality is a contraction to Eq. (302) if we assume that\n\n<!-- formula-not-decoded -->\n\nWith this assumption, the spheres S i and S j do not intersect. Therefore, each x i has its separate fixed point in S i . We define\n\n<!-- formula-not-decoded -->\n\nto obtain the master inequality\n\n<!-- formula-not-decoded -->\n\n· Patterns on a sphere.\n\nFor simplicity and in accordance with the results of the classical Hopfield network, we assume all patterns being on a sphere with radius M :\n\n<!-- formula-not-decoded -->\n\nUnder assumption Eq. (305) we have only to show that the master inequality Eq. (307) is fulfilled for each x i to have a separate fixed point near each x i .\n\nWe defined α ij as the angle between x i and x j . The minimal angle α min between two data points is\n\n<!-- formula-not-decoded -->\n\nOn the sphere with radius M we have\n\n<!-- formula-not-decoded -->\n\ntherefore it is sufficient to show the master inequality on the sphere:\n\n<!-- formula-not-decoded -->\n\nUnder assumption Eq. (305) we have only to show that the master inequality Eq. (307) is fulfilled for ∆ min . We consider patterns on the sphere, therefore the master inequality Eq. (307) becomes Eq. (311). First we show results when pattern positions on the sphere are constructed and ∆ min is ensured. Then we move on to random patterns on a sphere, where ∆ min becomes a random variable.\n\n· Storage capacity for patterns placed on the sphere.\n\nNext theorem says how many patterns we can stored (fixed point with attraction basin near pattern) if we are allowed to place them on the sphere.\n\nTheorem A3 (Storage Capacity (M=2): Placed Patterns) . We assume β = 1 and patterns on the sphere with radius M . If M = 2 √ d -1 and the dimension d of the space is d ≥ 4 or if M = 1 7 . √ d -1 and the dimension d of the space is d ≥ 50 , then the number of patterns N that can be stored (fixed point with attraction basin near pattern) is at least\n\n<!-- formula-not-decoded -->\n\nProof. For random patterns on the sphere, we have to show that the master inequality Eq. (311) holds:\n\n<!-- formula-not-decoded -->\n\nWe now place the patterns equidistant on the sphere where the pattern are separated by an angle α min :\n\nglyph[negationslash]\n\n<!-- formula-not-decoded -->\n\nIn a d -dimensional space we can place\n\n<!-- formula-not-decoded -->\n\npoints on the sphere. In a spherical coordinate system a pattern differs from its most closest patterns by an angle α min and there are d -1 angles. Solving for α min gives\n\n<!-- formula-not-decoded -->\n\nThe number of patterns that can be stored is determined by the largest N that fulfils\n\n<!-- formula-not-decoded -->\n\nWe set N = 2 2( d -1) and obtain for Eq. (317):\n\n<!-- formula-not-decoded -->\n\nThis inequality is equivalent to\n\n<!-- formula-not-decoded -->\n\nThe last inequality can be fulfilled with M = K √ d -1 and proper K . For β = 1 , d = 4 and K = 2 the inequality is fulfilled. The left hand side minus the right hand side is 4( d -1) -1 2 / 2( d -1) -1 -ln(8( d -1)) -4( d -1) ln 2 . Its derivative with respect to d is strict positive. Therefore, the inequality holds for d ≥ 4 .\n\nFor β = 1 , d = 50 and K = 1 7 . the inequality is fulfilled. The left hand side minus the right hand side is 2 89( . d -1) -1 2 / 2( d -1) -1 -ln(5 78( . d -1)) -4( d -1) ln 2 . Its derivative with respect to d is strict positive. Therefore, the inequality holds for d ≥ 50 .\n\nIf we want to store considerably more patterns, then we have to increase the length of the vectors or the dimension of the space where the vectors live. The next theorem shows results for the number of patterns N with N = 2 3( d -1) .\n\nTheorem A4 (Storage Capacity (M=5): Placed Patterns) . We assume β = 1 and patterns on the sphere with radius M . If M = 5 √ d -1 and the dimension d of the space is d ≥ 3 or if M = 4 √ d -1 and the dimension d of the space is d ≥ 13 , then the number of patterns N that can be stored (fixed point with attraction basin near pattern) is at least\n\n<!-- formula-not-decoded -->\n\nProof. We set N = 2 3( d -1) and obtain for Eq. (317):\n\n<!-- formula-not-decoded -->\n\nThis inequality is equivalent to\n\n<!-- formula-not-decoded -->\n\nThe last inequality can be fulfilled with M = K √ d -1 and proper K . For β = 1 , d = 13 and K = 4 the inequality is fulfilled. The left hand side minus the right hand side is 4 686292( . d -1) -1 2 / 3( d -1) -1 -ln(32( d -1)) -6( d -1) ln 2 . Its derivative with respect to d is strict positive. Therefore, the inequality holds for d ≥ 13 .\n\nFor β = 1 , d = 3 and K = 5 the inequality is fulfilled. The left hand side minus the right hand side is 7 32233( . d -1) -1 / 2 3( d -1) -1 -ln(50( d -1)) -6( d -1) ln 2 . Its derivative with respect to d is strict positive. Therefore, the inequality holds for d ≥ 3 .\n\n<!-- image -->\n\nNext we investigate random points on the sphere. Under assumption Eq. (305) we have to show that the master inequality Eq. (311) is fulfilled for α min , where now α min is now a random variable. We use results on the distribution of the minimal angles between random patterns on a sphere according to Cai et al. (2013) and Brauchart et al. (2018). Theorem 2 in Cai et al. (2013) gives the distribution of the minimal angle for random patterns on the unit sphere. Proposition 3.5 in Brauchart et al. (2018) gives a lower bound on the probability of the minimal angle being larger than a given constant. We require this proposition to derive the probability of pattern having a minimal angle α min . Proposition 3.6 in Brauchart et al. (2018) gives the expectation of the minimal angle.\n\nWe will prove high probability bounds for the expected storage capacity. We need the following tail-bound on α min (the minimal angle of random patterns on a sphere):\n\nLemma A13 ((Brauchart et al., 2018)) . Let d be the dimension of the pattern space,\n\n<!-- formula-not-decoded -->\n\nand δ &gt; 0 such that κ d -1 2 δ ( d -1) glyph[lessorequalslant] 1 . Then\n\n<!-- formula-not-decoded -->\n\nProof. The statement of the lemma is Eq. (3-6) from Proposition 3.5 in Brauchart et al. (2018).\n\nNext we derive upper and lower bounds on the constant κ d since we require them later for proving storage capacity bounds.\n\nLemma A14. For κ d defined in Eq. (323) we have the following bounds for every d ≥ 1 :\n\n<!-- formula-not-decoded -->\n\nProof. We use for x &gt; 0 the following bound related to Stirling's approximation formula for the gamma function, c.f. (Olver et al., 2010, (5.6.1)):\n\n<!-- formula-not-decoded -->\n\nUsing Stirling's formula Eq. (326), we upper bound κ d :\n\n<!-- formula-not-decoded -->\n\nFor the first inequality, we applied Eq. (326), while for the second we used (1 + 1 d ) d &lt; e for d ≥ 1 . Next, we lower bound κ d by again applying Stirling's formula Eq. (326):\n\n<!-- formula-not-decoded -->\n\nwhere the last inequality holds because of monotonicity of (1 + 1 d ) d and using the fact that for d = 1 it takes on the value 2.\n\nWe require a bound on cos to bound the master inequality Eq. (311).\n\nLemma A15. For 0 glyph[lessorequalslant] x glyph[lessorequalslant] π the function cos can be upper bounded by:\n\n<!-- formula-not-decoded -->\n\nProof. We use the infinite product representation of cos , c.f. (Olver et al., 2010, (4.22.2)):\n\n<!-- formula-not-decoded -->\n\nSince it holds that\n\n<!-- formula-not-decoded -->\n\nfor | x | glyph[lessorequalslant] π and n ≥ 2 , we can get the following upper bound on Eq. (330):\n\n<!-- formula-not-decoded -->\n\nThe last but one inequality uses x glyph[lessorequalslant] π , which implies x/π glyph[lessorequalslant] 1 . Thus Eq. (329) is proven.\n\n· Exponential storage capacity: the base c as a function of the parameter β , the radius of the sphere M , the probability p , and the dimension d of the space.\n\nWe express the number N of stored patterns by an exponential function with base c &gt; 1 and an exponent linear in d . We derive constraints on he base c as a function of β , the radius of the sphere M , the probability p that all patterns can be stored, and the dimension d of the space. With β &gt; 0 , K &gt; 0 , and d ≥ 2 (to ensure a sphere), the following theorem gives our main result.\n\nTheorem A5 (Storage Capacity (Main): Random Patterns) . We assume a failure probability 0 &lt; p glyph[lessorequalslant] 1 and randomly chosen patterns on the sphere with radius M := K √ d -1 . We define\n\n<!-- formula-not-decoded -->\n\nwhere W 0 is the upper branch of the Lambert W function (Olver et al., 2010, (4.13)) and ensure\n\n<!-- formula-not-decoded -->\n\nThen with probability 1 -p , the number of random patterns that can be stored is\n\n<!-- formula-not-decoded -->\n\nTherefore it is proven for c ≥ 3 1546 . with β = 1 , K = 3 , d = 20 and p = 0 001 . ( a +ln( ) b &gt; 1 27 . ) and proven for c ≥ 1 3718 . with β = 1 , K = 1 , d = 75 , and p = 0 001 . ( a +ln( ) b &lt; -0 94 . ).\n\nProof. We consider the probability that the master inequality Eq. (311) is fulfilled:\n\n<!-- formula-not-decoded -->\n\nUsing Eq. (329), we have:\n\n<!-- formula-not-decoded -->\n\nTherefore, with probability 1 -p the storage capacity is largest N that fulfills\n\n<!-- formula-not-decoded -->\n\nThis inequality is equivalent to\n\n<!-- formula-not-decoded -->\n\nWe use Eq. (324) to obtain:\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nFor Eq. (339) to be fulfilled, it is sufficient that\n\n<!-- formula-not-decoded -->\n\nIf we insert the assumption Eq. (334) of the theorem into Eq. (335), then we obtain N ≥ 2 . We now apply the upper bound κ d -1 / 2 &lt; κ d -1 &lt; 1 from Eq. (325) and the upper bound 2 βN glyph[lessorequalslant] 1 β from N ≥ 2 to inequality Eq. (341). In the resulting inequality we insert N = √ pc d -1 4 to check whether it is fulfilled with this special value of N and obtain:\n\n<!-- formula-not-decoded -->\n\nDividing by p , inserting M = K √ d -1 , and exponentiation of the left and right side by 2 d -1 gives:\n\n<!-- formula-not-decoded -->\n\nAfter some algebraic manipulation, this inequality can be written as\n\n<!-- formula-not-decoded -->\n\nwhere we used\n\n<!-- formula-not-decoded -->\n\nWe determine the value ˆ c of c which makes the inequality Eq. (344) equal to zero. We solve\n\n<!-- formula-not-decoded -->\n\nfor ˆ c :\n\n<!-- formula-not-decoded -->\n\nwhere W 0 is the upper branch of the Lambert W function (see Def. A6). Hence, the solution is\n\n<!-- formula-not-decoded -->\n\nThe solution exist, since the Lambert function W x 0 ( ) (Olver et al., 2010, (4.13)) is defined for -1 /e &lt; x and we have 0 &lt; exp( a +ln( ) b .\n\nSince ˆ c fulfills inequality Eq. (344) and therefore also Eq. (342), we have a lower bound on the storage capacity N :\n\n<!-- formula-not-decoded -->\n\nNext we aim at a lower bound on c which does not use the Lambert W function (Olver et al., 2010, (4.13)). Therefore, we upper bound W 0 (exp( a +ln( )) b to obtain a lower bound on c , therefore, also a lower bound on the storage capacity N . The lower bound is given in the next corollary.\n\nCorollary A1. We assume a failure probability 0 &lt; p glyph[lessorequalslant] 1 and randomly chosen patterns on the sphere with radius M = K √ d -1 . We define\n\n<!-- formula-not-decoded -->\n\nUsing the omega constant Ω ≈ 0 56714329 . we set\n\n<!-- formula-not-decoded -->\n\nand ensure\n\n<!-- formula-not-decoded -->\n\nThen with probability 1 -p , the number of random patterns that can be stored is\n\n<!-- formula-not-decoded -->\n\nExamples are c ≥ 3 1444 . for β = 1 , K = 3 , d = 20 and p = 0 001 . ( a + ln( ) b &gt; 1 27 . ) and c ≥ 1 2585 . for β = 1 K = 1 , d = 75 , and p = 0 001 . ( a +ln( ) b &lt; -0 94 . ).\n\nProof. We lower bound the c defined in Theorem A5. According to (Hoorfar &amp; Hassani, 2008, Theorem 2.3) we have for any real u and y &gt; 1 e :\n\n<!-- formula-not-decoded -->\n\nTo upper bound W x 0 ( ) for x ∈ [0 , 1] , we set\n\n<!-- formula-not-decoded -->\n\nwhere the Omega constant Ω is\n\n<!-- formula-not-decoded -->\n\nSee for these equations the special values of the Lambert W function in Lemma A31. We have the upper bound on W 0 :\n\n<!-- formula-not-decoded -->\n\nAt the right hand side of interval [0 , 1] , we have u = 0 and exp( u ) = 1 and get:\n\n<!-- formula-not-decoded -->\n\nTherefore, the bound is tight at the right hand side of of interval [0 , 1] , that is for exp( u ) = 1 , i.e. u = 0 . Wehave derived an bound for W 0 (exp( u )) with exp( u ) ∈ [0 , 1] or, equivalently, u ∈ -∞ [ , 0] . We obtain from Hoorfar &amp; Hassani (2008, Corollary 2.6) the following bound on W 0 (exp( u )) for 1 &lt; exp( u ) , or, equivalently 0 &lt; u :\n\n<!-- formula-not-decoded -->\n\nA lower bound on ˆ c is obtained via the upper bounds Eq. (357) and Eq. (355) on W 0 as W &gt; 0 0 . We set u = a +ln( ) b and obtain\n\n<!-- formula-not-decoded -->\n\nWe insert this bound into Eq. (347), the solution for ˆ c , to obtain the statement of the theorem.\n\n· Exponential storage capacity: the dimension d of the space as a function of the parameter β , the radius of the sphere M , and the probability p .\n\nWe express the number N of stored patterns by an exponential function with base c &gt; 1 and an exponent linear in d . We derive constraints on the dimension d of the space as a function of β , the radius of the sphere M , the probability p that all patterns can be stored, and the base of the exponential storage capacity. The following theorem gives this result.\n\nTheorem A6 (Storage Capacity (d computed): Random Patterns) . We assume a failure probability 0 &lt; p glyph[lessorequalslant] 1 and randomly chosen patterns on the sphere with radius M = K √ d -1 . We define\n\nglyph[negationslash]\n\n<!-- formula-not-decoded -->\n\nwhere W is the Lambert W function (Olver et al., 2010, (4.13)). For 0 &lt; a the function W is the upper branch W 0 and for a &lt; 0 we use the lower branch W -1 . If we ensure that\n\n<!-- formula-not-decoded -->\n\nthen with probability 1 -p , the number of random patterns that can be stored is\n\n<!-- formula-not-decoded -->\n\nProof. We consider the probability that the master inequality Eq. (311) is fulfilled:\n\n<!-- formula-not-decoded -->\n\nUsing Eq. (329), we have:\n\n<!-- formula-not-decoded -->\n\nTherefore, with probability 1 -p the storage capacity is largest N that fulfills\n\n<!-- formula-not-decoded -->\n\nThis inequality is equivalent to\n\n<!-- formula-not-decoded -->\n\nWe use Eq. (324) to obtain:\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nFor Eq. (365) to be fulfilled, it is sufficient that\n\n<!-- formula-not-decoded -->\n\nIf we insert the assumption Eq. (360) of the theorem into Eq. (361), then we obtain N ≥ 2 . We now apply the upper bound κ d -1 / 2 &lt; κ d -1 &lt; 1 from Eq. (325) and the upper bound 2 βN glyph[lessorequalslant] 1 β from N ≥ 2 to inequality Eq. (367). In the resulting inequality we insert N = √ pc d -1 4 to check whether\n\nit is fulfilled with this special value of N and obtain:\n\n<!-- formula-not-decoded -->\n\nDividing by p , inserting M = K √ d -1 , and exponentiation of the left and right side by 2 d -1 gives:\n\n<!-- formula-not-decoded -->\n\nThis inequality Eq. (369) can be reformulated as:\n\n<!-- formula-not-decoded -->\n\nUsing\n\n<!-- formula-not-decoded -->\n\n(371)\n\nwe write inequality Eq. (370) as\n\n<!-- formula-not-decoded -->\n\nWe determine the value ˆ d of d which makes the inequality Eq. (372) equal to zero. We solve\n\n<!-- formula-not-decoded -->\n\nfor ˆ d\n\nglyph[negationslash]\n\nFor a = 0 we have\n\n<!-- formula-not-decoded -->\n\nwhere W is the Lambert W function (see Def. A6). For a &gt; 0 we have to use the upper branch W 0 of the Lambert W function and for a &lt; 0 we use the lower branch W -1 of the Lambert W function (Olver et al., 2010, (4.13)). We have to ensure that -1 /e glyph[lessorequalslant] a exp( -b ) for a solution to exist. For a = 0 we have ˆ = 1 + exp( d -b ) .\n\nHence, the solution is\n\n<!-- formula-not-decoded -->\n\nSince ˆ d fulfills inequality Eq. (369) and therefore also Eq. (368), we have a lower bound on the storage capacity N :\n\n<!-- formula-not-decoded -->\n\nCorollary A2. We assume a failure probability 0 &lt; p glyph[lessorequalslant] 1 and randomly chosen patterns on the sphere with radius M = K √ d -1 . We define\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nand ensure\n\n<!-- formula-not-decoded -->\n\nthen with probability 1 -p , the number of random patterns that can be stored is\n\n<!-- formula-not-decoded -->\n\nSetting β = 1 , K = 3 , c = 2 and p = 0 001 . yields d &lt; 24 .\n\nProof. For a &lt; 0 the Eq. (359) from Theorem (A6) can be written as\n\n<!-- formula-not-decoded -->\n\nFrom Alzahrani &amp; Salem (2018, Theorem 3.1) we get the following bound on W -1 :\n\n<!-- formula-not-decoded -->\n\nfor u &gt; 0 . We apply Eq. (381) to Eq. (380) with u = -ln( -a ) + b -1 .\n\nSince a &lt; 0 we get\n\n<!-- formula-not-decoded -->\n\n· Storage capacity for the expected minimal separation instead of the probability that all patterns can be stored. In contrast to the previous paragraph, we want to argue about the storage capacity for the expected minimal separation. Therefore, we will use the following bound on the expectation of α min (minimal angle), which gives also a bound on the expected of ∆ min (minimal separation):\n\nLemma A16 (Proposition 3.6 in Brauchart et al. (2018)) . We have the following lower bound on the expectation of α min :\n\n<!-- formula-not-decoded -->\n\nThe bound is valid for all N ≥ 2 and d ≥ 2 .\n\nLet us start with some preliminary estimates. First of all we need some asymptotics for the constant C d -1 in Eq. (383):\n\nLemma A17. The following estimate holds for d ≥ 2 :\n\n<!-- formula-not-decoded -->\n\nProof. The recursion formula for the Gamma function is (Olver et al., 2010, (5.5.1)):\n\n<!-- formula-not-decoded -->\n\nWe use Eq. (325) and the fact that d 1 d ≥ 1 for d ≥ 1 to obtain:\n\n<!-- formula-not-decoded -->\n\nwhere in the last step we used the elementary inequality exp( x ) ≥ 1 + x , which follows from the mean value theorem.\n\nThe next theorem states the number of stored patterns for the expected minimal separation.\n\nTheorem A7 (Storage Capacity (expected separation): Random Patterns) . We assume patterns on the sphere with radius M = K √ d -1 that are randomly chosen. Then for all values c ≥ 1 for which\n\n<!-- formula-not-decoded -->\n\nholds, the number of stored patterns for the expected minimal separation is at least\n\n<!-- formula-not-decoded -->\n\nThe inequality Eq. (387) is e.g. fulfilled with β = 1 , K = 3 , c = 2 and d ≥ 17 .\n\nProof. Instead of considering the probability that the master inequality Eq. (311) is fulfilled we now consider whether this inequality is fulfilled for the expected minimal distance. We consider the expectation of the minimal distance ∆ min :\n\n<!-- formula-not-decoded -->\n\nFor this expectation, the master inequality Eq. (311) becomes\n\n<!-- formula-not-decoded -->\n\nWe want to find the largest N that fulfills this inequality.\n\nWe apply Eq. (329) and Jensen's inequality to deduce the following lower bound:\n\n<!-- formula-not-decoded -->\n\nNow we use Eq. (383) and Eq. (384) to arrive at\n\n<!-- formula-not-decoded -->\n\nfor sufficiently large d . Thus in order to fulfill Eq. (390), it is enough to find values that satisfy Eq. (387).\n\nA.1.6.2 Retrieval of Patterns with One Update and Small Retrieval Error. Retrieval of a pattern x i for fixed point x ∗ i and query ξ is defined via an glyph[epsilon1] by ‖ f ( ξ ) -x ∗ i ‖ &lt; glyph[epsilon1] , that is, the update is glyph[epsilon1] -close to the fixed point. The update rule retrieves a pattern with one update for well separated patterns, that is, ∆ i is large.\n\nTheorem A8 (Pattern Retrieval with One Update) . With query ξ , after one update the distance of the new point f ( ξ ) to the fixed point x ∗ i is exponentially small in the separation ∆ i . The precise bounds using the Jacobian J = ∂f ( ξ ) ∂ ξ and its value J m in the mean value theorem are:\n\n<!-- formula-not-decoded -->\n\nFor given glyph[epsilon1] and sufficient large ∆ i , we have ‖ f ( ξ ) -x ∗ i ‖ &lt; glyph[epsilon1] , that is, retrieval with one update.",
        "metadata": {
            "section_header": "A.1.6 PROPERTIES OF FIXED POINTS NEAR STORED PATTERN",
            "title": "HOPFIELD NETWORKS IS ALL YOU NEED",
            "type": "paper"
        }
    },
    {
        "page_content": "## Proof. From Eq. (180) we have\n\n‖ J m ‖ 2 glyph[lessorequalslant] 2 β N M 2 ( N -1) exp( -β (∆ i -2 max {‖ ξ -x i ‖ ‖ , x ∗ i -x i ‖} M )) . (395) After every iteration the mapped point f ( ξ ) is closer to the fixed point x ∗ i than the original point x i : ‖ f ( ξ ) -x ∗ ‖ glyph[lessorequalslant] ‖ J m ‖ ‖ ξ -x ∗ ‖ . (396)\n\n<!-- formula-not-decoded -->\n\nFor given glyph[epsilon1] and sufficient large ∆ i , we have ‖ f ( ξ ) -x ∗ i ‖ &lt; glyph[epsilon1] , since ‖ J m ‖ 2 foes exponentially fast to zero with increasing ∆ i .\n\nWe want to estimate how large ∆ i is. For x i we have:\n\nglyph[negationslash]\n\nglyph[negationslash]\n\n<!-- formula-not-decoded -->\n\nTo estimate how large ∆ i is, assume vectors x ∈ R d and y ∈ R d that have as components standard normally distributed values. The expected value of the separation of two points with normally distributed components is\n\n<!-- formula-not-decoded -->\n\nThe variance of the separation of two points with normally distributed components is\n\n<!-- formula-not-decoded -->\n\nglyph[negationslash]\n\n<!-- formula-not-decoded -->\n\nglyph[negationslash]\n\n<!-- formula-not-decoded -->\n\nglyph[negationslash]\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nThe expected value for the separation of two random vectors gives:\n\n<!-- formula-not-decoded -->\n\nThe retrieval error decreases exponentially with the separation ∆ i .\n\n<!-- formula-not-decoded -->\n\nTheorem A9 (Exponentially Small Retrieval Error) . The retrieval error ‖ f ( ξ ) -x i ‖ of pattern x i is bounded by\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nProof. We compute the retrieval error which is just ‖ f ( ξ ) -x i ‖ . From Lemma A4 we have\n\n<!-- formula-not-decoded -->\n\nFrom Eq. (179) we have\n\n<!-- formula-not-decoded -->\n\nFor ‖ x i -x ∗ i ‖ glyph[lessorequalslant] 1 2 β M and ‖ x i - ‖ ξ glyph[lessorequalslant] 1 2 β M Eq. (404) gives\n\n<!-- formula-not-decoded -->",
        "metadata": {
            "section_header": "Proof. From Eq. (180) we have",
            "title": "HOPFIELD NETWORKS IS ALL YOU NEED",
            "type": "paper"
        }
    },
    {
        "page_content": "## A.1.7 LEARNING ASSOCIATIONS\n\nWe consider three cases of learning associations, i.e. three cases of how sets are associated. (i) Non of the sets is mapped in an associative space. The raw state pattern r n is the state (query) pattern ξ n , i.e. ξ n = r n , and the raw stored pattern y s is the stored pattern (key), i.e. x s = y s . (ii) Either one of the sets is mapped to the space of the other set or an association matrix is learned. (iia) The state patterns are equal to the raw patterns, i.e. ξ n = r n , and raw stored patterns are mapped via W to the space of the state patterns, i.e. x s = Wy s . (iib) The stored patterns are equal to the raw patterns, i.e. x s = y s , and raw state patterns are mapped via W to the space of the stored patterns, i.e. ξ n = W r T n . (iic) The matrix W is an association matrix. We will compute the derivative of the new state pattern with respect to W , which is valid for all sub-cases (iib)-(iic). (iii) Both set of patterns are mapped in a common associative space. A raw state pattern r n is mapped by W Q to a state pattern (query) ξ n , that is ξ n = W r Q n . A raw stored pattern y s is mapped via W K to stored pattern (key) x s , that is x s = W y K s . We will compute the derivative of the new state pattern with respect to both W Q and W K .\n\nA.1.7.1 Association of Raw Patterns - No Mapping in an Associative Space. The sets are associated via their raw patterns, i.e. the raw state pattern r n is the state (query) pattern ξ n , i.e. ξ n = r n , and raw stored pattern y s is the stored pattern (key), i.e. x s = y s . There is no mapping in an associative space.\n\nThe update rule is\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nThe derivative with respect to ξ is\n\n<!-- formula-not-decoded -->\n\nThe derivative with respect to X is\n\n<!-- formula-not-decoded -->\n\nThese derivatives allow to apply the chain rule if a Hopfield layer is integrated into a deep neural network.\n\nwhere we used\n\nA.1.7.2 Learning an Association Matrix - Only One Set is Mapped in an Associative Space. Only one of the sets R or Y is mapped in the space of the patterns of the other set. Case (a): the state patterns are equal to the raw patterns ξ n = r n and raw stored patterns are mapped via W to the space of the state patterns, i.e. x s = Wy s . Case (b): the stored patterns are equal to the raw patterns x s = y s and raw state patterns are mapped via W to the space of the stored patterns, i.e. ξ n = W r T n . Case (c): the matrix W associates the sets R and Y . This case also includes that W T = W W T K Q , which is treated in next subsection. The next subsection focuses on a low rank approximation of W by defining the dimension d k of associative space and use the matrices W T K and W Q to define W , or equivalently to map R and Y into the associative space.\n\nFrom a mathematical point of view all these case are equal as they lead to the same update rule. Therefore, we consider in the following Case (a) with x s = Wy s and ξ n = r n . Still, the following formula are valid for all three cases (a)-(c).\n\nThe update rule is\n\n<!-- formula-not-decoded -->\n\nwhere we used\n\n<!-- formula-not-decoded -->\n\nWe consider the state (query) pattern ξ with result ξ new :\n\n<!-- formula-not-decoded -->\n\nFor multiple updates this update rule has to be used. However for a single update, or the last update we consider a simplified update rule.\n\nSince new state vector ξ new is projected by a weight matrix W V to another vector, we consider the simplified update rule:\n\n<!-- formula-not-decoded -->\n\nThe derivative with respect to W is\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nWe have the product of the 3-dimensional tensor ∂ ( W ξ T ) ∂ W with the vector a which gives a 2dimensional tensor, i.e. a matrix:\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nwhere J is the Jacobian of the update rule defined in Eq. (59).\n\nTo obtain the derivative of the full update rule Eq. (412) we have to add the term\n\n<!-- formula-not-decoded -->\n\nand include the factor W to get\n\n<!-- formula-not-decoded -->\n\nA.1.7.3 Learning Two Association Mappings - Both Sets are Mapped in an Associative Space. Both sets R and Y are mapped in an associative space. Every raw state pattern r n is mapped via W Q to a state pattern (query) ξ n = W r Q n . Every raw stored pattern y s is mapped via W K to a stored pattern (key) x s = W y K s . In the last subsection we considered a single matrix W . For W T = W W T K Q we have the case of the last subsection. However in this subsection we are looking for a low rank approximation of W . Toward this end we define the dimension d k of associative space and use the matrices W T K and W Q to map to the associative space.\n\nThe update rule is\n\n<!-- formula-not-decoded -->\n\nwhere we used\n\n<!-- formula-not-decoded -->\n\nWe consider raw state patterns r n that are mapped to state patterns ξ n = W r Q n with Q T = Ξ = W R Q and raw stored pattern y s that are mapped to stored patterns x s = W y K s with K T = X = W Y K . The update rule is\n\n<!-- formula-not-decoded -->\n\nSince new state vector ξ new is projected by a weight matrix W V to another vector, we consider the simplified update rule:\n\n<!-- formula-not-decoded -->\n\nFor the simplified update rule, the vector ξ new does not live in the associative space but in the space of raw stored pattern y . However W K would map it to the associative space.\n\n· Derivative with respect to W Q . The derivative with respect to W Q is\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nWe have the product of the 3-dimensional tensor ∂ ( W r Q ) ∂ W Q with the vector a which gives a 2dimensional tensor, i.e. a matrix:\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nwhere J is the Jacobian of the update rule defined in Eq. (59).\n\nTo obtain the derivative of the full update rule Eq. (423) we have to include the factor W K , then get\n\n<!-- formula-not-decoded -->\n\n· Derivative with respect to W K . The derivative with respect to W K is\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nWehave the product of the 3-dimensional tensor ∂ ( Wr ) ∂ W K with the vector a which gives a 2-dimensional tensor, i.e. a matrix:\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nwhere J is the Jacobian of the update rule defined in Eq. (59).\n\nTo obtain the derivative of the full update rule Eq. (423) we have to add the term\n\n<!-- formula-not-decoded -->\n\nand to include the factor W K , then get\n\n<!-- formula-not-decoded -->",
        "metadata": {
            "section_header": "A.1.7 LEARNING ASSOCIATIONS",
            "title": "HOPFIELD NETWORKS IS ALL YOU NEED",
            "type": "paper"
        }
    },
    {
        "page_content": "## A.1.8 INFINITE MANY PATTERNS AND FORGETTING PATTERNS\n\nIn the next subsection we show how the new Hopfield networks can be used for auto-regressive tasks by causal masking. In the following subsection, we introduce forgetting to the new Hopfield networks by adding a negative value to the softmax which is larger if the pattern was observed more in the past.\n\nA.1.8.1 Infinite Many Patterns. The new Hopfield networks can be used for auto-regressive tasks, that is time series prediction and similar. Causal masking masks out the future by a large negative value in the softmax.\n\nWe assume to have infinite many stored patterns (keys) x 1 , x 2 , . . . that are represented by the infinite matrix\n\n<!-- formula-not-decoded -->\n\nThe pattern index is now a time index, that is, we observe x t at time t .\n\nThe pattern matrix at time t is\n\n<!-- formula-not-decoded -->\n\nThe query at time t is ξ t .\n\nFor M t = max 1 glyph[lessorequalslant] i glyph[lessorequalslant] t ‖ x t ‖ , the energy function at time t is E t\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nThe update rule is\n\n<!-- formula-not-decoded -->\n\nwhere we used\n\n<!-- formula-not-decoded -->\n\nWe can use an infinite pattern matrix with an infinite softmax when using causal masking. The pattern matrix at time t is\n\n<!-- formula-not-decoded -->\n\nwith the query ξ t and α →∞ . The energy function at time t is E t\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nFor α →∞ and ‖ ξ t ‖ &gt; 0 this becomes\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nA.1.8.2 Forgetting Patterns. We introduce forgetting to the new Hopfield networks by adding a negative value in the softmax which increases with patterns that are more in the past.\n\nWe assume to have infinite many patterns x 1 , x 2 , . . . that are represented by the infinite matrix\n\n<!-- formula-not-decoded -->\n\nThe pattern index is now a time index, that is, we observe x t at time t .\n\nThe pattern matrix at time t is\n\n<!-- formula-not-decoded -->\n\nThe query at time t is ξ t .\n\nThe energy function with forgetting parameter γ at time t is E t\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nThe update rule is\n\n<!-- formula-not-decoded -->\n\nwhere we used\n\n<!-- formula-not-decoded -->",
        "metadata": {
            "section_header": "A.1.8 INFINITE MANY PATTERNS AND FORGETTING PATTERNS",
            "title": "HOPFIELD NETWORKS IS ALL YOU NEED",
            "type": "paper"
        }
    },
    {
        "page_content": "## A.1.9 NUMBER OF SPURIOUS STATES\n\nThe energy E is defined as\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nSince the negative exponential function is strict monotonic decreasing, exp( -E) has minima, where E has maxima, and has maxima, where as has minima E .\n\n<!-- formula-not-decoded -->\n\nwhere C is a positive constant, λ ( x i , β ) = exp( 1 2 β x x T i i ) and G ( ξ x ; i , β -1 I ) is the Gaussian with mean x i and covariance matrix β -1 I .\n\nSince C is a positive constant and x β -1 = exp( β -1 ln x ) is strict monotonic for positive x , the minima of E are the maxima of\n\n<!-- formula-not-decoded -->\n\nIn Carreira-Perpiñán &amp; Williams (2003) it was shown that Eq. (458) can have more than N modes, that is, more than N maxima.",
        "metadata": {
            "section_header": "A.1.9 NUMBER OF SPURIOUS STATES",
            "title": "HOPFIELD NETWORKS IS ALL YOU NEED",
            "type": "paper"
        }
    },
    {
        "page_content": "## A.2 PROPERTIES OF SOFTMAX, LOG-SUM-EXPONENTIAL, LEGENDRE TRANSFORM, LAMBERT W FUNCTION\n\nFor β &gt; 0 , the softmax is defined as\n\nDefinition A1 (Softmax) .\n\n<!-- formula-not-decoded -->\n\np\n\ni\n\n= [softmax(\n\nβ\n\nx\n\n)]\n\ni\n\n=\n\nWe also need the log-sum-exp function ( lse ), defined as\n\nDefinition A2 (Log-Sum-Exp Function) .\n\n<!-- formula-not-decoded -->\n\nexp(\n\nβx\n\n∑\n\nk\n\nexp(\n\nβx\n\ni\n\n)\n\n.\n\n(460)\n\nk\n\n)\n\nWe can formulate the lse in another base:\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nIn particular, the base a = 2 can be used to speed up computations.\n\nNext, we give the relation between the softmax and the lse function.\n\nLemma A18. The softmax is the gradient of the lse\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nIn the next lemma we report some important properties of the lse function.\n\nLemma A19. We define\n\n<!-- formula-not-decoded -->\n\nwith L ≥ z T x . The lse is the maximum of L on the N -dimensional simplex D with D = { z | ∑ i z i = 1 0 , glyph[lessorequalslant] z i } :\n\n<!-- formula-not-decoded -->\n\nThe softmax p = softmax( β x ) is the argument of the maximum of L on the N -dimensional simplex D with D = { z | ∑ i z i = 1 0 , glyph[lessorequalslant] z i } :\n\n<!-- formula-not-decoded -->\n\nProof. Eq. (466) is obtained from Equation (8) in Gao &amp; Pavel (2017) and Eq. (467) from Equation (11) in Gao &amp; Pavel (2017).\n\nFrom a physical point of view, the lse function represents the 'free energy' in statistical thermodynamics (Gao &amp; Pavel, 2017).\n\nNext we consider the Jacobian of the softmax and its properties.\n\nLemma A20. The Jacobian J s of the softmax p = softmax( β x ) is\n\n<!-- formula-not-decoded -->\n\nwhich gives the elements\n\nglyph[negationslash]\n\n<!-- formula-not-decoded -->\n\nNext we show that J s has eigenvalue 0 .\n\nLemma A21. The Jacobian J s of the softmax function p = softmax( β x ) has a zero eigenvalue with eigenvector 1 .\n\nProof.\n\nglyph[negationslash]\n\n<!-- formula-not-decoded -->\n\nNext we show that 0 is the smallest eigenvalue of J s , therefore J s is positive semi-definite but not (strict) positive definite.\n\nLemma A22. The Jacobian J s of the softmax p = softmax( β ξ ) is symmetric and positive semidefinite.\n\nProof. For an arbitrary z , we have\n\n<!-- formula-not-decoded -->\n\nThe last inequality hold true because the Cauchy-Schwarz inequality says ( a a T )( b T b ) ≥ ( a b T ) 2 , which is the last inequality with a i = z i √ p i and b i = √ p i . Consequently ( diag( p ) -pp T ) is positive semi-definite.\n\nAlternatively ∑ i p z i 2 i -( ∑ i p z i i ) 2 can be viewed as the expected second moment minus the mean squared which gives the variance that is larger equal to zero.\n\nThe Jacobian is 0 &lt; β times a positive semi-definite matrix, which is a positive semi-definite matrix.\n\nMoreover, the softmax is a monotonic map, as described in the next lemma.\n\nLemma A23. The softmax softmax( β x ) is monotone for β &gt; 0 , that is,\n\n<!-- formula-not-decoded -->\n\nProof. We use the version of mean value theorem Lemma A32 with the symmetric matrix J m s = ∫ 1 0 J ( s λ x + (1 -λ ) x ' ) d λ :\n\n<!-- formula-not-decoded -->",
        "metadata": {
            "section_header": "A.2 PROPERTIES OF SOFTMAX, LOG-SUM-EXPONENTIAL, LEGENDRE TRANSFORM, LAMBERT W FUNCTION",
            "title": "HOPFIELD NETWORKS IS ALL YOU NEED",
            "type": "paper"
        }
    },
    {
        "page_content": "## Therefore\n\n<!-- formula-not-decoded -->\n\nsince J m s is positive semi-definite. For all λ the Jacobians J ( s λ x + (1 -λ ) x ' ) are positive semi-definite according to Lemma A22. Since\n\n<!-- formula-not-decoded -->\n\nis an integral over positive values for every x , J m s is positive semi-definite, too.\n\nNext we give upper bounds on the norm of J s .\n\nLemma A24. For a softmax p = softmax( β x ) with m = max i p i (1 -p i ) , the spectral norm of the Jacobian J s of the softmax is bounded:\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nIn particular everywhere holds\n\n<!-- formula-not-decoded -->\n\nIf p max = max i p i ≥ 1 -glyph[epsilon1] ≥ 0 5 . , then for the spectral norm of the Jacobian holds\n\n<!-- formula-not-decoded -->\n\nProof. We consider the maximum absolute column sum norm\n\n<!-- formula-not-decoded -->\n\nand the maximum absolute row sum norm\n\n<!-- formula-not-decoded -->\n\nWe have for A = J s = β ( diag( p ) -pp T )\n\nglyph[negationslash]\n\n<!-- formula-not-decoded -->\n\n= 2\n\nβ p\n\ni\n\n(1\n\n-\n\np\n\ni\n\n)\n\nglyph[lessorequalslant]\n\n2\n\nmβ ,\n\nglyph[negationslash]\n\n<!-- formula-not-decoded -->\n\n= 2\n\nβ p\n\nj\n\n(1\n\n-\n\np\n\nj\n\n)\n\nglyph[lessorequalslant]\n\n2\n\nmβ .\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nThe last inequality is a direct consequence of Hölder's inequality.\n\nFor 0 glyph[lessorequalslant] p i glyph[lessorequalslant] 1 , we have p i (1 -p i ) glyph[lessorequalslant] 0 25 . . Therefore, m glyph[lessorequalslant] 0 25 . for all values of p i .\n\nglyph[negationslash]\n\nIf p max ≥ 1 -glyph[epsilon1] ≥ 0 5 . ( glyph[epsilon1] glyph[lessorequalslant] 0 5 . ), then 1 -p max glyph[lessorequalslant] glyph[epsilon1] and for p i = p max p i glyph[lessorequalslant] glyph[epsilon1] . The derivative ∂x (1 -x /∂x ) = 1 -2 x &gt; 0 for x &lt; 0 5 . , therefore x (1 -x ) increases with x for x &lt; 0 5 . . Using x = 1 -p max and for p i = p max x = p i , we obtain p i (1 -p i ) glyph[lessorequalslant] glyph[epsilon1] (1 -glyph[epsilon1] ) for all i . Consequently, we have m glyph[lessorequalslant] glyph[epsilon1] (1 -glyph[epsilon1] ) .\n\nglyph[negationslash]\n\nUsing the bounds on the norm of the Jacobian, we give some Lipschitz properties of the softmax function.\n\nLemma A25. The softmax function p = softmax( β x ) is ( β/ 2) -Lipschitz. The softmax function p = softmax( β x ) is (2 βm ) -Lipschitz in a convex environment U for which m = max x ∈ U max i p i (1 -p i ) . For p max = min x ∈ U max i p i = 1 -glyph[epsilon1] , the softmax function p = softmax( β x ) is (2 βglyph[epsilon1] ) -Lipschitz. For β &lt; 2 m , the softmax p = softmax( β x ) is contractive in U on which m is defined.\n\nProof. The version of mean value theorem Lemma A32 states for the symmetric matrix J m s = ∫ 1 0 J( λ x +(1 -λ ) x ' ) d λ :\n\n<!-- formula-not-decoded -->\n\nAccording to Lemma A24 for all ˜ = x λ x +(1 -λ ) x ' )\n\n<!-- formula-not-decoded -->\n\nTherefore, we have\n\nwhere ˜ m = max ˜ (1 i p i -˜ ) p i . Since x ∈ U and x ' ∈ U we have ˜ x ∈ U , since U is convex. For m = max x ∈ U max i p i (1 -p i ) we have ˜ m glyph[lessorequalslant] m for all ˜ m . Therefore, we have\n\n<!-- formula-not-decoded -->\n\nwhich also holds for the mean:\n\n<!-- formula-not-decoded -->\n\nTherefore,\n\n<!-- formula-not-decoded -->\n\nFrom Lemma A24 we know m glyph[lessorequalslant] 1 / 4 globally. For p max = min x ∈ U max i p i = 1 -glyph[epsilon1] we have according to Lemma A24: m glyph[lessorequalslant] glyph[epsilon1] .\n\nFor completeness we present a result about cocoercivity of the softmax:\n\nLemma A26. For m = max x ∈ U max i p i (1 -p i ) , softmax function p = softmax( β x ) is 1 (2 / mβ ) -cocoercive in U , that is,\n\n<!-- formula-not-decoded -->\n\nIn particular the softmax function p = softmax( β x ) is (2 /β ) -cocoercive everywhere. With p max = min x ∈ U max i p i = 1 -glyph[epsilon1] , the softmax function p = softmax( β x ) is 1 (2 / βglyph[epsilon1] ) -cocoercive in U .\n\nProof. We apply the Baillon-Haddad theorem (e.g. Theorem 1 in Gao &amp; Pavel (2017)) together with Lemma A25.\n\nFinally, we introduce the Legendre transform and use it to describe further properties of the lse . We start with the definition of the convex conjugate.\n\nDefinition A3 (Convex Conjugate) . The Convex Conjugate (Legendre-Fenchel transform) of a function f from a Hilbert Space X to [ -∞ ∞ , ] is f ∗ which is defined as\n\n<!-- formula-not-decoded -->\n\nSee page 219 Def. 13.1 in Bauschke &amp; Combettes (2017) and page 134 in Garling (2017). Next we define the Legendre transform, which is a more restrictive version of the convex conjugate.\n\nDefinition A4 (Legendre Transform) . The Legendre transform of a convex function f from a convex set X ⊂ R n to R ( f : X → R ) is f ∗ , which is defined as\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nSee page 91 in Boyd &amp; Vandenberghe (2009).\n\nDefinition A5 (Epi-Sum) . Let f and g be two functions from X to ( -∞ ∞ , ] , then the infimal convolution (or epi-sum) of f and g is\n\n↦\n\n<!-- formula-not-decoded -->\n\nSee Def. 12.1 in Bauschke &amp; Combettes (2017).\n\nLemma A27. Let f and g be functions from X to ( -∞ ∞ , ] . Then the following hold:",
        "metadata": {
            "section_header": "Therefore",
            "title": "HOPFIELD NETWORKS IS ALL YOU NEED",
            "type": "paper"
        }
    },
    {
        "page_content": "## 1. Convex Conjugate of norm squared\n\n<!-- formula-not-decoded -->\n\n- 2. Convex Conjugate of a function multiplied by scalar 0 &lt; α ∈ R\n\n<!-- formula-not-decoded -->\n\n- 3. Convex Conjugate of the sum of a function and a scalar β ∈ R\n\n<!-- formula-not-decoded -->\n\n- 4. Convex Conjugate of affine transformation of the arguments. Let A be a non-singular matrix and b a vector\n\n<!-- formula-not-decoded -->\n\n- 5. Convex Conjugate of epi-sums\n\n<!-- formula-not-decoded -->\n\nProof. 1. Since h t ( ) := t 2 2 is a non-negative convex function and h t ( ) = 0 ⇐⇒ t = 0 we have because of Proposition 11.3.3 in Garling (2017) that h ( ‖ x ‖ ) ∗ = h ∗ ( ‖ x ∗ ‖ ) . Additionally, by example (a) on page 137 we get for 1 &lt; p &lt; ∞ and 1 p + 1 q = 1 that ( | t | p p ) ∗ = | t ∗ | q q . Putting all together we get the desired result. The same result can also be deduced from page 222 Example 13.6 in Bauschke &amp; Combettes (2017).\n\n- 2. Follows immediately from the definition since\n\n<!-- formula-not-decoded -->\n\n3.\n\n(\n\nf\n\n+\n\nβ\n\n)\n\n4.\n\n<!-- formula-not-decoded -->\n\n- 5. From Proposition 13.24 (i) in Bauschke &amp; Combettes (2017) and Proposition 11.4.2 in Garling (2017) we get\n\n<!-- formula-not-decoded -->\n\nLemma A28. The Legendre transform of the lse is the negative entropy function, restricted to the probability simplex and vice versa. For the log-sum exponential\n\n<!-- formula-not-decoded -->\n\n∗\n\n:= sup\n\n(\n\nT\n\n∗\n\nx x\n\n-\n\nf\n\n(\n\nx\n\n)\n\n-\n\nβ\n\n)\n\n=:\n\nf\n\n∗\n\n-\n\nβ\n\nx\n\n∈\n\nX\n\nthe Legendre transform is the negative entropy function, restricted to the probability simplex:\n\n<!-- formula-not-decoded -->\n\nFor the negative entropy function, restricted to the probability simplex:\n\n<!-- formula-not-decoded -->\n\nthe Legendre transform is the log-sum exponential\n\n<!-- formula-not-decoded -->\n\nProof. See page 93 Example 3.25 in Boyd &amp; Vandenberghe (2009) and (Gao &amp; Pavel, 2017). If f is a regular convex function (lower semi-continuous convex function), then f ∗∗ = f according to page 135 Exercise 11.2.3 in Garling (2017). If f is lower semi-continuous and convex, then f ∗∗ = f according to Theorem 13.37 (Fenchel-Moreau) in Bauschke &amp; Combettes (2017). The log-sum-exponential is continuous and convex.\n\nLemma A29. Let XX T be non-singular and X a Hilbert space. We define\n\n<!-- formula-not-decoded -->\n\nand\n\n<!-- formula-not-decoded -->\n\nThe Legendre transform of lse( β, X ξ T ) with ξ ∈ X is\n\n<!-- formula-not-decoded -->\n\nwith ξ ∗ ∈ X ∗ and v ∈ X v . The domain of ( lse( β, X ξ T ) ) ∗ is X ∗ .\n\nFurthermore we have\n\n<!-- formula-not-decoded -->\n\nProof. We use the definition of the Legendre transform:\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nwhere we used v ∗ = X T ( XX T ) -1 ξ ∗ .\n\nAccording to page 93 Example 3.25 in Boyd &amp; Vandenberghe (2009), the equations for the maximum max v ∈ X v v T v ∗ -lse( β, v ) are solvable if and only if 0 &lt; v ∗ = X T ( XX T ) -1 ξ ∗ and 1 T v ∗ = 1 T X T ( XX T ) -1 ξ ∗ = 1 . Therefore, we assumed ξ ∗ ∈ X ∗ .\n\nThe domain of ( lse( β, X ξ T ) ) ∗ is X ∗ , since on page 93 Example 3.25 in Boyd &amp; Vandenberghe (2009) it was shown that outside X ∗ the sup v ∈ X v v T v ∗ -lse( β, v ) is not bounded.\n\nUsing\n\n<!-- formula-not-decoded -->\n\nthe Hessian of lse( β, X ξ T )\n\n<!-- formula-not-decoded -->\n\nis positive semi-definite since diag( p ) -pp T is positive semi-definite according to Lemma A22. Therefore, lse( β, X ξ T ) is convex and continuous.\n\nIf f is a regular convex function (lower semi-continuous convex function), then f ∗∗ = f according to page 135 Exercise 11.2.3 in Garling (2017). If f is lower semi-continuous and convex, then f ∗∗ = f according to Theorem 13.37 (Fenchel-Moreau) in Bauschke &amp; Combettes (2017). Consequently we have\n\n<!-- formula-not-decoded -->\n\nWe introduce the Lambert W function and some of its properties, since it is needed to derive bounds on the storage capacity of our new Hopfield networks.\n\nDefinition A6 (Lambert Function) . The Lambert W function (Olver et al., 2010, (4.13)) is the inverse function of\n\n<!-- formula-not-decoded -->\n\nThe Lambert W function has an upper branch W 0 for -1 glyph[lessorequalslant] y and a lower branch W -1 for y glyph[lessorequalslant] -1 . We use W if a formula holds for both branches. We have\n\n<!-- formula-not-decoded -->\n\nWe present some identities for the Lambert W function (Olver et al., 2010, (4.13)):\n\nLemma A30. Identities for the Lambert W function are\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nglyph[negationslash]\n\n<!-- formula-not-decoded -->\n\nWe also present some special values for the Lambert W function (Olver et al., 2010, (4.13)):\n\nLemma A31.\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nwhere the Omega constant Ω is\n\n<!-- formula-not-decoded -->\n\nWe need in some proofs a version of the mean value theorem as given in the next lemma.\n\nLemmaA32 (Mean Value Theorem) . Let U ⊂ R n be open, f : U → R m continuously differentiable, and x ∈ U as well as h ∈ R n vectors such that the line segment x + t h for 0 glyph[lessorequalslant] t glyph[lessorequalslant] 1 is in U . Then the following holds:\n\n<!-- formula-not-decoded -->\n\nwhere J is the Jacobian of f and the integral of the matrix is component-wise.\n\nProof. Let f , . . . , f 1 m denote the components of f and define g i : [0 , 1] → R by\n\n<!-- formula-not-decoded -->\n\nthen we obtain\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nThe statement follows since the Jacobian J has as entries ∂f i ∂x j\n\n.\n\nA.3 MODERN HOPFIELD NETWORKS: BINARY STATES (KROTOV AND HOPFIELD)\n\nA.3.1 MODERN HOPFIELD NETWORKS: INTRODUCTION\n\nA.3.1.1 Additional Memory and Attention for Neural Networks. Modern Hopfield networks may serve as additional memory for neural networks. Different approaches have been suggested to equip neural networks with an additional memory beyond recurrent connections. The neural Turing machine (NTM) is a neural network equipped with an external memory and an attention process (Graves et al., 2014). The NTM can write to the memory and can read from it. A memory network (Weston et al., 2014) consists of a memory together with the components: (1) input feature map (converts the incoming input to the internal feature representation) (2) generalization (updates old memories given the new input), (3) output feature map (produces a new output), (4) response\n\n(converts the output into the response format). Memory networks are generalized to an end-to-end trained model, where the arg max memory call is replaced by a differentiable softmax (Sukhbaatar et al., 2015a;b). Linear Memory Network use a linear autoencoder for sequences as a memory (Carta et al., 2020).\n\nTo enhance RNNs with additional associative memory like Hopfield networks have been proposed (Ba et al., 2016a;b). The associative memory stores hidden states of the RNN, retrieves stored states if they are similar to actual ones, and has a forgetting parameter. The forgetting and storing parameters of the RNN associative memory have been generalized to learned matrices (Zhang &amp; Zhou, 2017). LSTMs with associative memory via Holographic Reduced Representations have been proposed (Danihelka et al., 2016).\n\nRecently most approaches to new memories are based on attention. The neural Turing machine (NTM) is equipped with an external memory and an attention process (Graves et al., 2014). End to end memory networks (EMN) make the attention scheme of memory networks (Weston et al., 2014) differentiable by replacing arg max through a softmax (Sukhbaatar et al., 2015a;b). EMN with dot products became very popular and implement a key-value attention (Daniluk et al., 2017) for self-attention. An enhancement of EMN is the transformer (Vaswani et al., 2017a;b) and its extensions (Dehghani et al., 2018). The transformer had great impact on the natural language processing (NLP) community as new records in NLP benchmarks have been achieved (Vaswani et al., 2017a;b). MEMO uses the transformer attention mechanism for reasoning over longer distances (Banino et al., 2020). Current state-of-the-art for language processing is a transformer architecture called 'the Bidirectional Encoder Representations from Transformers' (BERT) (Devlin et al., 2018; 2019).\n\nA.3.1.2 Modern Hopfield networks: Overview. The storage capacity of classical binary Hopfield networks (Hopfield, 1982) has been shown to be very limited. In a d -dimensional space, the standard Hopfield model can store d uncorrelated patterns without errors but only Cd/ ln( d ) random patterns with C &lt; 1 2 / for a fixed stable pattern or C &lt; 1 / 4 if all patterns are stable (McEliece et al., 1987). The same bound holds for nonlinear learning rules (Mazza, 1997). Using tricks-of-trade and allowing small retrieval errors, the storage capacity is about 0 138 . d (Crisanti et al., 1986; Hertz et al., 1991; Torres et al., 2002). If the learning rule is not related to the Hebb rule then up to d patterns can be stored (Abu-Mostafa &amp; StJacques, 1985). Using Hopfield networks with non-zero diagonal matrices, the storage can be increased to Cd ln( d ) (Folli et al., 2017). In contrast to the storage capacity, the number of energy minima (spurious states, stable states) of Hopfield networks is exponentially in d (Tanaka &amp; Edwards, 1980; Bruck &amp; Roychowdhury, 1990; Wainrib &amp; Touboul, 2013).\n\nRecent advances in the field of binary Hopfield networks (Hopfield, 1982) led to new properties of Hopfield networks. The stability of spurious states or metastable states was sensibly reduced by a Hamiltonian treatment for the new relativistic Hopfield model (Barra et al., 2018). Recently the storage capacity of Hopfield networks could be increased by new energy functions. Interaction functions of the form F x ( ) = x n lead to storage capacity of α d n n -1 , where α n depends on the allowed error probability (Krotov &amp; Hopfield, 2016; 2018; Demircigil et al., 2017) (see (Krotov &amp; Hopfield, 2018) for the non-binary case). Interaction functions of the form F x ( ) = x n lead to storage capacity of α n c d n -1 n ln d for c n &gt; 2(2 n -3)!! (Demircigil et al., 2017).\n\nInteraction functions of the form F x ( ) = exp( x ) lead to exponential storage capacity of 2 d/ 2 where all stored patterns are fixed points but the radius of attraction vanishes (Demircigil et al., 2017). It has been shown that the network converges with high probability after one update (Demircigil et al., 2017).",
        "metadata": {
            "section_header": "Convex Conjugate of norm squared",
            "title": "HOPFIELD NETWORKS IS ALL YOU NEED",
            "type": "paper"
        }
    },
    {
        "page_content": "## A.3.2 ENERGY AND UPDATE RULE FOR BINARY MODERN HOPFIELD NETWORKS\n\nWe follow (Demircigil et al., 2017) where the goal is to store a set of input data x 1 , . . . , x N that are represented by the matrix\n\n<!-- formula-not-decoded -->\n\nThe x i is pattern with binary components x ij ∈ {-1 +1 , } for all i and j . ξ is the actual state of the units of the Hopfield model. Krotov and Hopfield (Krotov &amp; Hopfield, 2016) defined the energy function E with the interaction function F that evaluates the dot product between patterns x i and the\n\nactual state ξ :\n\n<!-- formula-not-decoded -->\n\nwith F a ( ) = a n , where n = 2 gives the energy function of the classical Hopfield network. This allows to store α d n n -1 patterns (Krotov &amp; Hopfield, 2016). Krotov and Hopfield (Krotov &amp; Hopfield, 2016) suggested for minimizing this energy an asynchronous updating dynamics T = ( T j ) for component ξ j :\n\nglyph[negationslash]\n\nglyph[negationslash]\n\n<!-- formula-not-decoded -->\n\nWhile Krotov and Hopfield used F a ( ) = a n , Demircigil et al. (Demircigil et al., 2017) went a step further and analyzed the model with the energy function F a ( ) = exp( a ) , which leads to an exponential storage capacity of N = 2 d/ 2 . Furthermore with a single update the final pattern is recovered with high probability. These statements are given in next theorem.\n\nTheorem A10 (Storage Capacity for Binary Modern Hopfield Nets (Demircigil et al. 2017)) . Consider the generalized Hopfield model with the dynamics described in Eq. (545) and interaction function F given by F x ( ) = e x . For a fixed 0 &lt; α &lt; ln(2) / 2 let N = exp( αd ) + 1 and let x 1 , . . . , x N be N patterns chosen uniformly at random from {-1 +1 , } d . Moreover fix glyph[rho1] ∈ [0 , 1 / 2) . For any i and any ˜ x i taken uniformly at random from the Hamming sphere with radius glyph[rho1]d centered in x i , S ( x i , glyph[rho1]d ) , where glyph[rho1]d is assumed to be an integer, it holds that\n\nglyph[negationslash]\n\n<!-- formula-not-decoded -->\n\nif α is chosen in dependence of glyph[rho1] such that\n\n<!-- formula-not-decoded -->\n\nwith\n\n↦\n\n<!-- formula-not-decoded -->\n\nProof. The proof can be found in Demircigil et al. (2017).\n\nglyph[negationslash]\n\nThe number of patterns N = exp( αd ) + 1 is exponential in the number d of components. The result Pr( ∃ i ∃ j : T j ( ˜ x i ) = x ij ) → 0\n\nmeans that one update for each component is sufficient to recover the pattern with high probability.\n\nThe constraint α &lt; I (1 -2 glyph[rho1] ) 2 on α gives the trade-off between the radius of attraction glyph[rho1]d and the number N = exp( αd ) + 1 of pattern that can be stored.\n\nTheorem A10 in particular implies that\n\nglyph[negationslash]\n\n<!-- formula-not-decoded -->\n\nas d →∞ , i.e. with a probability converging to 1 , all the patterns are fixed points of the dynamics. In this case we can have α → I (1) 2 = ln(2) / 2 .\n\nKrotov and Hopfield define the update dynamics T j ( ξ ) in Eq. (545) via energy differences of the energy in Eq. (544). First we express the energy in Eq. (544) with F a ( ) = exp( a ) (Demircigil et al., 2017) by the lse function. Then we use the mean value theorem to express the update dynamics T j ( ξ ) in Eq. (545) by the softmax function. For simplicity, we set β = 1 in the following. There exists a v ∈ -[ 1 1] , with\n\n<!-- formula-not-decoded -->\n\nwhere e j is the Cartesian unit vector with a one at position j and zeros elsewhere, [ ] . j is the projection to the j -th component, and\n\n<!-- formula-not-decoded -->",
        "metadata": {
            "section_header": "A.3.2 ENERGY AND UPDATE RULE FOR BINARY MODERN HOPFIELD NETWORKS",
            "title": "HOPFIELD NETWORKS IS ALL YOU NEED",
            "type": "paper"
        }
    },
    {
        "page_content": "## A.4 HOPFIELD UPDATE RULE IS ATTENTION OF THE TRANSFORMER\n\nThe Hopfield network update rule is the attention mechanism used in transformer and BERT models (see Fig. A.2). To see this, we assume N stored (key) patterns y i and S state (query) patterns r i that are mapped to the Hopfield space of dimension d k . We set x i = W y T K i , ξ i = W r T Q i , and multiply the result of our update rule with W V . The matrices Y = ( y 1 , . . . , y N ) T and R = ( r 1 , . . . , r S ) T combine the y i and r i as row vectors. We define the matrices X T = K = Y W K , Ξ T = Q = RW Q , and V = Y W K W V = X W T V , where W K ∈ R d y × d k , W Q ∈ R d r × d k , W V ∈ R d k × d v . If β = 1 / √ d k and softmax ∈ R N is changed to a row vector, we obtain for the update rule Eq. (3) multiplied by W V :\n\n<!-- formula-not-decoded -->\n\nThe left part of Eq. (548) is the transformer attention. Besides the attention mechanism, Hopfield networks allow for other functionalities in deep network architectures, which we introduce via specific layers in the next section. The right part of Eq. (548) serves as starting point for these specific layers.\n\nFigure A.2: We generalized the energy of binary modern Hopfield networks for allowing continuous states while keeping fast convergence and storage capacity properties. We defined for the new energy also a new update rule that minimizes the energy. The new update rule is the attention mechanism of the transformer. Formulae are modified to express softmax as row vector as for transformers. \" = \"-sign means \"keeps the properties\".\n\n<!-- image -->",
        "metadata": {
            "section_header": "A.4 HOPFIELD UPDATE RULE IS ATTENTION OF THE TRANSFORMER",
            "title": "HOPFIELD NETWORKS IS ALL YOU NEED",
            "type": "paper"
        }
    },
    {
        "page_content": "## A.5 EXPERIMENTS",
        "metadata": {
            "section_header": "A.5 EXPERIMENTS",
            "title": "HOPFIELD NETWORKS IS ALL YOU NEED",
            "type": "paper"
        }
    },
    {
        "page_content": "## A.5.1 EXPERIMENT 1: ATTENTION IN TRANSFORMERS DESCRIBED BY HOPFIELD DYNAMICS\n\nA.5.1.1 Analysis of operating modes of the heads of a pre-trained BERT model. Weanalyzed pre-trained BERT models from Hugging Face Inc. (Wolf et al., 2019) according to these operating classes. In Fig. A.3 in the appendix the distribution of the pre-trained bert-base-cased model is depicted (for other models see appendix Section A.5.1.4). Operating classes (II) (large metastable states) and (IV) (small metastable states) are often observed in the middle layers. Operating class (I) (averaging over a very large number of patterns) is abundant in lower layers. Similar observations have been reported in other studies (Toneva &amp; Wehbe, 2019a;b; Tay et al., 2020). Operating class (III) (medium metastable states) is predominant in the last layers.\n\nA.5.1.2 Experimental Setup. Transformer architectures are known for their high computational demands. To investigate the learning dynamics of such a model and at the same time keeping training time manageable, we adopted the BERT-small setting from ELECTRA (Clark et al., 2020). It has 12 layers, 4 heads and a reduced hidden size, the sequence length is shortened from 512 to 128 tokens and the batch size is reduced from 256 to 128 . Additionally, the hidden dimension is reduced from 768 to 256 and the embedding dimension is reduced from 768 to 128 (Clark et al., 2020). The training of such a BERT-small model for 1 45 . million update steps takes roughly four days on a single NVIDIA V100 GPU.\n\n<!-- image -->\n\nFigure A.3: Analysis of operating modes of the heads of a pre-trained BERT model. For each head in each layer, the distribution of the minimal number k of patterns required to sum up the softmax values to 0 90 . is displayed as a violin plot in a panel. k indicates the size of a metastable state. The bold number in the center of each panel gives the median ¯ k of the distribution. The heads in each layer are sorted according to ¯ k . Attention heads belong to the class they mainly operate in. Class (IV) in blue: Small metastable state or fixed point close to a single pattern, which is abundant in the middle layers (6, 7, and 8). Class (II) in orange: Large metastable state, which is prominent in middle layers (3, 4, and 5). Class (I) in red: Very large metastable state or global fixed point, which is predominant in the first layer. These heads can potentially be replaced by averaging operations. Class (III) in green: Medium metastable state, which is frequently observed in higher layers. We hypothesize that these heads are used to collect information required to perform the respective task. These heads should be the main target to improve transformer and BERT models.\n\n| Layer  12 Head  1            | Head  2                           | Head  3                       | Head  4                   | Head  5                           |                                   | Head  6                          | Head  7 Head  8                   | Head  9                           | Head  10                          | Head  11                          | Head  12                          |\n|------------------------------|-----------------------------------|-------------------------------|---------------------------|-----------------------------------|-----------------------------------|----------------------------------|-----------------------------------|-----------------------------------|-----------------------------------|-----------------------------------|-----------------------------------|\n| Layer  11 12                 | 12                                | 17                            | 22                        | 30                                | 38                                | 85 0 100 200 300 400 k  patterns | 99                                | 132                               | 141                               | 151                               | 235 226                           |\n| Layer  10 13                 | 18                                | 20                            | 24                        | 24                                | 32                                | 33                               | 33                                | 55                                | 55                                | 67                                |                                   |\n| Layer  9 7 Layer  8 1        | 7 4                               | 8                             | 8                         | 8                                 | 16 7                              | 19                               | 42                                | 61                                | 103                               | 216                               | 271                               |\n| 1 Layer  4                   | 11                                | 48                            | 72                        | 5                                 | 9                                 | 7                                | 16                                | 16                                | 17                                | 86                                | 178                               |\n| Layer  3                     | 7                                 | 4 2                           | 5                         |                                   |                                   |                                  |                                   |                                   |                                   |                                   |                                   |\n| Layer  7 1                   | 2                                 | 4                             | 3                         | 4                                 | 4                                 | 6                                | 8                                 | 9                                 | 25                                | 65                                | 117                               |\n| Layer  6                     | 4                                 |                               | 7                         | 8                                 |                                   | 13                               | 16                                | 17                                | 22                                | 45                                | 133                               |\n| Layer  5 4                   | 6                                 | 21                            | 56                        | 67                                | 68                                | 74                               | 116                               | 132                               | 143                               | 190                               | 258                               |\n| 1                            |                                   |                               |                           | 111                               | 158                               | 174                              | 189                               | 191                               | 196                               | 214                               | 259                               |\n| 1                            |                                   | 62                            | 83                        | 108                               | 164                               | 173                              | 197                               | 228                               | 237                               | 247                               | 253                               |\n| Layer  2 1                   | 2                                 | 6                             | 11                        | 29                                | 46                                | 155                              | 157                               | 197                               | 250                               | 321                               | 322                               |\n| Layer  1                     | 115                               | 211                           | 231                       |                                   |                                   |                                  |                                   |                                   |                                   |                                   |                                   |\n| 0 100 200 300 k  patterns 27 | 400 0 100 200 300 400 k  patterns | 0 100 200 300 400 k  patterns | 0 100 200 300 k  patterns | 400 0 100 200 300 k  patterns 241 | 400 0 100 200 300 k  patterns 257 | 400 278                          | 0 100 200 300 400 k  patterns 299 | 0 100 200 300 400 k  patterns 333 | 0 100 200 300 400 k  patterns 350 | 0 100 200 300 400 k  patterns 356 | 0 100 200 300 400 k  patterns 374 |\n\nAs the code base we use the transformers repository from Hugging Face, Inc (Wolf et al., 2019). We aim to reproduce the dataset of Devlin et al. (2019) as close as possible, which consists of the English Wikipedia dataset and the Toronto BookCorpus dataset (Zhu et al., 2015). Due to recent copyright claims the later is not publicly available anymore. Therefore, the pre-training experiments use an uncased snapshot of the original BookCorpus dataset.\n\nA.5.1.3 Hopfield Operating Classes of Transformer and BERT Models. To better understand how operation modes in attention heads develop, we tracked the distribution of counts k (see main paper) over time in a BERT-small model. At the end of training we visualized the count distribution, grouped into four classes (see Figure A.4). The thresholds for the classes were chosen according to the thresholds of Figure 2 in the main paper. However, they are divided by a factor of 4 to adapt to the shorter sequence length of 128 compared to 512 . From this plot it is clear, that the attention in heads of Class IV commit very early to the operating class of small metastable states.\n\nA.5.1.4 Learning Dynamics of Transformer and BERT Models. To observe this behavior in the early phase of training, we created a ridge plot of the distributions of counts k for the first 20 000 , steps (see Figure A.5 (a)). This plot shows that the attention in heads of middle layers often change the operation mode to Class IV around 9 000 , to 10 000 , steps. At the same time the second big drop in the loss occurs. The question arises whether this is functionally important or whether it is an artefact which could be even harmful. To check if the attention mechanism is still able to learn after the change in the operation mode we analyzed the gradient flow through the softmax function. For every token we calculate the Frobenius norm of the Jacobian of the softmax over multiple samples. Then, for every head we plot the distribution of the norm (see Figure A.5(b)). The gradients with respect to the weights are determined by the Jacobian J defined in Eq. (59) as can be seen in Eq. (418), Eq. (429), and Eq. (435). We can see that the attention in heads of Class IV remain almost unchanged during the rest of the training.\n\nA.5.1.5 Attention Heads Replaced by Gaussian Averaging Layers. The self-attention mechanism proposed in Vaswani et al. (2017a) utilizes the softmax function to compute the coefficients of a convex combination over the embedded tokens, where the softmax is conditioned on the input. However, our analysis showed that especially in lower layers many heads perform averaging over a very large number of patterns. This suggests that at this level neither the dependency on the input nor a fine grained attention to individual positions is necessary. As an alternative to the original mechanism we propose Gaussian averaging heads which are computationally more efficient. Here, the softmax function is replaced by a discrete Gaussian kernel, where the location µ and the scale σ are learned. In detail, for a sequence length of N tokens we are given a vector of location parameters µ = ( µ , . . . , µ 1 N ) T and a vector of corresponding scale parameters σ = ( σ , . . . , σ 1 N ) T . We subdivide the interval [ -1 1] , into N equidistant supporting points { s j } N j =1 , where\n\n<!-- formula-not-decoded -->\n\nThe attention [ A ] i,j from the i -th token to the j -th position is calculated as\n\n<!-- formula-not-decoded -->\n\nwhere z i normalizes the i -th row of the attention matrix A to sum up to one:\n\n<!-- formula-not-decoded -->\n\nFor initialization we uniformly sample a location vector µ ∈ [ -1 1] , N and a scale vector σ ∈ [0 75 . , 1 25] . N per head. A simple way to consider the individual position of each token at initialization is to use the supporting points µ i = s i (see Figure A.6). In practice no difference to the random initialization was observed.\n\n· Number of parameters. Gaussian averaging heads can reduce the number of parameters significantly. For an input size of N tokens, there are 2 · N parameters per head. In contrast, a standard self-attention head with word embedding dimension d y and projection dimension d k has two weight matrices\n\nFigure A.4: Left : Ridge plots of the distribution of counts k over time for BERT-small Right : Violin plot of counts k after 1 450000 , steps, divided into the four classes from the main paper. The thresholds were adapted to the shorter sequence length.\n\n<!-- image -->\n\nFigure A.5: (a) : change of count density during training is depicted for the first 20 000 , steps. (b) : the corresponding distribution of the Frobenius norm of the Jacobian of the softmax function is depicted. The gradients with respect to the weights are determined by the Jacobian J defined in Eq. (59) as can be seen in Eq. (418), Eq. (429), and Eq. (435).\n\n<!-- image -->\n\nW ,W Q K ∈ R d k × d y , which together amount to 2 · d k · d y parameters. As a concrete example, the BERT-base model from Devlin et al. (2019) has an embedding dimension d y = 768 , a projection dimension d k = 64 and a sequence length of N = 512 . Compared to the Gaussian head, in this case (2 · 768 · 64) / (2 · 512) = 95 5 . times more parameters are trained for the attention mechanism itself. Only for very long sequences (and given that the word embedding dimension stays the same) the dependence on N may become a disadvantage. But of course, due to the independence from the input the Gaussian averaging head is less expressive in comparison to the original attention mechanism. A recently proposed input independent replacement for self-attention is the so called Random Synthesizer (Tay et al., 2020). Here the softmax -attention is directly parametrized with an N × N matrix. This amounts to 0 5 . · N more parameters than Gaussian averaging.\n\nFigure A.6: Attentions of a Gaussian averaging head at initialization for sequence length N = 128 . Every line depicts one Gaussian kernel. Here, the location parameters are initialized with the value of the supporting points µ i = s i .\n\n<!-- image -->\n\n     \n\nA.5.2 EXPERIMENT 2: MULTIPLE INSTANCE LEARNING DATASETS.\n\nA.5.2.1 Immune Repertoire Classification. An architecture called DeepRC, is based on our modern Hopfield networks, for immune repertoire classification and compared to other machine learning approaches. For DeepRC, we consider immune repertoires as input objects, which are represented as bags of instances. In a bag, each instance is an immune receptor sequence and each bag can contain a large number of sequences. At its core, DeepRC consists of a modern Hopfield network that extracts information from each repertoire. The stored patterns (keys) are representations of the immune amino acid sequences (instances) that are obtained by an 1D convolutional network with position encoding. Each state pattern (query) is static and learned via backpropagation. For details see Widrich et al. (2020a;b).\n\nOur new Hopfield network has been integrated into a deep learning architecture for immune repertoire classification, a massive multiple instance learning task (Widrich et al., 2020a;b). Theorem 3 states that modern Hopfield networks possess an exponential storage capacity which enables to tackle massive multiple instance learning (MIL) problems (Dietterich et al., 1997). Immune repertoire classification (Emerson et al., 2017) typically requires to extract few patterns from a large set of sequences, the repertoire, that are indicative for the respective immune status. Most MIL methods fail due the large number of instances.\n\nData is obtained by experimentally observed immune receptors as well as simulated sequences sequence motifs (Akbar et al., 2019; Weber et al., 2020) with low yet varying degrees of frequency are implanted. Four different categories of datasets are constructed: (a) Simulated immunosequencing data with implanted motifs, (b) immunosequencing data generated by long short-term memory (LSTM) with implanted motifs, (c) real-world immunosequencing data with implanted motifs, and (d) real-world immunosequencing data with known immune status (Emerson et al., 2017). Categories (a), (b), and (d) contain approx. 300,000 instances per immune repertoire. With over 30 billion sequences in total, this represents one of the largest multiple instance learning experiments ever conducted (Carbonneau et al., 2018). Despite the massive number of instances as well as the low frequency\n\nof sequences indicative of the respective immune status, deep learning architectures with modern Hopfield networks outperform all competing methods with respect to average area under the ROC curve in all four categories, (a), (b), (c) and (d) (for details see Widrich et al. (2020a)).\n\nWe evaluate and compare the performance of DeepRC to a set of machine learning methods that serve as baseline, were suggested, or can readily be adapted to immune repertoire classification. The methods comprise (i) known motif, which counts how often the known implanted motifs occur, (ii) Support Vector Machine (SVM) approach that uses a fixed mapping from a bag of sequences to the corresponding k -mer counts and used the MinMax and Jaccard kernel, (iii) k -Nearest Neighbor (KNN) with k -mer representation, transforming MinMax and Jaccard kernel to distances, (iv) logistic regression on the k -mer representation, (v) burden test that first identifies sequences or k -mers and then computes a burden score per individual, and (vi) logistic multiple instance learning (lMIL). On the real-world dataset DeepRC achieved an AUC of 0 832 . ± 0 022 . , followed by the SVM with MinMax kernel (AUC 0 825 . ± 0 022 . ) and the burden test with an AUC of 0 699 . ± 0 041 . . Overall on all datasets, DeepRC outperformed all competing methods with respect to average AUC (see Widrich et al. (2020a;b)).\n\nTable A.1 reports the average performance in the simulated immunosequencing datasets (last column) and the performance on datasets of the remaining three categories. DeepRC outperforms all competing methods with respect to average AUC. Across categories, the runner-up methods are either the SVM for MIL problems with MinMax kernel or the burden test.\n\n|                       | Real-world    | Real-world data with implanted signals   | Real-world data with implanted signals   | Real-world data with implanted signals   | Real-world data with implanted signals   | LSTM-generated data   | LSTM-generated data   | LSTM-generated data   | LSTM-generated data   | LSTM-generated data   | Simulated     |\n|-----------------------|---------------|------------------------------------------|------------------------------------------|------------------------------------------|------------------------------------------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|---------------|\n|                       | CMV           | OM1%                                     | OM0.1%                                   | MM1% MM0.1%                              | 10%                                      |                       | 1%                    | 0.5%                  | 0.1%                  | 0.05%                 | avg.          |\n| DeepRC                | 0.832 ± 0.022 | 1.00 ± 0.00                              | 0.98 ± 0.01                              | 1.00 ± 0.00                              | 0.94 ± 0.01                              | 1.00 ± 0.00           | 1.00 ± 0.00           | 1.00 ± 0.00           | 1.00 ± 0.00           | 1.00 ± 0.00           | 0.846 ± 0.223 |\n| SVM (MM)              | 0.825 ± 0.022 | 1.00 ± 0.00                              | 0.58 ± 0.02                              | 1.00 ± 0.00                              | 0.53 ± 0.02                              | 1.00 ± 0.00           | 1.00 ± 0.00           | 1.00 ± 0.00           | 1.00 ± 0.00           | 0.99 ± 0.01           | 0.827 ± 0.210 |\n| SVM (J)               | 0.546 ± 0.021 | 0.99 ± 0.00                              | 0.53 ± 0.02                              | 1.00 ± 0.00                              | 0.57 ± 0.02                              | 0.98 ± 0.04           | 1.00 ± 0.00           | 1.00 ± 0.00           | 0.90 ± 0.04           | 0.77 ± 0.07           | 0.550 ± 0.080 |\n| KNN (MM)              | 0.679 ± 0.076 | 0.74 ±                                   | 0.24 0.49 ± 0.03                         | 0.67 ± 0.18                              | 0.50 ± 0.02                              | 0.70 ± 0.27           | 0.72 ± 0.26           | 0.73 ± 0.26           | 0.54 ± 0.16           | 0.52 ± 0.15           | 0.634 ± 0.129 |\n| KNN (J)               | 0.534 ± 0.039 | 0.65 ±                                   | 0.16 0.48 ± 0.03                         | 0.70 ± 0.20                              | 0.51 ± 0.03                              | 0.70 ± 0.29           | 0.61 ± 0.24           | 0.52 ± 0.16           | 0.55 ± 0.19           | 0.54 ± 0.19           | 0.501 ± 0.007 |\n| Log. regr.            | 0.607 ± 0.058 | 1.00 ±                                   | 0.00 0.54 ± 0.04                         | 0.99 ± 0.00                              | 0.51 ± 0.04                              | 1.00 ± 0.00           | 1.00 ± 0.00           | 0.93 ± 0.15           | 0.60 ± 0.19           | 0.43 ± 0.16           | 0.826 ± 0.211 |\n| Burden test           | 0.699 ± 0.041 | 1.00 ±                                   | 0.00 0.64 ± 0.05                         | 1.00 ± 0.00                              | 0.89 ± 0.02                              | 1.00 ± 0.00           | 1.00 ± 0.00           | 1.00 ± 0.00           | 1.00 ± 0.00           | 0.79 ± 0.28           | 0.549 ± 0.074 |\n| Log. MIL (KMER) 0.582 | ± 0.065       | 0.54                                     | ± 0.07 0.51 ± 0.03                       | 0.99 ± 0.00                              | 0.62 ± 0.15                              | 1.00 ± 0.00           | 0.72 ± 0.11           | 0.64 ± 0.14           | 0.57 ± 0.15           | 0.53 ± 0.13           | 0.665 ± 0.224 |\n| Log. MIL (TCR ) b     | 0.515 ± 0.073 | 0.50 ±                                   | 0.03 0.50 ± 0.02                         | 0.99 ± 0.00                              | 0.78 ± 0.03                              | 0.54 ± 0.09           | 0.57 ± 0.16           | 0.47 ± 0.09           | 0.51 ± 0.07           | 0.50 ± 0.12           | 0.501 ± 0.016 |\n| Known motif b.        | -             | 1.00 ±                                   | 0.00 0.70 ± 0.03                         | 0.99 ± 0.00 0.62                         | ± 0.04 1.00                              | ± 0.00                | 1.00 ± 0.00           | 1.00 ± 0.00           | 1.00 ± 0.00           | 1.00 ± 0.00           | 0.890 ± 0.168 |\n| Known motif c.        | -             | 0.92 ± 0.00                              | 0.56 ± 0.03                              | 0.65 ± 0.03 0.52 ±                       | 0.03 1.00 ±                              | 0.00 1.00             | ± 0.00                | 0.99 ± 0.01           | 0.72 ± 0.09           | 0.63 ± 0.09           | 0.738 ± 0.202 |\n\nTable A.1: Results immune repertoire classification across all datasets. Results are given in terms of AUC of the competing methods on all datasets. The reported errors are standard deviations across 5 cross-validation (CV) folds (except for the column 'Simulated'). Real-world CMV: Average performance over 5 CV folds on the cytomegalovirus (CMV) dataset Emerson et al. (2017). Real-world data with implanted signals: Average performance over 5 CV folds for each of the four datasets. A signal was implanted with a frequency (=wittness rate) of 1% or 0 1% . . Either a single motif ('OM') or multiple motifs ('MM') were implanted. LSTM-generated data: Average performance over 5 CV folds for each of the 5 datasets. In each dataset, a signal was implanted with a frequency of 10% 1% 0 5% 0 1% , , . , . , and 0 05% . , respectively. Simulated: Here we report the mean over 18 simulated datasets with implanted signals and varying difficulties. The error reported is the standard deviation of the AUC values across the 18 datasets.\n\nA.5.2.2 Multiple Instance Learning Benchmark Datasets. Classical benchmarking datasets comprise UCSB breast cancer classification (Kandemir et al., 2014), and the Elephant, Fox, Tiger datasets (Andrews et al., 2003).\n\nElephant, Fox and Tiger are MIL datasets for image annotation which comprise color images from the Corel dataset that have been preprocessed and segmented. An image consists of a set of segments (or blobs), each characterized by color, texture and shape descriptors. The datasets have 100 positive and 100 negative example images. The latter have been randomly drawn from a pool of photos of other animals. Elephant has 1391 instances and 230 features. Fox has 1320 instances and 230 features. Tiger has 1220 instances and 230 features. Furthermore, we use the UCSB breast cancer classification (Kandemir et al., 2014) dataset, which consists of 2,002 instances across 58 input objects. An instance represents a patch of a histopathological image of cancerous or normal tissue. The layer HopfieldPooling is used, which allows for computing a per-input-object representation by\n\nTable A.2: Hyperparameter search-space of a manual hyperparameter selection on the respective validation sets of the Elephant, Fox, Tiger and UCSB breast cancer datasets.\n\n| parameter                 | values                          |\n|---------------------------|---------------------------------|\n| learning rates            | { 10 - 3 , 10 - 5 }             |\n| learning rate decay ( g ) | { 0 98 . , 0 96 0 94 . , . }    |\n| embedding layers          | { 1 2 3 , , }                   |\n| layer widths              | { 32 64 256 1024 2048 , , , , } |\n| number of heads           | { 8 12 16 32 , , , }            |\n| head dimensions           | { 16 32 64 , , }                |\n| scaling factors           | { 0 1 . , 1 0 . , 10 0 . }      |\n| hidden dimensions         | { 32 64 128 , , }               |\n| bag dropout               | { 0 0 . , 0 75 . }              |\n\nextracting an average of instances that are indicative for one of the two classes. The input to the HopfieldPooling layer is a set of embedded instances Y and a trainable but fixed state (query) pattern Q used for averaging of class-indicative instances. This averaging enables a compression of variable-sized bags to a fixed-sized representation to discriminate the bags. We performed a manual hyperparameter search on a validation set. In detail, we used the following architecture to perform the given task on the Elephant, Fox, Tiger and UCSCB breast cancer datasets: (I) we apply fully connected linear embedding layers with ReLU activation. (II) The output of this embedding serves as the input to our HopfieldPooling layer where the above described pooling operation is performed. (III) Thereafter we use 'ReLU - Linear blocks' as the final linear output layers that perform the classification. Among other hyperparameters, different hidden layer widths (for the fully connected pre- and postHopfieldPooling layers), learning rates and batch sizes were tried. Additionally our focus resided on the hyperparameters of the HopfieldPooling layer. Among those were the number of heads, the head dimension and the scaling factor b . All models were trained for 160 epochs using the AdamW optimizer (Loshchilov &amp; Hutter, 2017) with exponential learning rate decay (see Table A.2), and validated by 10-fold nested cross validation repeated five times with different splits on the data sets. The reported ROC AUC scores are the average of these repetitions. As overfitting imposed quite a problem, bag dropout was applied as the regularization technique of choice.",
        "metadata": {
            "section_header": "A.5.1 EXPERIMENT 1: ATTENTION IN TRANSFORMERS DESCRIBED BY HOPFIELD DYNAMICS",
            "title": "HOPFIELD NETWORKS IS ALL YOU NEED",
            "type": "paper"
        }
    },
    {
        "page_content": "## A.5.3 EXPERIMENT 3: CLASSIFICATION ON SMALL UCI BENCHMARK DATASETS\n\nA.5.3.1 Motivation. Datasets with a small number of samples, like the UCI benchmark datasets, are particularly difficult for neural networks to generalize on. In contrast to their performance on larger datasets, they are consistently outperformed by methods like e.g. gradient boosting, random forests (RF) and support vector machines (SVMs). Finding samples or even learning prototypes that are highly indicative for the class of a sample (query) suggest the use of Hopfield networks. We applied a modern Hopfield network via the layer Hopfield . The input vector is mapped to R using a self-normalizing net (SNN) and W K is learned, where the dimension of W K (the number of stored fixed pattern) is a hyperparameter. The output Z of Hopfield enters the output layer.\n\nA.5.3.2 Methods compared. Modern Hopfield networks via the layer Hopfield are compared to 17 groups of methods (Fernández-Delgado et al., 2014; Klambauer et al., 2017a):\n\n- 1. Support Vector Machines\n- 2. Random Forest\n- 3. Multivariate adaptive regression splines (MARS)\n- 4. Boosting\n- 5. Rule-based Methods\n- 6. Logistic and Multinomial Regression (LMR)\n- 7. Discriminant Analysis (DA)\n- 8. Bagging\n- 9. Nearest Neighbor\n- 10. Decision Trees\n- 11. Other Ensembles\n- 12. Neural Networks (standard NN, BatchNorm, WeighNorm, MSRAinit, LayerNorm, ResNet, Self-Normalizing Nets)\n- 13. Bayesian Methods\n- 14. Other Methods\n- 15. Generalized linear models (GLM)\n- 16. Partial Least Squares and Principal Component Regression (PLSR)\n- 17. Stacking (Wolpert)\n\nA.5.3.3 Experimental design and implementation details. As specified in the main paper, we consider 75 datasets of the UC Irvine Machine Learning Repository , which contain less than 1 000 , samples per dataset, following the dataset separation into large and small dataset in Klambauer et al. (2017a). On each dataset, we performed a grid-search to determine the best hyperparameter setting and model per dataset. The hyperparameter search-space of the grid-search is listed in Table A.3. All models were trained for 100 epochs with a mini-batch size of 4 samples using the cross entropy loss and the PyTorch SGD module for stochastic gradient descent without momentum and without weight decay or dropout. After each epoch, the model accuracy was computed on a separated validation set. Using early stopping, the model with the best validation set accuracy averaged over 16 consecutive epochs was selected as final model. This final model was then evaluated against a separated test set to determine the accuracy, as reported in Tables 2 and Table uci\\_detailed\\_results.csv in the supplemental materials.\n\nAs network architecture, we use { 0 1 7 , , } fully connected embedding layers with SELU Klambauer et al. (2017a) activation functions and { 32 128 1024 , , } hidden units per embedding layer. These embedding layers are followed by the layer Hopfield . The number of hidden units is also used as number of dimensions for the Hopfield association space with a number of { 1 32 , } heads. The layer Hopfield is followed by a mapping to the output vector, which has as dimension the number of classes. Finally, the softmax function is applied to obtain the predicted probability for a class.\n\nTable A.3: Hyperparameter search-space for grid-search on small UCI benchmark datasets. All models were trained for 100 epochs using stochastic gradient descent with early stopping based on the validation set accuracy and a minibatch size of 4 samples. The number of stored patterns is depending on the number of target classes of the individual tasks.\n\n| parameter         | values                      |\n|-------------------|-----------------------------|\n| learning rates    | { 0 05 . }                  |\n| embedding layers  | { 0 1 7 , , }               |\n| hidden units      | { 32 128 1024 , , }         |\n| heads             | { 1 32 , }                  |\n| β                 | { 1 0 . , 0 1 . , 0 001 . } |\n|",
        "metadata": {
            "section_header": "A.5.3 EXPERIMENT 3: CLASSIFICATION ON SMALL UCI BENCHMARK DATASETS",
            "title": "HOPFIELD NETWORKS IS ALL YOU NEED",
            "type": "paper"
        }
    },
    {
        "page_content": "# stored patterns | { 1 8 , } · n classes \\_     |\n\nA.5.3.4 Results. We compared the performance of 25 methods based on their method rank. For this we computed the rank per method per dataset based on the accuracy on the test set, which was then averaged over all 75 datasets for each method to obtain the method rank. For the baseline methods we used the scores summarized by (Klambauer et al., 2017a).",
        "metadata": {
            "section_header": "stored patterns | { 1 8 , } · n classes \\_     |",
            "title": "HOPFIELD NETWORKS IS ALL YOU NEED",
            "type": "paper"
        }
    },
    {
        "page_content": "## A.5.4 EXPERIMENT 4: DRUG DESIGN BENCHMARK DATASETS\n\nA.5.4.1 Experimental design and implementation details. We test Hopfield layers on 4 classification datasets from MoleculeNet (Wu et al., 2017), which are challenging for deep learning methods. The first dataset is HIV, which was introduced by the Drug Therapeutics Program (DTP) AIDS Antiviral Screen. The second dataset is BACE, which has IC50 measurements for binding affinities of inhibitors (molecules) to the human β -secretase 1 (BACE-1). The third dataset is BBBP (blood-brain barrier permeability), which stems from modeling and predicting the blood-brain barrier permeability (Martins et al., 2012). The fourth dataset is SIDER (Side Effect Resource) Kuhn et al. (2016) and contains 1427 approved drugs. These datasets represent four areas of modeling tasks in drug discovery, concretely to develop accurate models for predicting a) new anti-virals (HIV), b) new protein inhibitors (BACE), c) metabolic effects (BBBP), and d) side effects of a chemical compound (SIDER).\n\nWe implemented a Hopfield layer HopfieldLayer , in which we used the training-input as storedpattern Y or key, the training-label as pattern-projection Y W V or value and the input as state-pattern R or query. As described in section A.6 by concatenation of input z i and target t i the matrices W K and W V can be designed such that inside the softmax the input z i is used and outside the softmax the target t i .\n\nAll hyperparameters were selected on separate validation sets and we selected the model with the highest validation AUC on five different random splits.Table A.4: Hyperparameter search-space for grid-search on HIV, BACE, BBBP and SIDER. All models were trained if applicable for 4 epochs using Adam and a batch size of 1 sample.\n\n| parameter                           | values                                                  |\n|-------------------------------------|---------------------------------------------------------|\n| beta                                | { 0 0001 0 001 0 01 0 1 0 2 0 3 . , . , . , . , . , . } |\n| learning rates                      | { 0 0002 . }                                            |\n| heads                               | { 1 32 128 512 , , , }                                  |\n| dropout                             | { 0 0 . , 0 1 . , 0 2 . }                               |\n| state-pattern bias                  | { 0 0 . , - 0 1 . , - 0 125 0 15 . , . , - 0 2 . }      |\n| association-activation              | {None, LeakyReLU }                                      |\n| state- and stored-pattern static    | {False, True}                                           |\n| normalize state- and stored-pattern | {False, True}                                           |\n| normalize association projection    | {False, True}                                           |\n| learnable stored-pattern            | {False, True}                                           |\n\nA.5.4.2 Results. We compared the Hopfield layer Hopfieldlayer to Support Vector Machines (SVMs) (Cortes &amp; Vapnik, 1995; Schölkopf &amp; Smola, 2002), Extreme Gradient Boosting (XGBoost) (Chen &amp; Guestrin, 2016), Random Forest (RF) (Breiman, 2001), Deep Neural Networks (DNNs) (LeCun et al., 2015; Schmidhuber, 2015), and to graph neural networks (GNN) like\n\nGraph Convolutional Networks (GCNs) (Kipf &amp; Welling, 2016), Graph Attention Networks (GATs) (Veli˘ ckovi' et al., 2018), Message Passing Neural Networks (MPNNs) (Gilmer et al., 2017), and c Attentive FP (Xiong et al., 2020). Our architecture with HopfieldLayer has reached state-of-theart for predicting side effects on SIDER 0 672 . ± 0 019 . as well as for predicting β -secretase BACE 0 902 . ± 0 023 . . See Table A.5 for all results, where the results of other methods are taken from Jiang et al. (2020).\n\nTable A.5: Results on drug design benchmark datasets. Predictive performance (ROCAUC) on test set as reported by Jiang et al. (2020) for 50 random splits\n\n| Model           | HIV               | BACE              | BBBP              | SIDER             |\n|-----------------|-------------------|-------------------|-------------------|-------------------|\n| SVM             | 0 822 . ± 0 020 . | 0 893 . ± 0 020 . | 0 919 . ± 0 028 . | 0 630 . ± 0 021 . |\n| XGBoost         | 0 816 . ± 0 020 . | 0 889 . ± 0 021 . | 0 926 . ± 0 026 . | 0 642 . ± 0 020 . |\n| RF              | 0 820 . ± 0 016 . | 0 890 . ± 0 022 . | 0 927 . ± 0 025 . | 0 646 . ± 0 022 . |\n| GCN             | 0 834 . ± 0 025 . | 0 898 . ± 0 019 . | 0 903 . ± 0 027 . | 0 634 . ± 0 026 . |\n| GAT             | 0 826 . ± 0 030 . | 0 886 . ± 0 023 . | 0 898 . ± 0 033 . | 0 627 . ± 0 024 . |\n| DNN             | 0 797 . ± 0 018 . | 0 890 . ± 0 024 . | 0 898 . ± 0 033 . | 0 627 . ± 0 024 . |\n| MPNN            | 0 811 . ± 0 031 . | 0 838 . ± 0 027 . | 0 879 . ± 0 037 . | 0 598 . ± 0 031 . |\n| Attentive FP    | 0 822 . ± 0 026 . | 0 876 . ± 0 023 . | 0 887 . ± 0 032 . | 0 623 . ± 0 026 . |\n| Hopfield (ours) | 0 815 . ± 0 023 . | 0 902 . ± 0 023 . | 0 910 . ± 0 026 . | 0 672 . ± 0 019 . |",
        "metadata": {
            "section_header": "A.5.4 EXPERIMENT 4: DRUG DESIGN BENCHMARK DATASETS",
            "title": "HOPFIELD NETWORKS IS ALL YOU NEED",
            "type": "paper"
        }
    },
    {
        "page_content": "## A.6 PYTORCH IMPLEMENTATION OF HOPFIELD LAYERS\n\nThe implementation is available at: https://github.com/ml-jku/hopfield-layers",
        "metadata": {
            "section_header": "A.6 PYTORCH IMPLEMENTATION OF HOPFIELD LAYERS",
            "title": "HOPFIELD NETWORKS IS ALL YOU NEED",
            "type": "paper"
        }
    },
    {
        "page_content": "## A.6.1 INTRODUCTION\n\nIn this section, we describe the implementation of Hopfield layers in PyTorch (Paszke et al., 2017; 2019) and, additionally, provide a brief usage manual. Possible applications for a Hopfield layer in a deep network architecture comprise:\n\n- · multiple instance learning (MIL) (Dietterich et al., 1997),\n- · processing of and learning with point sets (Qi et al., 2017a;b; Xu et al., 2018),\n- · set-based and permutation invariant learning (Guttenberg et al., 2016; Ravanbakhsh et al., 2016; Zaheer et al., 2017; Korshunova et al., 2018; Ilse et al., 2018; Zhai et al., 2020),\n- · attention-based learning (Vaswani et al., 2017a),\n- · associative learning,\n- · natural language processing,\n- · sequence analysis and time series prediction, and\n- · storing and retrieving reference or experienced data, e.g. to store training data and retrieve it by the model or to store experiences for reinforcement learning.\n\nThe Hopfield layer in a deep neural network architecture can implement:\n\n- · a memory (storage) with associative retrieval (Danihelka et al., 2016; Ba et al., 2016a),\n- · conditional pooling and averaging operations (Wang et al., 2018; Ilse et al., 2020),\n- · combining data by associations (Agrawal et al., 1993),\n- · associative credit assignment (e.g. Rescorla-Wagner model or value estimation) (Sutton &amp; Barto, 2018), and\n- · attention mechanisms (Vaswani et al., 2017a; Bahdanau et al., 2014).\n\nIn particular, a Hopfield layer can substitute attention layers in architectures of transformer and BERT models. The Hopfield layer is designed to be used as plug-in replacement for existing layers like\n\n- · pooling layers (max-pooling or average pooling),\n\n- · permutation equivariant layers (Guttenberg et al., 2016; Ravanbakhsh et al., 2016),\n- · GRU &amp; LSTM layers, and\n- · attention layers.\n\nIn contrast to classical Hopfield networks, the Hopfield layer is based on the modern Hopfield networks with continuous states that have increased storage capacity, as discussed in the main paper. Like classical Hopfield networks, the dynamics of the single heads of a Hopfield layer follow a energy minimization dynamics. The energy minimization empowers our Hopfield layer with several advantages over other architectural designs like memory cells, associative memory, or attention mechanisms. For example, the Hopfield layer has more functionality than a transformer self-attention layer (Vaswani et al., 2017a) as described in Sec. A.6.2. Possible use cases are given in Sec. A.6.3. Source code will be provided under github .",
        "metadata": {
            "section_header": "A.6.1 INTRODUCTION",
            "title": "HOPFIELD NETWORKS IS ALL YOU NEED",
            "type": "paper"
        }
    },
    {
        "page_content": "## A.6.2 FUNCTIONALITY\n\nNon-standard functionalities that are added by a Hopfield layer are\n\n- · Association of two sets ,\n- · Multiple Updates for precise fixed points,\n- · Variable Beta that determines the kind of fixed points,\n- · Dimension of the associative space for controlling the storage capacity,\n- · Static Patterns for fixed pattern search, and\n- · Pattern Normalization to control the fixed point dynamics by norm of the patterns and shift of the patterns.\n\nA functional sketch of our Hopfield layer is shown in Fig. A.7.\n\n- · Association of two sets. The Hopfield layer makes it possible to associate two sets of vectors. This general functionality allows\n- · for transformer-like self-attention,\n- · for decoder-encoder attention,\n- · for time series prediction (maybe with positional encoding),\n- · for sequence analysis,\n- · for multiple instance learning,\n- · for learning with point sets,\n- · for combining data sources by associations,\n- · for constructing a memory,\n- · for averaging and pooling operations, and\n- · for many more.\n\nThe first set of vectors consists of S raw state patterns R = ( r 1 , . . . , r S ) T with r s ∈ R d r and the second set of vectors consists of N raw stored patterns Y = ( y 1 , . . . , y N ) T with y i ∈ R d y . Both the S raw state patterns and N raw stored patterns are mapped to an associative space in R d k via the matrices W Q ∈ R d r × d k and W K ∈ R d y × d k , respectively. We define a matrix Q ( Ξ T ) of state patterns ξ n = W r Q n in an associative space R d k and a matrix K X ( T ) of stored patterns x i = W y K s in the associative space R d k :\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nIn the main paper, Eq. (3) defines the novel update rule:\n\n<!-- formula-not-decoded -->\n\nFor multiple patterns, Eq. (3) becomes:\n\n<!-- formula-not-decoded -->\n\nwhere Ξ = ( ξ 1 , . . . , ξ N ) is the matrix of N state (query) patterns, X is the matrix of stored (key) patterns, and Ξ new is the matrix of new state patterns, which are averages over stored patterns. A new state pattern can also be very similar to a single stored pattern, in which case we call the stored pattern to be retrieved.\n\nThese matrices allow to rewrite Eq. (552) as:\n\n<!-- formula-not-decoded -->\n\nFor β = 1 / √ d k and changing in Eq. (553) softmax ∈ R N to a row vector (and evaluating a row vector), we obtain:\n\n<!-- formula-not-decoded -->\n\nwhere Q new is again the matrix of new state patterns. The new state patterns Ξ new are projected via W V to the result patterns Z = Ξ new W V , where W V ∈ R d k × d v . With the pattern projection V = KW V , we obtain the update rule Eq. (10) from the main paper:\n\n<!-- formula-not-decoded -->\n\n· Multiple Updates. The update Eq. (553) can be iteratively applied to the initial state ξ of every Hopfield layer head. After the last update, the new states Ξ new are projected via W V to the result patterns Z = Ξ new W V . Therefore, the Hopfield layer allows multiple update steps in the forward pass without changing the number of parameters. The number of update steps can be given for every Hopfield head individually. Furthermore, it is possible to set a threshold for the number of updates of every Hopfield head based on ‖ ξ -ξ new ‖ 2 . In the general case of multiple initial states Ξ , the maximum over the individual norms is taken.\n\n· Variable β . In the main paper, we have identified β as a crucial parameter for the fixed point dynamics of the Hopfield network, which governs the operating mode of the attention heads. In appendix, e.g. in Lemma A7 or in Eq. (102) and Eq. (103), we showed that the characteristics of the fixed points of the new modern Hopfield network are determined by: β M , (maximal pattern norm), m max (spread of the similar patterns), and ‖ m x ‖ (center of the similar patterns). Low values of β induce global averaging and higher values of β metastable states. In the transformer attention, the β parameter is set to β = 1 / √ d k as in Eq. (555). The Hopfield layer, however, allows to freely choose β &gt; 0 , since the fixed point dynamics does not only depend on the dimension of the associative space d k . Additionally, β heavily influences the gradient flow to the matrices W Q and W K . Thus, finding the right β for the respective application can be crucial.\n\n· Variable dimension of the associative space. Theorem A5 says that the storage capacity of the modern Hopfield network grows exponentially with the dimension of the associative space. However higher dimension of the associative space also means less averaging and smaller metastable states. The dimension of the associative space trades off storage capacity against the size of metastable states, e.g. over how many pattern is averaged. In Eq. (550) and in Eq. (549), we assumed N raw state patterns R = ( r 1 , . . . , r N ) T and S raw stored patterns Y = ( y 1 , . . . , y S ) T that are mapped to a d k -dimensional associative space via the matrices W Q ∈ R d r × d k and W K ∈ R d y × d k , respectively. In the associative space R d k , we obtain the state patterns Q = Ξ T = RW Q and the stored patterns K = X T = Y W K . The Hopfield view relates the dimension d k to the number of input patterns N that have to be processed. The storage capacity depends exponentially on the dimension d k (the dimension of the associative space) and the size to metastable states is governed by this dimension, too. Consequently, d k should be chosen with respect to the number N of patterns one wants to store and the desired size of metastable states, which is the number of patterns one wants to average over. For example, if the input consists of many low dimensional input patterns, it makes sense to project the patterns into a higher dimensional space to allow a proper fixed point dynamics. Intuitively, this coincides with the construction of a richer feature space for the patterns.\n\n· Static Patterns. In Eq. (550) and Eq. (549), the N raw state patterns R = ( r 1 , . . . , r N ) T and S raw stored patterns Y = ( y 1 , . . . , y S ) T are mapped to an associative space via the matrices W Q ∈ R d r × d k and W K ∈ R d y × d k , which gives the state patterns Q = Ξ T = RW Q and the stored\n\npatterns K = X T = Y W K . We allow for static state and static stored patterns. Static pattern means that the pattern does not depend on the network input, i.e. it is determined by the bias weights and remains constant across different network inputs. Static state patterns allow to determine whether particular fixed patterns are among the stored patterns and vice versa. The static pattern functionality is typically needed if particular patterns must be identified in the data, e.g. as described for immune repertoire classification in the main paper, where a fixed d k -dimensional state vector ξ is used.\n\n· Pattern Normalization. In the appendix, e.g. in Lemma A7 or in Eq. (102) and Eq. (103), we showed that the characteristics of the fixed points of the new modern Hopfield network are determined by: β , M (maximal pattern norm), m max (spread of the similar patterns), and ‖ m x ‖ (center of the similar patterns). We already discussed the parameter β while the spread of the similar patterns m max is given by the data. The remaining variables M and m x that both control the fixed point dynamics are adjusted pattern normalization. M is the maximal pattern norm and m x the center of the similar patterns. Theorem A5 says that larger M allows for more patterns to be stored. However, the size of metastable states will decrease with increasing M . The vector m x says how well the (similar) patterns are centered. If the norm ‖ m x ‖ is large, then this leads to smaller metastable states. The two parameters M and m x are controlled by pattern normalization and determine the size and convergence properties of metastable states. These two parameters are important for creating large gradients if heads start with global averaging which has small gradient. These two parameters can shift a head towards small metastable states which have largest gradient as shown in Fig. A.5(b). We allow for three different pattern normalizations, where the first is the default setting:\n\n- · pattern normalization of the input patterns,\n- · pattern normalization after mapping into the associative space,\n- · no pattern normalization.",
        "metadata": {
            "section_header": "A.6.2 FUNCTIONALITY",
            "title": "HOPFIELD NETWORKS IS ALL YOU NEED",
            "type": "paper"
        }
    },
    {
        "page_content": "## A.6.3 USAGE\n\nAs outlined in Sec. A.6.1, there are a variety of possible use cases for the Hopfield layer, e.g. to build memory networks or transformer models. The goal of the implementation is therefore to provide an easy to use Hopfield module that can be used in a wide range of applications, be it as part of a larger architecture or as a standalone module. Consequently, the focus of the Hopfield layer interface is set on its core parameters: the association of two sets, the scaling parameter β , the maximum number of updates, the dimension of the associative space, the possible usage of static patterns, and the pattern normalization. The integration into the PyTorch framework is built such that with all the above functionalities disabled, the 'HopfieldEncoderLayer' and the 'HopfieldDecoderLayer', both extensions of the Hopfield module, can be used as a one-to-one plug-in replacement for the TransformerEncoderLayer and the TransformerDecoderLayer , respectively, of the PyTorch transformer module.\n\nThe Hopfield layer can be used to implement or to substitute different layers:\n\n- · Pooling layers: We consider the Hopfield layer as a pooling layer if only one static state (query) pattern exists. Then, it is de facto a pooling over the sequence, which results from the softmax values applied on the stored patterns. Therefore, our Hopfield layer can act as a pooling layer.\n- · Permutation equivariant layers: Our Hopfield layer can be used as a plug-in replacement for permutation equivariant layers. Since the Hopfield layer is an associative memory it assumes no dependency between the input patterns.\n- · GRU &amp; LSTM layers: Our Hopfield layer can be used as a plug-in replacement for GRU &amp; LSTM layers. Optionally, for substituting GRU &amp; LSTM layers, positional encoding might be considered.\n- · Attention layers: Our Hopfield layer can act as an attention layer, where state (query) and stored (key) patterns are different, and need to be associated.\n- · Finally, the extensions of the Hopfield layer are able to operate as a self-attention layer (HopfieldEncoderLayer) and as cross-attention layer (HopfieldDecoderLayer), as described in (Vaswani et al., 2017a). As such, it can be used as building block of transformer-based or general architectures.\n\nFigure A.7: A flowchart of the Hopfield layer. First, the raw state (query) patterns R and the raw stored (key) patterns Y are optionally normalized (with layer normalization), projected and optionally normalized (with layer normalization) again. The default setting is a layer normalization of the input patterns, and no layer normalization of the projected patterns. The raw stored patterns Y can in principle be also two different input tensors. Optionally, multiple updates take place in the projected space of Q and K . This update rule is obtained e.g. from the full update Eq. (423) or the simplified update Eq. (424) in the appendix.\n\n<!-- image -->",
        "metadata": {
            "section_header": "A.6.3 USAGE",
            "title": "HOPFIELD NETWORKS IS ALL YOU NEED",
            "type": "paper"
        }
    }
]