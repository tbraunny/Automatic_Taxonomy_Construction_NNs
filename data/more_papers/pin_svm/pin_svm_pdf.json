[
    {
        "page_content": "## I. INTRODUCTION\n\nSupport Vector Machines (SVMs) [3] [4] [5] are popular machine learning algorithms. These algorithms are based on Structural Risk Minimization (SRM) principle [4]. For binary classification problem with given training set T = ( { x , y i i ) : x i ∈ R n , y i ∈ {-1 1 , } , i = 1 2 . , ..., l } , SVM models obtain a separating kernel generated decision function w φ x T ( i )+ b = 0 by minimizing a good trade-off between the empirical risk and model complexity in its optimization problem. SVM models use a loss function to measure the empirical risk of the given training set. For minimizing the model complexity, SVM models minimize a regularization term in their optimization problem.\n\nPritam Anand is with the Dhirubhai Ambani Institute of Information Technology, Gandhinagar Gujrat, India - 382007. (e-mail: ltpritamanand@gmail.com, pritam anand@daiict.ac.in)\n\nReshma Rastogi is with the Department of Computer Science, South Asian University, New Delhi-110021. (e-mail: reshma.khemchandani@sau.ac.in)\n\nSuresh Chandra was with the Department of Mathematics, Indian Institute of Technology, Delhi-110016. (e-mail: sureshiitdelhi@gmail.com.)\n\nThe standard C-SVM model minimizes the Hinge loss function along with the L 2 -norm regularization in its formulation. Thus, it minimizes\n\n<!-- formula-not-decoded -->\n\nwhere L Hinge = max u, ( 0) is the Hinge loss function and C 0 ≥ 0 is the user supplied parameter. The use of Hinge loss function in C-SVM model makes it ignore the data points which satisfy y i ( w φ x T ( i ) + b ) &gt; 1 . There are few data points satisfying y i ( w φ x T ( i )+ ) b ≤ 1 , which contribute for the empirical risk. These data points are called 'support vectors' and lie near the boundary of the separating hyperplane w φ x T ( )+ b = 0 . The separating hyperplane in C-SVM model is only constructed by using these support vectors. This causes the sparsity in C-SVM model. But, data points near the boundary of the separating hyperplane may be noisy which can mislead the resulting separating hyperplane. To improve the C-SVM model, Huang et al. [2] have suggested to use the pinball loss function [6] in SVM model. For the classification problem, the pinball loss function is given by\n\n<!-- formula-not-decoded -->\n\nwhere 0 ≤ τ ≤ 1 is its parameter. For τ = 0 , the pinball loss function reduces to the Hinge loss function. For τ = 1 , it reduces to the l 1 loss function.\n\nThe Pin-SVM model (Huang et al., [2]) minimizes the empirical risk using the pinball loss function along with the L 2 -norm regularization in its formulation. This leads to the following optimization problem\n\n<!-- formula-not-decoded -->\n\nwhich can be equivalently converted to the following optimization problem\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nwhere ξ 1 , ξ 2 , .., ξ l are slack variables and C , 0 0 ≤ τ ≤ 1 are user supplied parameters.\n\n1\n\nThe pinball loss function in Pin-SVM model penalizes (assigns positive risk) every data point but, with different rate. Data points satisfying y i ( w φ x T ( i ) + b ) ≤ 1 are penalized with unit rate and other data points are penalized with comparatively lower rate τ . This penalization in Pin-SVM model causes it to also minimize the scatter of data points along the separating hyperplane. But then, it takes away the very nice property of SVM, namely sparsity. However, the PinSVM model is a general SVM model in the sense that it can reduce to the standard C-SVM model for its parameter τ = 0 .\n\nTo reduce the effect of the unbalanced class labeling, we consider a l -dimensional vector C = ( C , C , . . . C 1 2 l ) , rather than a single constant C 0 , such that\n\n<!-- formula-not-decoded -->\n\nwhere p is defined as optimization\n\np = number of data points on 'Class +1' number of data points in 'Class -1' and seek the solution of following problem\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nRather than solving the primal problem (5), we prefer to solve its Wolfe's dual problem, which is obtained as follows\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nsubject to,\n\n<!-- formula-not-decoded -->\n\nMore information about properties of pinball loss function and Pin-SVM model can be found in (Huang et al., [2]).\n\nWe organize the rest of this paper as follows. Section II describes the optimization problem of Pin-SVM model for -1 ≤ τ &lt; 0 as proposed in (Hunag et al., [1]). In section III, we derive the right optimization problem for Pin-SVM model for -1 ≤ τ &lt; 0 . In section IV, we propose a unified optimization problem which can obtain the solution of Pin-SVM model without bothering the sign of its parameter τ in [ -1 1] , . We term this proposed model as Unified Pin-SVM model. Section V presents numerical results which empirically verify that the proposed Unified Pin-SVM model corrects the existing Pin-SVM model by minimizing the pinball loss function in true sense.",
        "metadata": {
            "section_header": "I. INTRODUCTION",
            "title": "Improvement over Pinball Loss Support Vector Machine",
            "type": "paper"
        }
    },
    {
        "page_content": "## II. PINBALL LOSS FUNCTION WITH NEGATIVE τ VALUE AND SVM MODEL\n\nHuang et al. extended the pin-ball loss function for negative τ using the same expression in their work (Huang et al. [1]) . The pinball loss function with negative τ is given by\n\n<!-- formula-not-decoded -->\n\nThe above pinball loss function (7) is convex loss function for τ ≥ -1 . Huang et al. have formulated the Pin-SVM model for - ≤ 1 τ &lt; 0 using\n\n<!-- formula-not-decoded -->\n\nFor minimizing (8), they have chosen to minimize the following Quadratic Programming Problem (QPP)\n\n<!-- formula-not-decoded -->\n\nwhere -1 ≤ τ &lt; 0 is user supplied parameter. It should be noted that, Huang et al. have used the same Pin-SVM optimization problem for both positive and negative values of τ in [ -1 1] , . Contrary to this, we claim in the next section of this paper that the Pin SVM model for -1 ≤ τ &lt; 0 requires the solution of a QPP which is different from (9).",
        "metadata": {
            "section_header": "II. PINBALL LOSS FUNCTION WITH NEGATIVE τ VALUE AND SVM MODEL",
            "title": "Improvement over Pinball Loss Support Vector Machine",
            "type": "paper"
        }
    },
    {
        "page_content": "## III. PIN-SVM WITH NEGATIVE τ VALUES\n\nThis paper improves the existing Pin-SVM model for - ≤ 1 τ &lt; 0 (Huang et al., [1]). We shall show that the optimization problem of existing Pin-SVM model for -1 ≤ τ &lt; 0 obtained in (Huang et al., [1]) is not correct and derive the right optimization problem for it.\n\nThe pinball loss function (7) has been used in (Huang et al., [1] [2]) for -1 ≤ τ ≤ 1 . At first, we consider the loss function\n\n<!-- formula-not-decoded -->\n\nfor -1 ≤ τ ≤ 1 . For -1 ≤ τ ≤ 1 , we can obtain max u, ( -τu ) = { u, if u ≥ 0 . -τu, otherwise .\n\nIt makes us realize that the pinball loss function is equivalent to the max u, ( -τu ) for -1 ≤ τ ≤ 1 .\n\nNow, we shall state and justify our claim about the existing Pin-SVM model with -1 ≤ τ &lt; 0 . We claim that the Pin-SVM model with -1 ≤ τ &lt; 0 (problem (8)) is not equivalent to the solving QPP (9) used in (Huang et al., [1]) and vice-versa. The justification of this claim is detailed as follows.\n\nThe Pin-SVM for -1 ≤ τ &lt; 0 (problem (8)) is equivalent to\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n-\n\nτ\n\n(1\n\n-\n\ny\n\ni\n\n(\n\nw φ x\n\nT\n\n(\n\ni\n\n) +\n\nb\n\n)))\n\nwhere\n\n-\n\n1\n\n≤\n\nτ &lt;\n\n0\n\n.\n\n(11) primal problem (15) as follows where C i are as defined in (4) and -1 ≤ τ &lt; 0 . In order to find the solution of above primal problem, we need to derive its corresponding Wolfe's dual problem. For this, we construct the Lagrangian function for\n\nLet us consider slack variables ξ i = max ((1 -y i ( w φ x T ( i ) + b )) , -τ (1 -y i ( w φ x T ( i ) + b ))) , i = 1 2 , , ..., l . Then, the optimization problem (11) of PinSVM can be given by\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nSince τ &lt; 0 in above optimization problem (12), so its second constraint\n\n<!-- formula-not-decoded -->\n\nSimilarly, the first constraint of problem (12)\n\n<!-- formula-not-decoded -->\n\nNow, optimization problem (12) can be obtained as\n\n<!-- formula-not-decoded -->\n\nwhere -1 ≤ τ &lt; 0 , which is different from QPP (9) used in (Huang et al., [1]). It also infers that QPP (14) is the actual minimizer of the Pin-SVM model with -1 ≤ τ &lt; 0 (problem (8)).",
        "metadata": {
            "section_header": "III. PIN-SVM WITH NEGATIVE τ VALUES",
            "title": "Improvement over Pinball Loss Support Vector Machine",
            "type": "paper"
        }
    },
    {
        "page_content": "## A. Solution of QPP for Pin-SVM with negative τ\n\nFor unbalanced training set, the Pin-SVM optimization problem with -1 ≤ τ &lt; 0 can also be modified\n\nWe list some relevant Karush-Kuhn-Tucker(KKT) conditions for the optimization problem (15) as follows\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nUsing the KKT conditions, the Wolfe's dual of the primal problem (15) can be obtained as follows\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nBy using a positive semi-definite kernel K x , x ( i j ) = φ x ( i ) T φ x ( j ) , satisfying Mercer condition (Mercer,\n\n[7]), the above dual problem can be obtained as\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nAfter obtaining the solution of the dual problem (20), the value of w can be obtained from the KKT condition (17) as follows\n\n<!-- formula-not-decoded -->\n\nLet us now define the following set\n\n<!-- formula-not-decoded -->\n\nUsing the complementary slackness condition, we compute the values of the b for each i ∈ S , from\n\n<!-- formula-not-decoded -->\n\nand take their average value as the final value of the bias b . For given test point x ∈ R n , the decision function is obtained as\n\n<!-- formula-not-decoded -->",
        "metadata": {
            "section_header": "A. Solution of QPP for Pin-SVM with negative τ",
            "title": "Improvement over Pinball Loss Support Vector Machine",
            "type": "paper"
        }
    },
    {
        "page_content": "## IV. A UNIFIED QPP FOR SOLVING PIN-SVM PROBLEM\n\nWe can observe that minimizing the Pin-SVM problem with positive and negative τ value in [ -1 1] , results into two different QPPs. Minimizing different QPPs for negative and positive τ value in Pin-SVM problem may not be handful for searching best τ ∈ -[ 1 1] , , which corresponds to the optimal accuracy. Taking motivation from this, we also propose a unified optimization problem which can obtain the solution of Pin-SVM problem without bothering about the sign of its parameter τ . For a given τ ∈ [ -1 1] , , the Pin-SVM model should minimize\n\n<!-- formula-not-decoded -->\n\nAfter introducing the slack variable ξ i = max (1 -y i ( w φ x T ( i ) + b , ) -τ (1 -y i ( w φ x T ( i ) + b )) , the PinSVM problem becomes\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nFor the unbalanced training set, the suitable Pin-SVM problem can be given by\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nWe obtain the Lagrangian function for the primal problem (25) as follow\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nWe list some relevant KKT optimality conditions for the optimization problem (25) as follows.\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nUsing the KKT optimality conditions, the Wolfe's dual of the primal problem (25) is obtained as follows\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nIf we consider the replacement of variable β := | τ β | in dual problem (30) and define a signum function\n\n<!-- formula-not-decoded -->\n\nthen, the dual problem\n\n<!-- formula-not-decoded -->\n\nsubject to,\n\n<!-- formula-not-decoded -->\n\nIt is notable that for 0 ≤ τ ≤ 1 , the proposed dual problem (31) is equivalent to dual problem (6) of PinSVM model. For -1 ≤ τ &lt; 0 , the proposed dual problem (31) can be found to be equivalent to the dual problem (20) of Pin-SVM model for -1 ≤ τ &lt; 0 . This is because τ = s τ | τ | .\n\nAfter obtaining the solution of the dual problem (31), the value of w can be given by\n\n<!-- formula-not-decoded -->\n\nFor finding the value of b , we consider each index i such that α i &gt; 0 and β i &gt; 0 , and compute the value of b using the complementary slackness condition as follow\n\n<!-- formula-not-decoded -->\n\nWe consider the final value of b by taking the average over all possible values of b . For given test point x ∈ R n , the decision function is obtained as\n\n<!-- formula-not-decoded -->\n\nThe proposed unified QPP (31) should be solved for minimizing the pinball loss function in SVM for -1 ≤ τ ≤ 1 . Some properties of Pin-SVM models like noise-insensitivity and non-sparsity have been only induced by the use of pinball loss function in the SVM model. Therefore, these properties do not vary in the proposed Unified Pin-SVM model. For clarity, we explicitly describe the algorithm of the proposed Unified Pin-SVM model in Algorithm 1.",
        "metadata": {
            "section_header": "IV. A UNIFIED QPP FOR SOLVING PIN-SVM PROBLEM",
            "title": "Improvement over Pinball Loss Support Vector Machine",
            "type": "paper"
        }
    },
    {
        "page_content": "## V. EXPERIMENTAL RESULTS\n\nIn this section, we justify our claims made in this paper empirically. For this, we perform numerical experiments with some commonly real-world benchmark datasets. Table I shows the description of the used",
        "metadata": {
            "section_header": "V. EXPERIMENTAL RESULTS",
            "title": "Improvement over Pinball Loss Support Vector Machine",
            "type": "paper"
        }
    },
    {
        "page_content": "## Algorithm 1 Unified Pin-SVM\n\nInput:Training set T = { ( x , y i i ) : x i ∈ R n , y i ∈ {-1 1 , } , i = 1 2 . , ..., l } , test data x ∈ R n , and parameter τ .\n\nOutput:Predicted label for test data x .\n\n- (i) Select a penalty parameter C 0 &gt; 0 and kernel parameter q , if required . These parameter are commonly selected through validation.\n- (ii) For i = 1 2 , , .., l , compute C i using (4).\n- (iii) For the linear kernel compute k x , x ( i j ) = x T i x j . For Gaussian kernel compute k x , x ( i j ) = exp ( -|| x i -x j || 2 2 q 2 ) .\n- (iv) Obtain the solution vectors α , β by solving the proposed QPP (31).\n- (v) Also obtain the value of bias b using (33).\n- (vi) Predict the label of test point x using (34).\n\nTABLE I: Dataset Description\n\n| Dataset                                                                                                                                                | Size                                                                                                                     | Training points                                       |\n|--------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------|\n| Monk 1 Monk 2 Monk 3 Spect                                                                                                                             | 556 × 7 601 × 7 554 × 7 267 × 22                                                                                         | 124 169 122 80                                        |\n| Fertility D. Echocardiogram Plrx Sonar Heart Statlog Haberman Votes Ecoil Ionosphere Bupa Liver Pima Indian Breast Cancer Australian Diabetes Spambase | 100 × 10 131 × 10 182 × 13 208 × 61 270 × 14 306 × 4 435 × 17 327 × 8 351 × 34 345 × 7 768 × 9 569 × 31 690 × 15 768 × 9 | 50 80 100 100 150 150 200 200 200 250 300 400 400 500 |\n|                                                                                                                                                        | 4601 × 57                                                                                                                | 4000                                                  |\n\ndatasets in our experiments. The first four datasets in Table I contain the training and testing set provided. For other datasets, we have divided the training and testing set in Table I. We have normalized the training and testing set in [ -1 1] , .\n\nNow, we describe our experimental setup. We have performed all experiments in MATLAB 2018 (in.mathworks.com) environment on a Dell Xeon processor with 16 GB of RAM and Windows 10 operating system. We have solved the dual QPP (6) of the Pin-SVM model and the proposed dual QPP (31) of Unified Pin-SVM model with 'quadprog' function available in MATLAB. We have used linear kernel and RBF kernel of the form exp ( -|| x - || y 2 2 q 2 ) in these QPPs of Pin-SVM models. The MATLAB codes of the proposed Unified Pin-SVM model and existing Pin-SVM models are available at https://github.com/ltpritamanand/UnifiedPinSVM/ tree/mycode/Unfied-Pin-SVM-master.\n\nBefore reporting final numerical results, we have\n\nobtained the best possible choices of parameter C 0 and RBF kernel parameter q of Pin-SVM models. For this, we have set τ = 0 in Pin-SVM model and searched best possible values of ( C, q ) from the set { 2 -7 , 2 -6 , ......, 2 6 , 2 7 } × { 2 -7 , 2 -6 , ......, 2 6 , 2 7 } . After tunning the value of these parameters, we have obtained the accuracy of the Pin-SVM model and proposed Unified Pin-SVM model for different values of τ on different datasets listed in Table I.\n\nFigure 1 shows the plot of accuracy on several datasets obtained by the existing Pin-SVM model and proposed Unified Pin-SVM model against different τ values from the set {-1 , -0 99 . , ...., .. 0 99 . , 1 } with linear kernel. In these plots, the red-line represents the accuracy obtained by Pin-SVM and the black line represents the accuracy obtained by the proposed Unified Pin-SVM model. It should be noted that at τ = 0 , the Pin-SVM and proposed Unified Pin-SVM model reduces to the C -SVM model. We can obtain the following observations from plots in Figure 1.\n\n- 1) In each plot, we can observe that the black line hides the red line on the right side of the Y -axis. It confirms that for τ ≥ 0 , the Pin-SVM model and proposed Unified SVM model are equivalent.\n- 2) In each plot, the red line differs from the black line in left side of the Y -axis. It empirically confirms that the Pin-SVM model (9) for -1 ≤ τ &lt; 0 is different from the Unified Pin-SVM model for - ≤ 1 τ &lt; 0 .\n- 3) Further, we can observe that the black line appears above the red line on the left side of Y -axis in most of the cases. It means that for -1 ≤ τ &lt; 0 , the proposed Unified Pin-SVM can obtain better accuracy than the existing Pin-SVM model (9). It is because of the fact that Unified Pin-SVM minimizes the pinball loss function for -1 ≤ τ &lt; 0 in true spirit.\n\nWe have also plotted the accuracy obtained by the proposed Unified Pin-SVM model and existing PinSVM model against different values of parameter C and τ in Figure 3 for few datasets. For this, we have varied τ and C in the range {-1 , -0 9 . , ..., 0 9 . , 1 } and { 2 -7 , 2 -6 , ..., 2 6 , 2 7 } respectively. Figure 3 confirms that irrespective of choice of parameters, the proposed Unified Pin-SVM model outperforms the existing PinSVM model for - ≤ 1 τ &lt; 0 .\n\nTable II lists the optimal performance of the existing C -SVM model, Pin-SVM model and Unified Pin-SVM model along with their training time and tunned parameters value. It can be observed that in the case of several datasets like Spect, Haberman, Echo, Australian, Diabetes, Sonar and Spambase, the use of proposed Unified Pin-SVM over existing Pin-SVM and C-SVM model can result in significant improvement\n\nTABLE II: Pin-SVM models with linear kernel\n\n| Dataset                    | SVM models              | Accuracy    | Time(s)   | τ          |\n|----------------------------|-------------------------|-------------|-----------|------------|\n| Monk1 C = 0 . 0625         | Unified Pin-SVM         | 65.28       | 0.17      | 0.00       |\n| 0                          | Pin-SVM                 | 65.28       | 0.16      | 0.00       |\n|                            | C-SVM                   | 65.28       | 0.15      | -          |\n| Monk2                      | Unified Pin-SVM         | 67.13       | 0.29      | -0.60      |\n| C 0 = 0 . 0078             | Pin-SVM                 | 67.13       | 0.29      | -0.99      |\n|                            | SVM                     | 67.13       | 0.22 0.17 | -          |\n| Monk3                      | Unified Pin-SVM         | 83.10       | 0.17      | 0.16       |\n| C = 0 . 0078               | Pin-SVM                 | 83.10       |           | 0.16 -     |\n| Spect                      |                         |             |           |            |\n| 0                          | C-SVM                   | 81.02       | 0.15      |            |\n|                            | Unified Pin-SVM         | 93.58       | 0.07      | -0.85      |\n| C 0 = 0 . 0156             | Pin-SVM                 | 91.98       | 0.08      | -0.99      |\n| Haberman C 0 = 0 . 0078    | Unified Pin-SVM Pin-SVM | 76.28 73.08 | 0.19 0.11 | -0.61 0.00 |\n|                            |                         | 73.08       | 0.10      | -          |\n| Heart Statlog C = 0 . 0625 | Unified Pin-SVM Pin-SVM | 86.67       | 0.09      | 0.00       |\n|                            | C-SVM                   | 86.67       | 0.09      | 0.00       |\n| 0 Ionosphere               | C-SVM Unified Pin-SVM   | 86.67       | 0.09      | -          |\n|                            |                         | 94.04       | 0.16      | 0.00       |\n| C 0 = 2                    | Pin-SVM                 | 94.04       | 0.16      | 0.00       |\n| Pima C 0 = 0 . 0156        | C-SVM Unified Pin-SVM   | 94.04 67.31 | 0.16 0.89 | - -0.99    |\n|                            | Pin-SVM                 | 68.80       | 9.68      | -1.00      |\n| Breast C.                  | C-SVM Unified Pin-SVM   | 67.09       | 0.51      | - -0.25    |\n| C = 0 . 0078               |                         |             |           | 0.11       |\n|                            |                         | 97.63       | 0.95      |            |\n| 0                          | Pin-SVM                 | 97.63       | 1.10      |            |\n| Echo C 0 = 0 . 0078        | C-SVM                   | 85.80       | 0.54      | - -0.51    |\n|                            | Pin-SVM                 | 74.51       | 0.03      | 0.00       |\n|                            | Unified Pin-SVM C-SVM   | 90.20 74.51 | 0.04 0.03 | -          |\n| Australian                 |                         |             |           |            |\n|                            | Unified Pin-SVM         | 87.24       | 1.05      | -0.30      |\n| C 0 = 0 . 0313             | Pin-SVM                 | 84.48       | 0.64      | 0.00       |\n| Bupa Liver                 | C-SVM Unified Pin-SVM   | 84.48 63.16 | 0.63 0.19 | - 0.00     |\n| C 0 = 0 . 0156             | Pin-SVM C-SVM           | 63.16 63.16 | 0.20 0.20 | 0.00 -     |\n| Votes                      | Unified Pin-SVM         | 93.62       | 0.29      | -0.08      |\n| C 0 = 0 . 0156             | Pin-SVM                 | 94.47       | 0.32      | -0.99      |\n| Diabetes C 0 = 0 . 0078    | Unified Pin-SVM Pin-SVM | 75.75 67.91 | 1.72 0.89 | -0.59 0.00 |\n|                            | C-SVM                   | 67.91       | 0.87      | -          |\n| Fertility                  | Unified Pin-SVM         | 94.00       | 0.19      | -1.00      |\n| C = 0 . 0078               | Pin-SVM                 |             |           |            |\n| 0                          | C-SVM                   | 94.00 94.00 | 0.01 0.01 | 0.00 -     |\n| Sonar C 0 = 0 . 0313       | Unified Pin-SVM         | 81.48       | 0.07      | -0.63      |\n|                            | Pin-SVM                 | 77.78       | 0.86      | -1.00      |\n|                            | Unified Pin-SVM         | 96.85       | 0.15      | 0.00       |\n|                            | C-SVM                   | 75.93       | 0.05      | -          |\n| Ecoil C 0 = 2              | Pin-SVM                 |             |           |            |\n|                            | C-SVM                   | 96.85 96.85 | 0.15 0.15 | 0.00 -     |\n| Parlx C 0 = 0 . 0078       | Unified Pin-SVM         | 67.07       | 0.75      | -1.00      |\n|                            | Pin-SVM                 | 67.07       | 0.04      | -1.00      |\n| Spambase                   |                         |             |           | -0.95      |\n|                            | C-SVM                   | 67.07       | 0.04      | -          |\n|                            | Unified Pin-SVM         | 68.39       | 265.35    |            |\n|                            | Pin-SVM                 | 59.15       |           | 0          |\n| C 0 = 0 . 0078             |                         | 59.15       | 68.12     |            |\n|                            | C-SVM                   |             | 68.58     |            |\n\nof accuracy. It is because of the fact that unlike the existing Pin-SVM model, the proposed Unified PinSVM model also minimizes the pinball loss function for - ≤ 1 τ &lt; 0 in true spirit.\n\nWe repeat the similar numerical experiments with the existing C -SVM model, Pin-SVM model and proposed Unified Pin-SVM model for RBF kernel also. The numerical results are listed in Table III. Figure 2 shows the plot of accuracy on several datasets obtained by the existing Pin-SVM model and proposed Unified Pin-SVM model against different τ values from the set {-1 , -0 9 . , ...., .. 0 9 . , 1 } with RBF kernel. These plots and numerical results are consistent with the\n\nFig. 1: Comparison of existing Pin-SVM model and proposed Unified Pin-SVM model with linear kernel.\n\n<!-- image -->\n\nFig. 2: Comparison of existing Pin-SVM model and proposed Unified Pin-SVM model with RBF kernel.\n\n<!-- image -->\n\nTABLE III: Pin-SVM models with RBF kernel\n\nobservations which have been made in the linear kernel case.\n\n| Dataset        | SVM models      |   Acc. |   Time(s) | τ     |\n|----------------|-----------------|--------|-----------|-------|\n| Monk1          | Unified Pin-SVM |  84.95 |      0.22 | 0.12  |\n| p = 1          | Pin-SVM         |  84.95 |      0.19 | 0.12  |\n| C 0 = 16       | C-SVM           |  83.33 |      0.17 | -     |\n| Monk2          | Unified Pin-SVM |  86.11 |      0.28 | -0.32 |\n| p = 0 . 5      | Pin-SVM         |  85.65 |      0.25 | 0     |\n| C 0 = 1        | SVM             |  85.65 |      0.25 | -     |\n| Monk3          | Unified Pin-SVM |  91.67 |      0.17 | 0     |\n| p = 2 ,        | Pin-SVM         |  91.67 |      0.17 | 0     |\n| C 0 = 2        | C-SVM           |  91.67 |      0.18 | -     |\n| Spect          | Unified Pin-SVM |  93.58 |      0.06 | -0.59 |\n| p = 0 . 0078   | Pin-SVM         |  93.58 |      0.05 | 0     |\n| C 0 = 0 . 5    | C-SVM           |  93.58 |      0.05 | -     |\n| Pima           | Unified Pin-SVM |  76.07 |      0.65 | 0.45  |\n| p = 0 . 5      | Pin-SVM         |  76.07 |      0.63 | 0.45  |\n| C 0 = 0 . 0625 | C-SVM           |  75.85 |      0.53 | -     |\n| German         | Unified Pin-SVM |  68.8  |      0.95 | -0.14 |\n| p = 1          | Pin-SVM         |  68    |      1.1  | 0     |\n| C 0 = 2        | C-SVM           |  68    |      0.54 | -     |\n| Australian     | Unified Pin-SVM |  87.59 |      0.95 | -0.86 |\n| C 0 = 2        | Pin-SVM         |  82.41 |      0.66 | 0.00  |\n| C 0 = 0 . 0078 | C-SVM           |  82.41 |      0.66 | -     |\n| Bupa Liver     | Unified Pin-SVM |  65.26 |      0.34 | -0.74 |\n| p = 0 . 25     | Pin-SVM         |  65.26 |      0.32 | 0.84  |\n| C 0 = 0 . 1250 | C-SVM           |  64.21 |      0.24 | -     |\n| Diabetes       | Unified Pin-SVM |  79.1  |      1.8  | 0.01  |\n| p = 0 . 5      | Pin-SVM         |  79.1  |      1.81 | 0.01  |\n| C 0 = 0 . 0313 | C-SVM           |  78.36 |      0.91 | -     |",
        "metadata": {
            "section_header": "Algorithm 1 Unified Pin-SVM",
            "title": "Improvement over Pinball Loss Support Vector Machine",
            "type": "paper"
        }
    },
    {
        "page_content": "## VI. CONCLUSIONS\n\nThis paper proposes a significant improvement over the Pin-SVM model. For this, it re-look the pinball loss function for -1 ≤ τ &lt; 0 and its corresponding optimization problem used in the Pin-SVM model. It finds that the optimization problem used in (Huang et al, [1]) fails to minimize the pinball loss function for -1 ≤ τ &lt; 0 in its true sense. Thereafter, it develops the right optimization problem which can minimize the pinball loss function for -1 ≤ τ &lt; 0 in its true sense.\n\nIt makes us realize that the Pin-SVM model requires to solve different QPP for its positive and negative τ values in [ -1 1] , . Taking motivation from this, we further propose a Unified Pin-SVM QPP which can be used to solve the Pin-SVM model without bothering the sign of its parameter τ in [ -1 1] , . The proposed Unified Pin-SVM model can obtain a significant improvement in accuracy over the Pin-SVM model, as it can also minimize the pinball loss function with\n\nFig. 3: Plot of accuracy obtained by Pin-SVM models for different values of its parameters τ and C o .\n\n<!-- image -->\n\n-1 ≤ τ &lt; 0 in true sense. We have performed extensive numerical experiments with nineteen realworld datasets and shown empirically that the proposed Unified Pin-SVM model can always obtain an improvement over the existing Pin-SVM model.",
        "metadata": {
            "section_header": "VI. CONCLUSIONS",
            "title": "Improvement over Pinball Loss Support Vector Machine",
            "type": "paper"
        }
    },
    {
        "page_content": "## ACKNOWLEDGMENTS\n\nWe are extremely grateful to the anonymous reviewers and Editor for their valuable comments that helped us to enormously improve the quality of the paper.",
        "metadata": {
            "section_header": "ACKNOWLEDGMENTS",
            "title": "Improvement over Pinball Loss Support Vector Machine",
            "type": "paper"
        }
    }
]