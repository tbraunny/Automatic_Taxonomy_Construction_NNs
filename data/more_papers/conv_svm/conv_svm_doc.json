[
    {
        "page_content": "## 1. INTRODUCTION\n\nData mining is an effective method for examining and learning from extensive compound datasets of varying quality [1], and has been broadly applied to numerous practical problems in medicine [2, 3, 4], engineering  [5],  time  series  data  [6],  image  classification  [7],  speech  recognition  [8],  handwritten recognition  [9],  management  [10],  and  social  sciences  [11],  with  classification  being  one  of  the  most popular topics in data mining. Numerous classifiers for data mining have been established such as support vector machines (SVMs) [3, 4, 7] and deep learning algorithms [8, 9, 12].\n\nDeep learning based on artificial neural networks (ANNs) is made up of neurons that have learnable weights and biases such that the neural network, a special mathematical function, is connected or close to the  data  in  dataset  as  much  as  possible  [12,  13].  Deep  learning  techniques  include  convolution  neural networks  (CNNs) for  the  continuous  space  data  types  (e.g.,  image  and  speech  recognition)  [7,  8,  14]; recurrent neural networks (RNNs) for the time series data types (e.g., stock markets and language modeling) [12]; generative adversarial networks (GANs) for generating new examples and classifying examples [15]. Deep learning is an adequate and straightforward data-mining method for big data [12, 13]. Moreover, since deep learning techniques need big data to learn the classification rules, that is, they only work well for large datasets, they pose an enormous challenge to many applications with respect to obtaining large enough datasets [12, 13]. Furthermore, deep learning relies on good hardware, especially the graphics processing unit (GPU), to have better performance, but such hardware is still expensive [16, 17].\n\nThe SVM is another well-known and effective supervised learning model for selecting attributes and classifying  data.  Before  the  rise  of  deep  learning,  the  SVM outperformed  ANNs  in  various  real-life applications such as in the medicine [3, 4], semiconductor industry [18], on-line analysis [19], spectral unmixing resolution [20], imbalanced datasets [21], etc [22, 23, 24]. In comparison with deep learning techniques that try to connect data in terms of ANNs, the SVM separates (not to connect) different classes of data based on the kernels through mathematical optimization [25, 26]. In addition, an SVM has high accuracy with less computation power and small data, which are two shortcomings of deep learning [22, 23, 24]. Therefore, besides the original SVM, various enhanced SVMs have been developed before the development deep learning [21, 22, 23, 24]. SVMs are discussed in detail in Section 5.1.\n\nSmall  data  are  well-formatted  data  with  small  volumes  that  are  accessible,  understandable,  and actionable for decision makers [27]. The value of data lies in the information content, but not the volume of  data  [28].  For  some  cases  such  as  the  marketing  strategies  of  targeting  campaigns  or  delivering personalized experiences, big data might not be appropriate because they do not require full-on big data [29]. Conversely, small data extract an individual's data and provide valuable information to help decision makers formulate strategies. Moreover, the occurrence of small data is rare, with the process of collecting\n\nthem being expensive and strenuous [4, 21]. Hence, if the data mining of small data is improved, it will aid in making useful, cost-efficient, and timely decisions in small data applications.\n\nDeep learning techniques and SVMs belong to a broader family of machine learning algorithms. Deep learning techniques (e.g., convolution neural networks (CNNs)) based on neural networks are powerful for mining big data, but less effective in smaller datasets. On the contrary, SVMs outperform all neural network types in smaller datasets, but are less effective in mining big data. This paper proposes a novel convolutional SVM (CSVM) that has the advantages of both SVM and deep learning to enhance SVM by maximizing its prediction accuracy, and tests for classifying two-class datasets.\n\nThe  proposed  CSVM  employs  a  supervised  learning  technique  that  is  based  on  simplified  swarm optimization (SSO), which is another powerful machine learning algorithm [2, 6, 30, 31, 32, 33]. Numerical experiments  and  comparative  research  with  ANNs  and  the  traditional  SVM  show  the  accuracy  and effectiveness of the proposed CSVM tested on five two-class datasets.",
        "metadata": {
            "section_header": "INTRODUCTION",
            "title": "Convolutional Support Vector Machine",
            "type": "paper"
        }
    },
    {
        "page_content": "## 2. PROPOSED AND TRADITIONAL CONVOLUTION PRODUCTS\n\nThe major difference between the proposed CSVM and the traditional SVM is the convolution product. Hence, the traditional and the proposed new convolution products are introduced and discussed in Section 2.",
        "metadata": {
            "section_header": "PROPOSED AND TRADITIONAL CONVOLUTION PRODUCTS",
            "title": "Convolutional Support Vector Machine",
            "type": "paper"
        }
    },
    {
        "page_content": "## 2.1  Convolution-related concept\n\nCNNs represent some of the most significant models of deep learning, and their performance has been verified  in  numerous  recognition  research  areas.  Among  the  vital  operation  techniques  of  CNNs,  we introduce some that are used in this paper [12, 13].\n\n- 1. Padding: To prevent the reduction in data size generated by the convolution process in the next layer, we add zeros around the input image, with such action being called padding.\n- 2. Stride: A kernel that is moving a horizontal or vertical distance each time is called a stride. The greater the stride is, the more independent the neighboring values in the convolution process.\n- 3. Convolution: In each operation of convolution, multiplication of the values between the input and the\n\nkernel (filter) moves through based on the given stride after padding. Then, these products are summed up and filled in the corresponding positions on the next layer.",
        "metadata": {
            "section_header": "Convolution-related concept",
            "title": "Convolutional Support Vector Machine",
            "type": "paper"
        }
    },
    {
        "page_content": "## 2.2  Proposed convolution product with repeated attributes\n\nSuppose that Natt, Nsol, Nfilter, and Nvar are the numbers of attributions, solutions, filters constructed in each solution, and the variables contained in each filter, respectively. Let vr a , = vr a , ,0 be the value of the a th attribute in the r th record and vr a f , , be the value of the a th attribute in the r th record after using the  th filter, f where a = 1, 2, …, Natt, s = 1, 2, …, Nsol, and   = 1, 2, …, Nfilter. For example, nine attributes are used in the f breast cancer dataset of University of California Irvine (UCI) [34], and the vector representing the first normalized record is listed below:\n\n<!-- formula-not-decoded -->\n\nThe vr a f , , is calculated using the convolution product in terms of vr a f , , -1 , vr a , +1, -1 f , …, vr a , +Nvar-1, -1, f and the filter Xs f , as follows:\n\n<!-- formula-not-decoded -->\n\nFrom Eq. (2), there are Nvar attributes that are included in the a th attribute if l &lt; Natt .  However, no attributes vr ,Natt+1 , vr ,Natt+2 ,  …, vr ,Natt+Nvar-1 are  included  when  we  need  to  use  Eq.  (2)  to  update  the  last l attributes with   &lt; Nvar. l\n\nLet filter Xs ,1 = [ xs ,1,1 , xs ,1,2 , xs ,1,3 ] = [-1, 0, 1]. The procedures for generating the new attributes using the convolution product are listed as follows.\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nFrom the above, the first and second old attributes (i.e., v 1,1 and v 1,2 ) are used only once as shown in Eq. (3) and twice as shown in Eqs. (3) and (4) for generating v 1,1,1 and v 1,2,1 , respectively. Similarly, the last and the last second attributes in I 1 ,  (i.e., v 1,9 and v 1,8 )  are  only showed in Eq. (9) and Eqs. (8) and (9), respectively. Moreover, there are no new attributes v 1,8,1 and v 1,9,1 based on Eq. (2).\n\nThere is no padding in the proposed CSVM. However, we need to guarantee that the following two situations are satisfied to fix the above problems:\n\n- 1) each attribute is included in the same number (i.e., Nvar) of convolution products,\n- 2) the last   attributes still exist after each convolution product. j\n\nThe first (Nvar -1) attributes are repeated and appended in the last attribute of the same record such that the total number of attributes is an integer multiple of Nvar, that is, vr , Natt+ a f , = vr a f , , -1 for a = 1, 2, …, Nvar -1 and   = 1, 2, …, Nfilter. Hence, following the same example discussed above, we have f\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nThus,  each  new  attribute  is  generated  by  three  convolution  products,  and  we  have  the  new I 1 accordingly.\n\n<!-- formula-not-decoded -->\n\nLet I i j , be the updated  th record after using the  th filter, i j I i = I i ,0 . The next example demonstrates the updated I 1 after two filters are used, with each having Nvar = 3 variables; Xs ,2 = [ xs ,2,1 , xs ,2,2 , xs ,2,3 ] = [1.8, 0.9, 0.7].\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n```\nv 1,4,2 = v 1,4,1 × xs, 2,1 + 1,5,1 v × xs, 2,2 + 1,6,1 v × xs, 2,3 = 0.0 × (1.8) + 0.1 × (-0.9) + 0.0 × (0.7) = -0.09, (16) v 1,5,2 = v 1,5,1 × xs, 2,1 + 1,6,1 v × xs, 2,2 + 1,7,1 v × xs, 2,3 = 0.1 × (1.8) + 0.0 × (-0.9) -0.1 × (0.7) = 0.11, (17) v 1,6,2 = v 1,6,1 × xs, 2,1 + 1,7,1 v × xs, 2,2 + 1,8,1 v × xs, 2,3 = 0.0 × (1.8) -0.10 × (-0.9) + 0.4 × (0.7) = 0.37, (18) v 1,7,2 = v 1,7,1 × xs ,2,1 + 1,8,1 v × xs, 2,2 + 1,9,1 v × xs, 2,3 = -0.1 × (1.8) + 0.4 × (-0.9) + 0.0 × (0.7) = -0.54, (19) v 1,8,2 = v 1,8,1 × xs, 2,1 + 1,9,1 v × xs, 2,2 + 1,1,1 v × xs, 2,3 = 0.4 × (1.8) + 0.0 × (-0.9) -0.4 × (0.7) = 0.44, (20) v 1,9,2 = v 1,9,1 × xs, 2,1 + 1,1,1 v × xs, 2,2 + 2,1,1 v × xs, 2,3 = 0.0 × (1.8) -0.4 × (-0.9) + 0.0 × (0.7) = 0.36. (21)\n```\n\nThus, after using the two filters Xs ,1 and Xs ,2 , we have\n\n<!-- formula-not-decoded -->\n\nThe basic idea of the proposed convolution product with repeated attributes is that the first (Nvar -1) attributes are repeated and appended in the last attribute of each (updated) record such that the total number of attributes is an integer multiple of Nvar, that is, vr , Natt+ a f , = vr a f , , -1 for a = 1, 2, …, Nvar -1. The pseudo code of the proposed convolution product with repeated attributes is listed as follows:\n\nInput: The  th record r I r = [ vr ,1 , vr ,2 , …, vr ,Natt ] and the s th solution Xs .\n\nOutput: The I r ⊗ Xs .\n\nSTEP C0. Let   = 1 and f vr i , ,0 = vr i , for   =1, 2, …, Natt. i\n\nSTEP C1. Let a = 1, vr i f , , -1 = vr k f , , -1 for   = Natt i + 1, 2, …, Natt + Nvar -1, k = i -Natt .\n\nSTEP C2. Let b = 0,   = i a , and   = 1. j\n\nSTEP C3. Let b = b + vr i f , , -1 × xs f j , , .\n\nSTEP C4. If   &lt; Nvar, let   =   + 1,   =   + 1, and go to STEP C3. j i i j j\n\nSTEP C5. If a &lt; Natt, let vr a f , , = b , a = a + 1, and go to STEP C2.\n\nSTEP C6. If   &lt; Nfilter f , let   =   + 1 and go to STEP C1. f f\n\nAdditionally, we obtain the following properties after employing the proposed convolution product with repeated attributes:\n\n<!-- formula-not-decoded -->",
        "metadata": {
            "section_header": "Proposed convolution product with repeated attributes",
            "title": "Convolutional Support Vector Machine",
            "type": "paper"
        }
    },
    {
        "page_content": "## 3. PROPOSED AND TRADITIONAL SSO\n\nIn the proposed CSVM, all values in filters of the proposed convolution product with repeated attributes are updated based on the proposed new SSO. The traditional SSO is introduced briefly, and the proposed SSO including the new self-adaptive solution structure with pFilter , the novel one-solution one-filter onevariable greedy update mechanism, and the fitness function are presented in Section 3.",
        "metadata": {
            "section_header": "PROPOSED AND TRADITIONAL SSO",
            "title": "Convolutional Support Vector Machine",
            "type": "paper"
        }
    },
    {
        "page_content": "## 3.1  Traditional SSO\n\nThe SSO is one of the simplest machine-learning methods [2, 6, 30, 31, 32, 33] in terms of its update mechanism. It was first proposed by Yeh, and has been tested to be a very useful and efficient algorithm for optimization problems [31, 33], including data mining [2, 6]. Owing to its simplicity and efficiency, SSO is used here to find the best values in filters of the proposed CSVM.\n\nThe basic idea of SSO is that each variable, such as the  th variable in the  th solution j i xi j , , needs to be updated based on the following stepwise function [2, 6, 30, 31, 32, 33]:\n\n<!-- formula-not-decoded -->\n\nwhere the value ρ [0,1] ∈ [0, 1] is generated randomly, the parameters Cg , Cp -Cg , Cw -Cp , 1 Cw are all in [0, 1] and are the probabilities of the current variable that are copied and pasted from the best of all solutions, the best  th solution, the current solution, and a random generated feasible value, respectively. i\n\nThere are different variants of the traditional SSO that are customized to different problems from the no free lunch theorem; for example, the four items in Eq. (24) are also reduced to three items to increase the efficiency; parameters Cg , Cp , and Cw are all self-adapted; special values or equations are implemented\n\nto replace gj , pi j , , xi j , , and x ; or only a certain number of variables is selected to be updated, etc. However, the SSO update mechanism is always based on the stepwise function.",
        "metadata": {
            "section_header": "Traditional SSO",
            "title": "Convolutional Support Vector Machine",
            "type": "paper"
        }
    },
    {
        "page_content": "## 3.2  Fitness function\n\nFitness functions help solutions learn toward optimization to attain goals in artificial intelligence, such as the proposed CSVM, the traditional SVM, and the CNN. The accuracy obtained by the SVM, based on the records transferred from the proposed convolutions, is adopted here to represent the fitness to maximize in the CSVM:\n\n<!-- formula-not-decoded -->\n\nInput: All records I r = [ vr ,1 , vr ,2 , …, vr ,Natt ] and the s th solution Xs for r =1, 2, …, Nrec.\n\nOutput: The F Xs ( ).\n\nSTEP F0. Calculate I r * = I r ⊗ Xs based on the pseudo code provided in Section 2.2 for   =1, 2, …, Nrec. r STEP F1. Classifier { I * 1 , I * 2 , …, I * Nrec } using the SVM and let the accuracy be F Xs ( ).",
        "metadata": {
            "section_header": "Fitness function",
            "title": "Convolutional Support Vector Machine",
            "type": "paper"
        }
    },
    {
        "page_content": "## 3.3  Self-adaptive solution structure and pFilter\n\nIn the proposed CSVM, each variable of all filters in each solution is initialized randomly from [-2, 2]. Each filter and solution is presented by Nvar × 1 and Nfilter × Nvar, respectively, since the number of filters may be more than one. For example, the  th solution s Xs and the  th filter f Xs f , in Xs are denoted as follows:\n\n<!-- formula-not-decoded -->\n\nwhere\n\n<!-- formula-not-decoded -->\n\nHowever, overall, the number of filters is equal, that is, Nfilter for each solution and all generations. However, a greater number of filters does not always guarantee a better fitness value. Hence, we need to record the best number of filters for each solution. Let filter   be the best filter of solution j s = 1, 2, …, Nsol, and define pFilter s [ ] =   if j F Xs f [ , ] ≤ F Xs j [ , ] for all k =1, 2, …, Nfilter. Note that Xh i , is the best solution for pFilter h [ ]=  among all existing solutions if i F Xs f [ , ] ≤ F Xh i [ , ] for all s = 1, 2, …, Nsol and   =1, 2, …, Nfilter. f\n\nIn the end, only the best solution (e.g., Xs ) and its best number of filters, namely, Xs ,1 , Xs ,2 ,…, Xs j , where pFilter s [ ] =  , are reported. In addition, the update mechanism is based on the best filter in the proposed j CSVM. Hence, the solution is self-adapted by the best number of filters.",
        "metadata": {
            "section_header": "Self-adaptive solution structure and pFilter",
            "title": "Convolutional Support Vector Machine",
            "type": "paper"
        }
    },
    {
        "page_content": "## 3.4  One-solution one-filter one-variable greedy SSO update mechanism\n\nThe proposed new one-solution one-filter one-variable greedy SSO update mechanism is discussed in this subsection.",
        "metadata": {
            "section_header": "One-solution one-filter one-variable greedy SSO update mechanism",
            "title": "Convolutional Support Vector Machine",
            "type": "paper"
        }
    },
    {
        "page_content": "## 3.4.1 OnepFilter is selected randomly to be updated in each generation\n\nIn the  proposed  CSVM,  all  values  in  filters  are  variables  that  must  be  determined  to  implement convolution products. Without the help from the GPU, it takes a long time to update variables to deepen the SVM. Hence, instead of the traditional algorithms, including SSO, the genetic algorithm (GA), particle swarm  optimization  (PSO),  of  which  all  solutions  need  to  be  updated,  only  one  solution  is  selected randomly for updating in each generation of the proposed new SSO update mechanism. Let solution   be s selected to be updated based on the following equations:\n\n<!-- formula-not-decoded -->\n\nwhere ρ [0,1] is a random floating-point number generated from interval [0, 1] and ρ [1,Nsol ] ∈ {1, 2, …, Nsol} is the index of the solution selected randomly, gBest is the index of the best solution found, and the 0 is a\n\nnew solution generated randomly. The new updated solution X s will be either discarded or replaced with the old Xs based on the process described next.",
        "metadata": {
            "section_header": "OnepFilter is selected randomly to be updated in each generation",
            "title": "Convolutional Support Vector Machine",
            "type": "paper"
        }
    },
    {
        "page_content": "## 3.4.2 One-filter one-variable greedy update mechanism\n\nAll variables need to be updated, namely, the all-variable update mechanism, in the traditional SSO, and it has a higher probability of escaping the local trap compared to the updates with only some variables. However, the all-variable update mechanism may cause solutions that are near optimums to be kept away from their current positions. Additionally, its runtime is Nsol times that of the one-variable update, which selects one variable randomly to be updated. Hence, to reduce the runtime, only one variable in one filter in the solution selected in Section 3.4.1 is updated.\n\nLet s be the solution selected to be updated. In the proposed new SSO, only one filter; for example, f where   = 1, 2, …, f pFilter f [ ] =  , in solution j s is chosen randomly. Moreover, one variable; for example, xs f k , , where   = 1, 2, …, Nvar, in such filter k Xs f , is also selected randomly to be updated based on the following simple process:\n\n<!-- formula-not-decoded -->\n\nAfter resetting all variables in these filters Xs h , to a random number generated from [-2, 2] for all h &gt; , f we have\n\n<!-- formula-not-decoded -->\n\nAlso, F Xs l [ , ] = F Xs f [ , -1 ] for all for all  &lt; . l f\n\nMoreover, the updated solutions Xs * , including these new updated variables and filters, are all discarded, if their fitness value are not better than that of Xs , i.e.,\n\n<!-- formula-not-decoded -->",
        "metadata": {
            "section_header": "One-filter one-variable greedy update mechanism",
            "title": "Convolutional Support Vector Machine",
            "type": "paper"
        }
    },
    {
        "page_content": "## 3.5  Pseudocode of the proposed SSO\n\nThe pseudocode of the proposed SSO based on the new self-adaptive solution structure, pFilter , and the new update mechanism are listed as follows:\n\nInput: A random selected solution (e.g., Xs ) with its pFilter .\n\nOutput:\n\nThe updated\n\nXs\n\n.\n\nSTEP U0. Generate a random number ρ [0, 1] from [0, 1] and select a solution, say Xs where s ∈ {1,\n\n2, …, Nsol} based on Eq.(28).\n\nSTEP U1. Select a filter, say Xs j , where j ∈ {1, 2, …, pFilter s [ ]}.\n\nSTEP U2. Update Xs to X * based on Eq. (30).\n\nSTEP U3. Based on Eq. (31) to decide to let Xs = X * or discard X * .\n\nSTEP U4. If Xs = X * , let pFilter s [ ] =  , where f F Xi ( * ) ≤ F Xf ( * ) for all   = 1, 2, …, Nfilter i . Otherwise, halt.\n\nSTEP U5. If F XgBest pFilter gBest ( , [ ] ) ≤ F Xs pFilter s ( , [ ] ), let gBest = s .",
        "metadata": {
            "section_header": "Pseudocode of the proposed SSO",
            "title": "Convolutional Support Vector Machine",
            "type": "paper"
        }
    },
    {
        "page_content": "## 4. PROPOSED SMALL-SAMPLE OA TO TUNE PARAMETERS\n\nIt is important to select the most representative combination of parameters to find good results for all algorithms, such as the three parameters Cg , Cp , and Cw in SSO. To reduce the computation burden, a novel concept called small-sample orthogonal array (OA) is proposed in terms of OA test to tune parameters in Section 4.",
        "metadata": {
            "section_header": "PROPOSED SMALL-SAMPLE OA TO TUNE PARAMETERS",
            "title": "Convolutional Support Vector Machine",
            "type": "paper"
        }
    },
    {
        "page_content": "## 4.1  OA\n\nThe design of experiment (DOE) adopts an array design that arranges the tests and factors in rows and columns, respectively, such that rows and columns are independent of each other, and there is only one test level  in  each  factor  level  [35].  The  DOE  is  able  to  select  better  parameters  from  some  representative predefined combinations to reduce test numbers [2, 36].\n\nThe Taguchi OA test, first developed by Taguchi [35], is a DOE that is implemented to achieve the objective of this study. OA is denoted by L ( n a b ), where ( log ( 1) ) a b n a +     = , a , and b are the numbers of tries, levels of each factor, and factors, respectively. For example, Table 1 represents an OA denoted by L9(3 ). 4\n\nTable 1. An experiment of four factors with three levels using an OA\n\n|   Try ID |   Factor 1 |   Factor 2 |   Factor 3 |   Factor 4 |\n|----------|------------|------------|------------|------------|\n|        1 |          1 |          1 |          1 |          1 |\n|        2 |          1 |          2 |          2 |          2 |\n|        3 |          1 |          3 |          3 |          3 |\n|        4 |          2 |          1 |          2 |          3 |\n|        5 |          2 |          2 |          3 |          1 |\n|        6 |          2 |          3 |          1 |          2 |\n|        7 |          3 |          1 |          3 |          2 |\n|        8 |          3 |          2 |          1 |          3 |\n|        9 |          3 |          3 |          2 |          1 |\n\nFrom Table 1, we can see that the characteristics of the OA are orthogonal as follows.\n\n- 1. The number of different levels in each column is equal; for example, numbers, 1, 2, and 3 appear three times in each column in Table 1.\n- 2. All ordered pairs of the two factors for the same test also appear exactly once, e.g., (1, 1), (1, 2), (1, 3), (2, 1), (2, 2), (2, 3), (3, 1), (3, 2), and (3,3) in Columns 1 and 2, 1 and 3, 1 and 4, 2 and 3, 2 and 4, and 3 and 4, to ensure that each level is dispersed evenly in the complete combination of each level of factors.",
        "metadata": {
            "section_header": "OA",
            "title": "Convolutional Support Vector Machine",
            "type": "paper"
        }
    },
    {
        "page_content": "## 4.2  Proposed small-sample OA\n\nThere are three major methods for tuning parameters:\n\n- 1. The try-and-error method: It implements the tests exhaustively by trying all possible cases to find the one with the better results. It is the simplest, but also the most inefficient one.\n- 2. The parameter-adapted method: It selects and tests some set of parameters from the existing parameters, which are already used in some applications. This method may have some issues with respect to identifying the characteristic of new problems.\n\n- 3. The DOE: It selects the parameters from the experiment design. Compared to the two aforementioned methods, this method is the most efficient and effective one. However, this method faces an efficiency problem in large datasets or needs to be repeated very often.\n\nHence, to overcome these aforementioned problems, a novel method called the small-sample OA test is proposed to improve the OA method for tuning parameters. To reduce the runtime, the proposed smallsample OA test only samples few data randomly from the dataset and conducts the OA test on the subsets of such small-sample data to find the best parameters that result in the highest accuracy, the shortest runtime, and/or the largest number of solutions with the maximal number of obtained highest accuracy based on the following three rules:\n\n- Rule 1. The one with the highest accuracy among all others;\n- Rule 2. The one with the shortest runtime, with a big gap between such runtime and others if there is a tie based on Rule 1;\n- Rule 3. The one with the largest number of solutions that have the highest accuracy if there is a tie based on Rule 2.\n\nThen, this selected parameter set is applied to the rest of the unsampled dataset. The example for this proposed test is provided in Section 6.",
        "metadata": {
            "section_header": "Proposed small-sample OA",
            "title": "Convolutional Support Vector Machine",
            "type": "paper"
        }
    },
    {
        "page_content": "## 5. PROPOSED CSVM and TRADITIONAL SVM\n\nThe proposed CSVM is a convolutional SVM modified by employing a new convolution product, which is updated based on the proposed new SSO. The traditional SVM is introduced briefly, and then the proposed pseudocode of the proposed CSVM is presented.",
        "metadata": {
            "section_header": "PROPOSED CSVM and TRADITIONAL SVM",
            "title": "Convolutional Support Vector Machine",
            "type": "paper"
        }
    },
    {
        "page_content": "## 5.1  Traditional SVM\n\nSVMs are excellent machine learning tools for binary classification cases [25, 26]. The purpose of an SVM is to maximize the margin between two support hyperplanes to separate two classes of data. Let X = { z 1 = ( x 1 , y 1 ), z 2 = ( x 2 , y 2 ), …, zn = ( xn , yn )} be a two-class dataset for training. For example, in a linear SVM, a hyperplane is a line, and we want to find the best hyperplane W T X + b = 0 to separate these two classes of data in X , where W is the weight vector and b is the bias perpendicular to such hyperplane such that || W || is as large as possible. The above linear SVM is a constrained optimization model and can be written as follows [25, 26]:\n\n<!-- formula-not-decoded -->\n\nAfter  applying  the  Lagrange  multiplier  method  to  the  constrained  optimization  model,  the  SVM problem is a convex quadratic programming problem that can be presented as follows [25]:\n\n<!-- formula-not-decoded -->\n\nwhere λ i is the Lagrange multiplier.\n\nFor these high-dimensional data, it is very difficult to find a single linear line to separate two different sets. Hence, these data are mapped into a higher dimensional space using a function that is called the kernel in SVM. Then, a hyperplane can be found to separate the mapped data. Here, we list some popular kernel functions [25, 26]:\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nFor more details of SVM and its development, refer to [25, 26].",
        "metadata": {
            "section_header": "Traditional SVM",
            "title": "Convolutional Support Vector Machine",
            "type": "paper"
        }
    },
    {
        "page_content": "## 5.2  Pseudocode of the proposed CSVM\n\nThe  pseudocode  of  the  proposed  CSVM  is  described  below  together  with  the  integration  of  the proposed convolution product discussed in Subsection 2.2, the proposed SSO introduced in Section 3, and the proposed small-sample OA presented in Subsection 4.2.",
        "metadata": {
            "section_header": "Pseudocode of the proposed CSVM",
            "title": "Convolutional Support Vector Machine",
            "type": "paper"
        }
    },
    {
        "page_content": "## PROCEDURE CSVM0\n\n- Input: A dataset.\n- Output: The accuracy of the classifier CSVM.\n- STEP 0. Separate the dataset into k folds randomly, and then select one fold (e.g., the k * th fold of the dataset); for the small-sample OA, it has N tries.\n- STEP 1. Implement CSVM0 ( , i k * ) using the  th parameter setting on the i k * th fold of the dataset for   = i 1, 2, …, N , and then let the parameter setting of the try (e.g., i * ) with the highest accuracy among all N tries.\n- STEP 2. Implement CSVM0 ( i * , j ) on the  th fold of the dataset using the parameter setting of the j i * th try for   = 1, 2, …, j k .",
        "metadata": {
            "section_header": "PROCEDURE CSVM0",
            "title": "Convolutional Support Vector Machine",
            "type": "paper"
        }
    },
    {
        "page_content": "## PROCEDURE CSVM0 ( α β , )\n\nInput: The parameter setting in the α th try of the small-sample OA and the β th fold of the dataset.\n\nOutput:\n\nThe accuracy.\n\nSTEP W0. Generate solutions Xs randomly, then calculate F Xs f ( , ) based on the proposed convolution product and the SVM. Find pFilter s [ ] and gBest such that F XgBest pFilter gBest ( , [ ] ) ≥ F Xs f ( , ), where s = 1, 2, …, Nsol and   =1, 2, …, Nfilter. f\n\nSTEP W1. Let   = 1. t\n\nSTEP W2. Update a randomly selected solution based on the pseudocode of the new SSO provided in Subsection 3.5 and the parameter setting in the α th try of OA.\n\nSTEP W3. Increase the value of   by 1, that is, let   =   + 1, and then go to STEP W2 if   &lt;Ngen. t t t t\n\nSTEP W4. Halt, F XgBest pFilter gBest ( , [ ] ) is the accuracy, and XgBest pFilter gBest , [ ] is the classifier.",
        "metadata": {
            "section_header": "PROCEDURE CSVM0 ( α β , )",
            "title": "Convolutional Support Vector Machine",
            "type": "paper"
        }
    },
    {
        "page_content": "## 6. EXPERIMENTAL RESULTS AND SUMMARY\n\nThere are two experiments: Ex1 and Ex2 in this study. Ex1 is based on the proposed small-sample OA concept  to  find  the  parameters Cg , Cp , Cw ,  Ngen ,  Nfilter ,  and  Nvar  in  the  proposed  CSVM.  Then,  these parameters are employed in Ex2 to conduct an extension test to compare these results with those obtained from the DSCM, SVM, 3-layer ANN, and 4-layer ANN, respectively.",
        "metadata": {
            "section_header": "EXPERIMENTAL RESULTS AND SUMMARY",
            "title": "Convolutional Support Vector Machine",
            "type": "paper"
        }
    },
    {
        "page_content": "## 6.1  Simulation environment\n\nFour algorithms are developed and adapted in this study including the proposed CSVM, SVM, the 3layer  ANN, and the 4-layer ANN. The proposed CSVM is implemented using Dev C++ Version 5.11 C/C++, and the SVM part is integrated by calling the libsvm library [26] with all default setting parameters. The codes of both the 3-layer and 4-layer ANNs are modified using the source code provided in [37], which is coded in Python and run in Anaconda with epochs = 150, batch\\_size = 10, loss = 'binary\\_crossentropy', optimizer = 'adam', activation = 'relu' and 12 neurons in the first hidden layer, and activation = 'sigmoid' in the second hidden layer of the 4-layer ANN. The test environment is: Intel(R) Core(TM) i9-9900K CPU @ 3.60 GHz, 32.0 GB memory and Windows 10 64 bits.\n\nTo validate the proposed CSVM, the proposed CSVM was compared with the traditional SVM and the 3-layer and 4-layer ANNs on five well-known datasets: 'Australian Credit Approval' (A), 'breast-cancer' (B), 'diabetes' (D), 'fourclass' (F), and 'Heart Disease' (H) [34] based on a tenfold cross-validation in Ex2. Summary of the five datasets is provided in Table 2.\n\nTable 2. Information and characteristics of the five datasets.\n\n| ID   | Full Name                  |   Record Number |   Attribute Number | Attribute Characteristics   |\n|------|----------------------------|-----------------|--------------------|-----------------------------|\n| A    | Australian Credit Approval |             690 |                 14 | Integer, Real               |\n| B    | Breast-cancer              |             699 |                 10 | Integer, Real               |\n| D    | Diabetes                   |             768 |                  8 | Integer, Real               |\n| F    | Fourclass                  |             862 |                  2 | Integer, Real               |\n| H    | Heart Disease              |             270 |                 13 | Integer, Real               |\n\nLet Φ ,  T,  G,  f,  and  N  be  the  highest  accuracy  levels  obtained  in  the  end,  the  runtime,  the  earliest generation  that  obtained Φ ,  the  number  of  filters  generating Φ ,  the  number  of  solutions  that  has Φ , respectively. To be easily recognized, the subscripts 25, 50, 75, 100, avg, max, min, and std represent the related values obtained at the end of the 25 th , 50 th , 75 th , and 100 th generations, the average, the maximum, minimum, and the standard deviation, respectively.",
        "metadata": {
            "section_header": "Simulation environment",
            "title": "Convolutional Support Vector Machine",
            "type": "paper"
        }
    },
    {
        "page_content": "## 6.2  Ex1: SMALL-SAMPLE OA test\n\nThe orthogonal array used in this study is called L (3 ) as shown in Table 3. In L9(3 ), there are nine 9 4 4 tries and four factors: C =( cg , cp , cw , cr ), Nsol , Nvar, and Nfilter ; each factor has three levels as shown in Table 4. The higher the level, the larger related values with the exception of C , e.g., in level 1, Nsol =25 is smaller than that in level 2. The most distinguishable difference amongst all three levels in C of Table 3 is that level 2 has a higher cr which is to increase the global search ability while level 3 has the lower value of cr to enhance the local search ability.\n\nTable 3. The L (3 ). 9 4\n\n|   Try |   N filter |   N sol |   N var |   ( c g , c p , c w , c r ) |\n|-------|------------|---------|---------|-----------------------------|\n|     1 |          1 |       1 |       1 |                           1 |\n|     2 |          1 |       2 |       2 |                           2 |\n|     3 |          1 |       3 |       3 |                           3 |\n|     4 |          2 |       3 |       2 |                           1 |\n|     5 |          2 |       3 |       3 |                           2 |\n|     6 |          2 |       1 |       1 |                           3 |\n|     7 |          3 |       2 |       3 |                           1 |\n|     8 |          3 |       3 |       1 |                           2 |\n|     9 |          3 |       1 |       2 |                           3 |\n\nTable 4. Levels of all-factor test for selecting the parameters.\n\n|   Level Code |   N filter |   N sol | N var              | C = ( c g , c p , c w , c r )   |\n|--------------|------------|---------|--------------------|---------------------------------|\n|            1 |          1 |      25 | att N / 4      | (0.40, 0.30, 0.20, 0.10)        |\n|            2 |          3 |      50 | att 2N / 4     | (0.35, 0.25, 0.15, 0.25)        |\n|            3 |          4 |      75 | att 3N / 4     | (0.45, 0.30, 0.20, 0.05)        |\n\nThe results obtained from the proposed CSVM in terms of the proposed small-sample OA test are listed in Table 5, in which each try is run fifteen times, with the larger the Nfilter, Nsol, Nvar, and/or Ngen, the longer the runtime. However, it is not necessary to have better fitness values from Table 5. For example, the best\n\nfitness value has already been found in G25, namely, F25 = F50 = F75, in all datasets except dataset D whose best fitness value is found in G75.\n\nTable 5. Levels of all-factor test for selecting the parameters.\n\n1 . Select the setting based on Rule 1.\n\n| ID   | Try     | T 25        | G 25 f 25 N   | G 25 f 25 N   | 25               |   T 50 |   G 50 |   f 50 N 50 |                      | 100F 50           | T 75 G 75       | f 75 N 75   |   100F 75 |\n|------|---------|-------------|---------------|---------------|------------------|--------|--------|-------------|----------------------|-------------------|-----------------|-------------|-----------|\n| A 2  | 1       | 7           |               | 1 12          | 100F 25 85.00000 |  26.11 |     19 |           1 | 1                    | 86.66666          | 39.07 19        | 1 1         |   86.6667 |\n|      | 2       | 13.09 27.68 | 9             | 1 1           | 88.33334         |  55.33 |     13 |           1 | 1                    | 88.33334          | 82.59 28        | 1 2         |   88.3333 |\n|      | 3       | 42.81       | 4             | 4             | 88.33334         |  86.04 |     11 |           1 | 5                    | 88.33334          | 128.39          | 1 6         |   88.3333 |\n|      | 4       | 130.54      | 6             | 1 3 13        | 88.33334         | 260.18 |     11 |           3 | 15 88.33334          | 389.33            | 26 11           | 3 15        |   88.3333 |\n|      | 5       | 43.44       | 6             | 3 1           | 90.00000         |  87.39 |     14 |           3 | 1 90.00000           | 131.20            | 14              | 3 1         |   90      |\n|      | 6       | 84.35       | 7             | 3 15          | 86.66666         | 168.38 |     13 |           3 | 2                    | 88.33334 252.73   | 27              | 3           |   88.3333 |\n|      | 7       | 149.28      | 8             | 5 4           | 90.00000         | 298.47 |     15 |           5 | 6 90.00000           | 447.50            | 24              | 3 5 7       |   90      |\n|      | 8       | 215.83      | 9             | 5 14          | 88.33334         | 427.65 |      9 |           5 | 14 88.33334          |                   | 637.65          | 1           |   88.3333 |\n|      | 9       | 113.32      | 9             | 5 8           | 88.33334         | 225.84 |     18 |           5 | 11                   | 88.33334          | 21 337.70 25    | 5 5 15      |   88.3333 |\n| B 2  | 1       | 16.03       | 6             | 1 13          | 97.05882         |  31.99 |      6 |           1 | 13                   | 97.05882          | 47.83           | 1 14        |   97.0588 |\n|      | 2       | 33.41       | 5             | 1 15          | 97.05882         |  67.26 |      5 |           1 | 15                   | 101.45            | 26 5            | 1 15        |   97.0588 |\n|      | 3       | 51.63       | 6             | 1 15          | 97.05882         | 103.01 |      6 |           1 | 97.05882 15 97.05882 | 155.17            | 6               | 1 15        |   97.0588 |\n|      | 4       | 160.82      | 0             | 3 15          | 97.05882         | 324.22 |      0 |           3 | 15 97.05882          | 485.32            | 0               | 3 15        |   97.0588 |\n|      | 5       | 53.08       | 2             | 3 15 3        | 97.05882         | 106.13 |      2 |           3 | 15                   | 159.76            | 2               | 3 15        |   97.0588 |\n|      | 6       | 103.51      | 0             | 15            | 97.05882         | 209.98 |      0 |           3 | 97.05882 15 97.05882 | 316.64            | 0               | 3 15        |   97.0588 |\n|      | 7       | 185.49      | 0             | 5 15          | 97.05882         | 372.27 |     17 |           5 | 1 98.52941           | 558.38            | 17              | 5 1         |   98.5294 |\n|      | 8       | 271.51      | 9             | 5 1           | 98.52941         | 542.93 |      9 |           5 | 1                    | 98.52941 815.96   | 9               | 5 1         |   98.5294 |\n|      | 9       | 91.88       | 5             | 5 1           | 98.52941         | 184.5  |     17 |           5 | 2                    | 98.52941          | 276.67          | 5 2         |   98.5294 |\n| D 1  | 22.70   |             | 0             | 1 15          | 78.08219         |  45.41 |      0 |           1 | 15                   | 78.08219          | 17 0            | 1 15        |   78.0822 |\n|      | 1 2     | 45.36       | 8             | 1 8           | 80.82191         |  90.93 |     19 |           1 | 11                   | 80.82191          | 68.09 136.65 21 | 1 12        |   80.8219 |\n|      | 3       | 68.45       | 4             | 1 2           | 82.19178         | 137.29 |     13 |           1 | 2                    | 82.19178          | 206.65          | 1 4         |   82.1918 |\n|      | 4       | 207.15      | 8             | 3 2           | 82.19178         | 416.77 |     19 |           3 | 5                    | 82.19178          | 27 624.94 29    | 3 6         |   82.1918 |\n|      | 5       | 68.42       | 9             | 3 3           | 82.19178         | 137.33 |     17 |           3 | 5                    | 82.19178          | 206.86          | 3 6         |   82.1918 |\n|      | 6       | 136.61      | 9             | 3 7           | 80.82191         | 274.81 |     15 |           3 | 10                   | 412.70            | 29 23           | 3 12        |   80.8219 |\n|      | 7       | 232.55      | 6             | 6             | 82.19178         | 464.95 |     18 |           4 | 10                   | 80.82191 82.19178 | 696.72          | 5 1         |   83.5617 |\n|      | 8       | 345.07      | 4             | 5 5 15        | 80.82191         | 691.15 |      4 |           5 | 15                   | 80.821911037.06   | 25 4            | 5 15        |   80.8219 |\n|      | 9       | 114.58      | 9             | 5 1           | 82.19178         | 230.88 |     18 |           5 | 2                    | 82.19178          | 347.32 29       | 5 5         |   82.1918 |\n| F 3  | 1       | 19.82       | 0             | 1 15          | 80.23256         |  39.71 |      0 |           1 | 15                   | 80.23256          | 59.61           | 1 15        |   80.2326 |\n|      | 2       | 39.97       | 0             | 1 15          | 80.23256         |  80.54 |      0 |           1 | 15                   | 121.10            | 0 0             | 1 15        |   80.2326 |\n|      | 3       | 61.63       | 9             | 1 12          | 83.72093         | 123.97 |     19 |           1 | 14                   | 80.23256 83.72093 | 186.52 22       | 1 15        |   83.7209 |\n|      | 4       | 181.25      | 0             | 1 15          | 80.23256         | 364.8  |      0 |           1 | 15                   | 80.23256          | 548.34          | 1 15        |   80.2326 |\n|      | 5       | 62.05       | 6             | 3 15          | 83.72093         | 124.67 |      6 |           3 | 15                   | 83.72093          | 0 187.85        | 3 15        |   83.7209 |\n|      | 6       | 120.07      | 0             | 1 15          | 80.23256         | 242.21 |      0 |           1 | 15                   | 364.72            | 6 0             | 1 15        |   80.2326 |\n|      | 7       | 211.59      | 2             | 5 15          | 83.72093         | 424.27 |      2 |           5 | 80.23256 15 83.72093 | 637.24            | 2               | 5 15        |   83.7209 |\n|      | 8       | 303.38      | 0             | 1 15          | 80.23256         | 610.26 |      0 |           1 | 15                   | 918.33            | 0               | 1 15        |   80.2326 |\n|      | 9       | 100.15      | 0             | 15            | 80.23256         | 202.2  |      0 |           1 | 15                   | 80.23256 80.23256 | 304.68 0        | 1 15        |   80.2326 |\n| H 2  | 1 12.58 |             | 8             | 1 10          | 88.00000         |  26.11 |     19 |           1 | 11                   |                   | 39.67 24        | 1 13        |   88      |\n|      | 2       | 26.09       | 9             | 1 1 10        | 88.00000         |  54.12 |     19 |           1 | 14                   | 88.00000 88.00000 | 81.41 26        | 1 15        |   88      |\n|      | 3       | 40.98       | 9             | 1 5           | 88.00000         |  82.38 |     16 |           1 | 7                    | 124.00            | 28              | 1 9         |   88      |\n|      | 4       | 122.50      | 6             | 3 14          | 88.00000         | 244.67 |      6 |           3 | 14                   | 88.00000 88.00000 | 366.86 6        | 3 14        |   88      |\n|      | 5       | 39.94       | 8             | 3 10          | 88.00000         |  80.13 |      8 |           3 | 10 88.00000          | 122.23            | 24              | 3 12        |   88      |\n|      | 6       | 79.05       | 5             | 3 12          | 88.00000         | 160.39 |     16 |           3 | 15                   | 88.00000          | 241.89 16       | 3 15        |   88      |\n|      | 7       | 136.33      | 8             | 5 12          | 88.00000         | 273.14 |     15 |           5 | 13                   | 88.00000          | 409.73 21       | 5 14        |   88      |\n|      | 8       | 203.64      | 7             | 5 15          | 88.00000         | 407.86 |      7 |           5 | 15                   | 88.00000          | 612.67 7        | 5 15        |   88      |\n|      | 9       | 68.08       | 9             | 5 8           | 88.00000         | 135.69 |     17 |           5 | 9                    | 88.00000          | 203.97 25       | 5 12        |   88      |\n\n2 . Select the setting based on Rule 2.\n\n3 . Select the setting based on Rule 3.\n\nAdhering to Rule 1 listed in Section 4, only the try with the highest accuracy Φ is selected to be used for the rest of the unsampled dataset. In this case, Try 7 is selected for Dataset D, since the greatest accuracy is obtained from Try 7 in G75. From Rule 2, the runtime T must be considered if there are two tries tied in accuracy. For example, both Try 5 and Try 9 have the highest accuracy in Dataset A, but Try 5 is selected, since its runtime is only 43.43, which is considerably less than the runtime (149.28) of Try 9. Similarly, Try 7 and Try 1 are selected for Datasets B and H, respectively. The parameter setting for the rest of datasets, namely, Dataset F, is based on Rule 3, and Try 5 is selected in accordance with Rule 2.\n\nHence, we obtain the parameter settings listed in Table 6.\n\nTable 6. Parameters used in the proposed CSVM.\n\n| ID   |   Try ID N rec |     |   N att N filter N |    |   sol | N var                              | ( c g , c p , c w , c r ) N gen   | 100F SVM     | 100F Ngen      | T     |\n|------|----------------|-----|--------------------|----|-------|------------------------------------|-----------------------------------|--------------|----------------|-------|\n| A    |              5 | 690 |                 14 |  3 |    75 | att 3N / 4     =11 (0.35,      | 0.25, 0.15, 0.25)                 | 25 81.666664 | 90.00000       | 43.44 |\n| B    |              9 | 699 |                 10 |  4 |    25 | att 2N / 4     =5 (0.45, 0.30, | 0.20, 0.05)                       | 25 95.588234 | 98.52941       | 91.88 |\n| D    |              7 | 768 |                  8 |  4 |    50 | att 3N / 4     =6 (0.40, 0.30, | 0.20, 0.10)                       | 75 76.712326 | 83.56165696.72 |       |\n| F    |              5 | 862 |                  2 |  3 |    75 | att 3N / 4     =2 (0.35, 0.25, | 0.15, 0.25)                       | 25 80.232559 | 83.72093       | 62.05 |\n| H    |              1 | 270 |                 13 |  1 |    25 | att N / 4     =4 (0.40,        | 0.30, 0.20, 0.10)                 | 25 80.000000 | 88.00000       | 12.58 |\n\nIn dataset F, there is only two attributes resulting in also two variables in each filter of Table 6. Another observation is that the values Nfilter, Nvar, Nsol, and Ngen are always the smallest, since all the best final fitness values are equal to 88.00000 regardless of the generation number. Then, the parameter setting with less runtime is selected, which is reasonable. This is similar to dataset B whose solution number is only 25, with less local search ability.\n\nIn Table 5, the accuracy levels obtained from SVM for the first fold of each dataset are listed in the last  second  column  named  100FSVM.  From  Table  5,  all  values  in  FNgen are  better  than  those  in  the corresponding FSVM. Moreover, also from Table 5, all fitness values obtained from G25, namely, F25, are already at least equal to FSVM, that is, FSVM ≤ F25 ≤ F50 ≤ F75 ≤ F100. Hence, the proposed CSVM outperforms the traditional SVM in the small-sample OA, and the wide discrepancy between the final performances of the  CSVM and the SVM is further reinforced in Subsection 6.3 using the parameters setting from the proposed small-sample OA.",
        "metadata": {
            "section_header": "Ex1: SMALL-SAMPLE OA test",
            "title": "Convolutional Support Vector Machine",
            "type": "paper"
        }
    },
    {
        "page_content": "## 6.3  Ex2\n\nThe results obtained from the proposed CSVM for G25, G50, G75, and G100 and from the 3-layer and 4layer  ANNs  are  marked  1-6,  respectively  in  Fig.  1.  The  results  for  G100  are  collected  to  evaluate  the effectiveness of the concept of the proposed small-sample OA and verify any possible effects on the average and the best fitness values of higher generation numbers. The complete data including the average, best, worst, and standard deviation of fitness of each fold for each dataset are listed in Appendix A.\n\n<!-- image -->\n\n<!-- image -->",
        "metadata": {
            "section_header": "Ex2",
            "title": "Convolutional Support Vector Machine",
            "type": "paper"
        }
    },
    {
        "page_content": "## 6.3.1 Boxplots of the experimental results from Ex2\n\nFrom Fig. 1, both results obtained from the 3-layer and 4-layer ANNs are the least favorable with a big gap between the proposed CSVM and the traditional SVM. Hence, these two ANN-based methods are not discussed further, and we only focus on the proposed CSVM and the traditional SVM.\n\nWe determined from Fig 1 that the higher the generation number, the better average fitness value. However, it can be observed that the best fitness value remains unchanged from G75 to G100 except for the 8 th fold in Dataset A, the 1 st and 10 th folds in Dataset B, and the 5 th fold in Dataset F. Therefore, Ngen =75 is acceptable and there is no need for Ngen = 100 to increase the fitness value of the best solution, as shown in Fig 1. The position (the fitness values obtained) and the length (the range of the fitness values) of box in G100 are frequently higher and shorter than those of G25 in most boxplots. Hence, a larger generation number has a higher probability of enhancing the average solution quality under the cost of the longer runtime, but ultimately does little to improve the best fitness value.",
        "metadata": {
            "section_header": "Boxplots of the experimental results from Ex2",
            "title": "Convolutional Support Vector Machine",
            "type": "paper"
        }
    },
    {
        "page_content": "## 6.3.2 Number of folds for finding the final best fitness values\n\nTable 7 lists the number of folds that have found the final best fitness values. The subscripts of dataset ID in the first column of Table 7 indicate the generation number used in Ex 2; for example, B25 indicates that 25 generations are used for dataset B based on the parameters obtained with small-sample OA in Ex 1. Folds 7, 8, 10, 8, and 10 (see bold numbers in Table 7) under G25, G25, G75, G25, and G25 in Dataset A, B, D, F, and H, respectively, have found the best final fitness values.\n\nThe folds written as subscripts indicate the final best fitness values that have failed to be found. For example, the 71,4,7 in (G25, A25) represents that there are seven folds (from the ten folds) that have already\n\nfound the best fitness values after 25 generations with the remaining three folds 1, 4, and 7 failing to do so in Dataset A.\n\nTable 7. Number of folds that have found the best solution.\n\n| Dataset   | G 25    | G 50   | G 75   |   G 100 |\n|-----------|---------|--------|--------|---------|\n| A 25      | 7 1,4,7 | 8 8,10 | 9 8    |      10 |\n| B 25      | 8 1,10  | 8 1,10 | 8 1,10 |      10 |\n| D 75      | 9 7     | 10     | 10     |      10 |\n| F 25      | 8 5,6   | 9 5    | 9 5    |      10 |\n| H 25      | 10      | 10     | 10     |      10 |\n\nTo calculate the probability of the best final fitness value in Table 7, we add the folds (7 + 8 + 10 + 8 +10) and divide the product by the total number of folds (50) in table 6 to get 86%, which informs us that the probability of finding the best final fitness value without reaching G100, which entails a significantly longer runtime is 86%.\n\nHence, the proposed small-sample OA is effective in setting parameters to increase the efficiency and solution  quality  of  the  proposed  CSVM.  The  above  observation  further  confirms  that  having  better parameters ultimately negates the need for a greater generation number to increase the fitness of the best solution.",
        "metadata": {
            "section_header": "Number of folds for finding the final best fitness values",
            "title": "Convolutional Support Vector Machine",
            "type": "paper"
        }
    },
    {
        "page_content": "## 6.3.3 ANOVA of the Experimental Results\n\nTo investigate the small-sample OA, the Analysis of variance (ANOVA) is carried out to test the average fitness obtained from the proposed CSVM in terms of the parameters set by the small-sample OA, as shown in Table 8. The cells marked with 'v' indicate that there is a significant gap between the pair of distinctive generation numbers listed in their respective rows in the fold denoted by the column. This is reinforced through the distinct difference between the average fitness values obtained from G25 and G75 in all folds of dataset A.\n\nFrom  Table  8,  the  minimal  generation  numbers  should  be  75  and  50  for  only  Datasets  A  and  F, respectively, with an insignificant gap between the fitness values in each fold. Hence, the proposed smallsmall OA is still effective in determining the generation number to reduce the significant difference among\n\nall fitness values, even it focuses only on the best fitness value and not the average fitness value that we found.\n\nTable 8. The significant differences between two parameter settings.\n\n|                                                                      | 1          | 2          | 3          | 4          | 5          | 6          | 7          | 8          | 9          | 10         |\n|----------------------------------------------------------------------|------------|------------|------------|------------|------------|------------|------------|------------|------------|------------|\n| A (25, 50)                                                           | A (25, 50) | A (25, 50) | A (25, 50) | A (25, 50) | A (25, 50) | A (25, 50) | A (25, 50) | A (25, 50) | A (25, 50) | A (25, 50) |\n| (25, 75)                                                             | v          | v          | v          | v          | v          | v          | v          | v          | v          | v          |\n| (25, 100) (50, 75)                                                   | v          | v          | v          | v          | v          | v          | v          | v          | v          | v          |\n| (50, 100) (75, 100)                                                  | v          | v          | v          | v          | v          | v          | v          | v          | v          | v          |\n| B (25, 50) (25, 75) (25, 100) (50, 75) (50, 100)                     |            |            |            |            |            |            |            |            |            |            |\n| (75, 100) D (25, 50) (25, 75) (25, 100) (50, 75) (50, 100) (75, 100) |            |            |            |            |            |            |            |            |            |            |\n| F (25, 50)                                                           | v          | v          | v          | v          | v          | v          | v          | v          | v          | v          |\n| (25, 75) (25, 100) (50, 75) (50, 100)                                | v v        | v v        | v v        | v v        | v v        | v v        | v v        | v v        | v v        | v v        |\n| (75, 100) G (25, 50) (25, 75) (25, 100) (50, 75) (50, 100) (75, 100) |            |            |            |            |            |            |            |            |            |            |",
        "metadata": {
            "section_header": "ANOVA of the Experimental Results",
            "title": "Convolutional Support Vector Machine",
            "type": "paper"
        }
    },
    {
        "page_content": "## 6.3.4 MPI of the Experimental Results\n\nTo  further  investigate  the  development  of  the  proposed  CSVM,  two  other  indices:  the  average maximum possible improvement (MPIavg%) and the best maximum possible improvement (MPIavg%) are introduced and defined as:\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nThe MPIavg% and MPImax% results are listed in Tables 9 and 10, respectively, where the cells marked '*' indicate that both the related Fsvm and the average and/or the best fitness obtained are 100% correct; for example, the 2 nd , 4 th , and 5 th folds in dataset B in Table 9. The bold numbers denote the best values among\n\nall folds for each dataset under the same generation number. Note that a value of 100, as in the 7th fold of Dataset B in Table 10, indicate that the related accuracy is 100%.\n\nTABLE 9. The 100 × MPIavg%.\n\n|    | 1     |     2 | 3     |     4 |       | 5     |     6 |     7 |     8 |     9 |    10 |   avg |\n|----|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|\n| A  | F 25  | 46.67 | 58.33 | 58.46 | 20.00 | 48.75 | 33.03 | 50.95 | 35.46 | 50.91 | 35    | 43.76 |\n|    | F 50  | 49.39 | 65.00 | 61.28 | 20.67 | 51.25 | 35.76 | 54.05 | 37.58 | 53.94 | 36.67 | 46.56 |\n|    | F 75  | 50.61 | 68.33 | 64.36 | 20.67 | 52.50 | 36.06 | 54.76 | 38.18 | 55.76 | 38.06 | 47.93 |\n|    | F 100 | 52.42 | 70.00 | 65.13 | 22.67 | 52.92 | 36.36 | 55.71 | 40.3  | 56.97 | 39.44 | 49.19 |\n| B  | F 25  | 66.67 | *     | 38.89 | *     | *     | 50    | 52.22 |  0    | 75    |  0    | 40.4  |\n|    | F 50  | 66.67 | *     | 43.33 | *     | *     | 50    | 55.56 |  0    | 75    |  0    | 41.51 |\n|    | F 75  | 66.67 | *     | 45.56 | *     | *     | 50    | 61.11 |  0    | 75    |  0    | 42.62 |\n|    | F 100 | 67.78 | *     | 48.89 | *     | *     | 50    | 62.22 |  0    | 75    |  1.67 | 43.65 |\n| D  | F 25  | 29.22 | 24.58 | 24.37 | 13.12 | 0.16  |  8.97 |  0    |  9.09 | 14    | 26.04 | 14.96 |\n|    | F 50  | 30.98 | 26.46 | 25.83 | 13.75 | 0.32  | 11.41 |  0.22 | 10.15 | 16.22 | 28.75 | 16.41 |\n|    | F 75  | 31.37 | 27.29 | 27.08 | 14.17 | 0.48  | 12.18 |  0.22 | 11.21 | 17.11 | 30.21 | 17.13 |\n|    | F 100 | 31.37 | 27.71 | 27.08 | 14.37 | 0.48  | 12.18 |  0.44 | 11.82 | 17.56 | 30.62 | 17.36 |\n| F  | F 25  | 30.39 | 10.16 | 21.59 | 11.25 | 8.33  |  5.88 |  0    | 13.91 | 36.83 | 18.18 | 15.65 |\n|    | F 50  | 34.31 | 11.11 | 23.02 | 12.92 | 8.33  |  6.08 |  0    | 14.64 | 39.21 | 18.18 | 16.78 |\n|    | F 75  | 35.49 | 11.59 | 23.33 | 13.96 | 8.33  |  6.27 |  0    | 15.07 | 40    | 18.18 | 17.22 |\n|    | F 100 | 36.27 | 12.06 | 23.65 | 15.63 | 8.61  |  6.47 |  0    | 15.36 | 40.48 | 18.18 | 17.67 |\n| H  | F 25  | 42    | 78.67 | 40    | 38.33 | *     | 67.5  | 75    |  5.56 | 44.81 | 26.67 | 46.5  |\n|    | F 50  | 42.67 | 79.33 | 40.67 | 42.50 | *     | 73.33 | 75.83 | 11.11 | 45.56 | 28.33 | 48.81 |\n|    | F 75  | 44.67 | 80.00 | 40.67 | 44.17 | *     | 74.17 | 75.83 | 13.33 | 45.93 | 30    | 49.86 |\n|    | F 100 | 45.33 | 80.00 | 40.67 | 44.17 | *     | 75    | 75.83 | 16.67 | 45.93 | 32.5  | 50.68 |",
        "metadata": {
            "section_header": "MPI of the Experimental Results",
            "title": "Convolutional Support Vector Machine",
            "type": "paper"
        }
    },
    {
        "page_content": "## TABLE 10. The 100 × MPImax%.\n\n|    | 1     |      2 |             | 3     | 4     | 5     |     6 |      7 |     8 |     9 |    10 |   avg |\n|----|-------|--------|-------------|-------|-------|-------|-------|--------|-------|-------|-------|-------|\n| A  | F 25  |  54.55 | 75.00       | 76.92 | 20.00 | 62.50 | 36.36 |  57.14 | 45.45 | 63.64 | 41.67 | 53.32 |\n|    | F 50  |  63.64 | 75.00 76.92 |       | 40.00 | 62.50 | 36.36 |  64.29 | 45.45 | 63.64 | 41.67 | 56.95 |\n|    | F 75  |  63.64 | 75.00       | 76.92 | 40.00 | 62.50 | 36.36 |  64.29 | 45.45 | 63.64 | 50    | 57.78 |\n|    | F 100 |  63.64 | 75.00       | 76.92 | 40.00 | 62.50 | 36.36 |  64.29 | 54.55 | 63.64 | 50    | 58.69 |\n| B  | F 25  |  66.67 | *           | 66.67 | *     | *     | 50    | 100    |  0    | 75    |  0    | 51.19 |\n|    | F 50  |  66.67 | *           | 66.67 | *     | *     | 50    | 100    |  0    | 75    |  0    | 51.19 |\n|    | F 75  |  66.67 | *           | 66.67 | *     | *     | 50    | 100    |  0    | 75    |  0    | 51.19 |\n|    | F 100 | 100    | *           | 66.67 | *     | *     | 50    | 100    |  0    | 75    | 50    | 63.1  |\n| D  | F 25  |  35.29 | 37.50       | 31.25 | 18.75 | 4.76  | 15.38 |   0    | 13.64 | 20    | 43.75 | 22.03 |\n|    | F 50  |  35.29 | 37.50       | 31.25 | 18.75 | 4.76  | 15.38 |   6.67 | 13.64 | 20    | 43.75 | 22.7  |\n|    | F 75  |  35.29 | 37.50       | 31.25 | 18.75 | 4.76  | 15.38 |   6.67 | 13.64 | 20    | 43.75 | 22.7  |\n|    | F 100 |  35.29 | 37.50       | 31.25 | 18.75 | 4.76  | 15.38 |   6.67 | 13.64 | 20    | 43.75 | 22.7  |\n| F  | F 25  |  41.18 | 14.29       | 28.57 | 25.00 | 8.33  |  5.88 |   0    | 17.39 | 42.86 | 18.18 | 20.17 |\n|    | F 50  |  41.18 | 14.29       | 28.57 | 25.00 | 8.33  | 11.76 |   0    | 17.39 | 42.86 | 18.18 | 20.76 |\n|    | F 75  |  41.18 | 14.29       | 28.57 | 25.00 | 8.33  | 11.76 |   0    | 17.39 | 42.86 | 18.18 | 20.76 |\n|    | F 100 |  41.18 | 14.29       | 28.57 | 25.00 | 16.67 | 11.76 |   0    | 17.39 | 42.86 | 18.18 | 21.59 |\n| H  | F 25  |  60    | 80.00       | 40.00 | 50.00 | *     | 75    | 100    | 33.33 | 55.56 | 50    | 60.43 |\n|    | F 50  |  60    | 80.00       | 60.00 | 50.00 | *     | 75    | 100    | 33.33 | 55.56 | 50    | 62.65 |\n|    | F 75  |  60    | 80.00       | 60.00 | 50.00 | *     | 75    | 100    | 33.33 | 55.56 | 50    | 62.65 |\n|    | F 100 |  60    | 80.00       | 60.00 | 50.00 | *     | 75    | 100    | 33.33 | 55.56 | 50    | 62.65 |\n\nAs shown in Tables 9 and 10, the results obtained from the proposed CSVM are at least 14.96% and 20.17%, with at most a 50.68% and 63.10% improvement in MPIavg% and MPImax%, respectively. The results shed light on the effectiveness of the proposed CSVM in comparison with the traditional SVM. It can be also observed that the more attributes, the greater the results obtained from the proposed CSVM regardless of the number of records. Ultimately, compared to the traditional SVM, the proposed CSVM is more suitable for small data.",
        "metadata": {
            "section_header": "TABLE 10. The 100 × MPImax%.",
            "title": "Convolutional Support Vector Machine",
            "type": "paper"
        }
    },
    {
        "page_content": "## 7. CONCLUSIONS AND FUTURE WORK\n\nClassification  is  of  utmost  importance  in  data  mining.  The  proposed  new  classifier,  CSVM,  is  a convolutional SVM modified with a new repeated-attribute convolution product, in which all variables in each  filter  are  updated  and  trained  based  on  the  proposed  novel  SSO.  Equipped  with  a  self-adaptive structure and pFilter , this greedy SSO is a one-solution, one-filter, one-variable type and its parameters are delineated by the proposed small-sample OA.\n\nAccording to the experiment results for the five UCI datasets, namely, Australian Credit Approval, Breast-cancer, Diabetes, Fourclass, and Heart Disease [34], from Ex2 in Section 6, the proposed CSVM with the parameter setting selected from Ex1 outperforms the traditional SVM, the 3-layer ANN, and the 4-layer ANN with an improved accuracy of at least 14.96% and up to 50.68% in MPIavg%. Hence, the proposed  small-sample  OA  discussed  in  Subsection  4.2  enables  the  CSVM  to  improve  its  overall performance, while the proposed CSVM ultimately serves as a successful concoction of the advantages of SVM, the convolution product, and SSO.\n\nThe classifier design method is a crucial element in the provision of useful information in the modern world.  Through  comparisons  of  the  results  of  experiments,  it  can  be  determined  that  whether  further research will be conducted on the proposed CSVM, which will be applied to multi-class datasets with more attributes, classes, and records, and amalgamated with particular feature selections.",
        "metadata": {
            "section_header": "CONCLUSIONS AND FUTURE WORK",
            "title": "Convolutional Support Vector Machine",
            "type": "paper"
        }
    },
    {
        "page_content": "## ACKNOWLEDGMENTS\n\nThis research was supported in part by the National Science Council of Taiwan, R.O.C. under grant MOST 104-2221-E-007-061-MY3 and MOST 107-2221-E-007-072-MY3.",
        "metadata": {
            "section_header": "ACKNOWLEDGMENTS",
            "title": "Convolutional Support Vector Machine",
            "type": "paper"
        }
    },
    {
        "page_content": "## APPENDEX A.\n\nTable A1. The results obtained based on the Australian credit approval dataset.\n\n|   Fold | Index   |     SVM |      F 25 |      F 50 |      F 75 |     F 100 |\n|--------|---------|---------|-----------|-----------|-----------|-----------|\n|      1 | AVG     | 81.6667 | 90.2222   | 90.7222   | 90.9444   | 91.2778   |\n|      1 | MAX     | 81.6667 | 91.6667   | 93.3333   | 93.3333   | 93.3333   |\n|      1 | MIN     | 81.6667 | 88.3333   | 88.3333   | 90        | 90        |\n|      1 | STDEV   |  0      |  1.04801  |  1.04344  |  1.04344  |  0.947201 |\n|      2 | AVG     | 92.1569 | 96.732    | 97.2549   | 97.5163   | 97.6471   |\n|      2 | MAX     | 92.1569 | 98.0392   | 98.0392   | 98.0392   | 98.0392   |\n|      2 | MIN     | 92.1569 | 96.0784   | 96.0784   | 96.0784   | 96.0784   |\n|      2 | STDEV   |  0      |  0.940124 |  0.977006 |  0.881915 |  0.797722 |\n|      3 | AVG     | 83.1169 | 92.987    | 93.4632   | 93.9827   | 94.1126   |\n|      3 | MAX     | 83.1169 | 96.1039   | 96.1039   | 96.1039   | 96.1039   |\n|      3 | MIN     | 83.1169 | 90.9091   | 90.9091   | 90.9091   | 90.9091   |\n|      3 | STDEV   |  0      |  1.43081  |  1.42606  |  1.54357  |  1.55296  |\n|      4 | AVG     | 92.1875 | 93.75     | 93.8021   | 93.8021   | 93.9583   |\n|      4 | MAX     | 92.1875 | 93.75     | 95.3125   | 95.3125   | 95.3125   |\n|      4 | MIN     | 92.1875 | 93.75     | 93.75     | 93.75     | 93.75     |\n|      4 | STDEV   |  0      |  0        |  0.285272 |  0.285272 |  0.540228 |\n|      5 | AVG     | 87.8788 | 93.7879   | 94.0909   | 94.2424   | 94.2929   |\n|      5 | MAX     | 87.8788 | 95.4545   | 95.4545   | 95.4545   | 95.4545   |\n|      5 | MIN     | 87.8788 | 92.4242   | 92.4242   | 93.9394   | 93.9394   |\n|      5 | STDEV   |  0      |  0.728274 |  0.728274 |  0.616422 |  0.651793 |\n|      6 | AVG     | 80.7018 | 87.076    | 87.6023   | 87.6608   | 87.7193   |\n|      6 | MAX     | 80.7018 | 87.7193   | 87.7193   | 87.7193   | 87.7193   |\n|      6 | MIN     | 80.7018 | 84.2105   | 85.9649   | 85.9649   | 87.7193   |\n|      6 | STDEV   |  0      |  0.975533 |  0.445102 |  0.320306 |  0        |\n|      7 | AVG     | 82.2785 | 91.308    | 91.8565   | 91.9831   | 92.1519   |\n|      7 | MAX     | 82.2785 | 92.4051   | 93.6709   | 93.6709   | 93.6709   |\n|      7 | MIN     | 82.2785 | 88.6076   | 91.1392   | 91.1392   | 91.1392   |\n|      7 | STDEV   |  0      |  1.0371   |  0.71939  |  0.691987 |  0.69729  |\n|      8 | AVG     | 85.1351 | 90.4054   | 90.7207   | 90.8108   | 91.1261   |\n|      8 | MAX     | 85.1351 | 91.8919   | 91.8919   | 91.8919   | 93.2432   |\n|      8 | MIN     | 85.1351 | 89.1892   | 89.1892   | 90.5405   | 90.5405   |\n|      8 | STDEV   |  0      |  0.740167 |  0.586719 |  0.54978  |  0.768    |\n|      9 | AVG     | 86.4198 | 93.3333   | 93.7449   | 93.9918   | 94.1564   |\n|      9 | MAX     | 86.4198 | 95.0617   | 95.0617   | 95.0617   | 95.0617   |\n|      9 | MIN     | 86.4198 | 92.5926   | 92.5926   | 92.5926   | 93.8272   |\n|      9 | STDEV   |  0      |  0.695363 |  0.555281 |  0.536015 |  0.555279 |\n|     10 | AVG     | 85.1852 | 90.3704   | 90.6173   | 90.823    | 91.0288   |\n|     10 | MAX     | 85.1852 | 91.358    | 91.358    | 92.5926   | 92.5926   |\n|     10 | MIN     | 85.1852 | 88.8889   | 88.8889   | 90.1235   | 90.1235   |\n|     10 | STDEV   |  0      |  0.753404 |  0.69536  |  0.701629 |  0.720113 |\n\nTable A2. The results obtained based on the Breast-cancer dataset.\n\n|   Fold | Index   |      SVM |       F 25 |       F 50 |       F 75 |      F 100 |\n|--------|---------|----------|------------|------------|------------|------------|\n|      1 | AVG     |  95.5882 |  98.5294   |  98.5294   |  98.5294   |  98.5784   |\n|      1 | MAX     |  95.5882 |  98.5294   |  98.5294   |  98.5294   | 100        |\n|      1 | MIN     |  95.5882 |  98.5294   |  98.5294   |  98.5294   |  98.5294   |\n|      1 | STDEV   |   0      |   0        |   0        |   0        |   0.268492 |\n|      2 | AVG     | 100      | 100        | 100        | 100        | 100        |\n|      2 | MAX     | 100      | 100        | 100        | 100        | 100        |\n|      2 | MIN     | 100      | 100        | 100        | 100        | 100        |\n|      2 | STDEV   |   0      |   0        |   0        |   0        |   0        |\n|      3 | AVG     |  95.5882 |  97.3039   |  97.5      |  97.598    |  97.7451   |\n|      3 | MAX     |  95.5882 |  98.5294   |  98.5294   |  98.5294   |  98.5294   |\n|      3 | MIN     |  95.5882 |  97.0588   |  97.0588   |  97.0588   |  97.0588   |\n|      3 | STDEV   |   0      |   0.557425 |   0.685429 |   0.720783 |   0.746201 |\n|      4 | AVG     | 100      | 100        | 100        | 100        | 100        |\n|      4 | MAX     | 100      | 100        | 100        | 100        | 100        |\n|      4 | MIN     | 100      | 100        | 100        | 100        | 100        |\n|      4 | STDEV   |   0      |   0        |   0        |   0        |   0        |\n|      5 | AVG     | 100      | 100        | 100        | 100        | 100        |\n|      5 | MAX     | 100      | 100        | 100        | 100        | 100        |\n|      5 | MIN     | 100      | 100        | 100        | 100        | 100        |\n|      5 | STDEV   |   0      |   0        |   0        |   0        |   0        |\n|      6 | AVG     |  94.1176 |  97.0588   |  97.0588   |  97.0588   |  97.0588   |\n|      6 | MAX     |  94.1176 |  97.0588   |  97.0588   |  97.0588   |  97.0588   |\n|      6 | MIN     |  94.1176 |  97.0588   |  97.0588   |  97.0588   |  97.0588   |\n|      6 | STDEV   |   0      |   0        |   0        |   0        |   0        |\n|      7 | AVG     |  95.5882 |  97.8922   |  98.0392   |  98.2843   |  98.3333   |\n|      7 | MAX     |  95.5882 | 100        | 100        | 100        | 100        |\n|      7 | MIN     |  95.5882 |  97.0588   |  97.0588   |  97.0588   |  97.0588   |\n|      7 | STDEV   |   0      |   0.92068  |   0.89188  |   0.870726 |   0.840216 |\n|      8 | AVG     |  98.5294 |  98.5294   |  98.5294   |  98.5294   |  98.5294   |\n|      8 | MAX     |  98.5294 |  98.5294   |  98.5294   |  98.5294   |  98.5294   |\n|      8 | MIN     |  98.5294 |  98.5294   |  98.5294   |  98.5294   |  98.5294   |\n|      8 | STDEV   |   0      |   0        |   0        |   0        |   0        |\n|      9 | AVG     |  94.1176 |  98.5294   |  98.5294   |  98.5294   |  98.5294   |\n|      9 | MAX     |  94.1176 |  98.5294   |  98.5294   |  98.5294   |  98.5294   |\n|      9 | MIN     |  94.1176 |  98.5294   |  98.5294   |  98.5294   |  98.5294   |\n|      9 | STDEV   |   0      |   0        |   0        |   0        |   0        |\n|     10 | AVG     |  97.1831 |  97.1831   |  97.1831   |  97.1831   |  97.23     |\n|     10 | MAX     |  97.1831 |  97.1831   |  97.1831   |  97.1831   |  98.5916   |\n|     10 | MIN     |  97.1831 |  97.1831   |  97.1831   |  97.1831   |  97.1831   |\n|     10 | STDEV   |   0      |   0        |   0        |   0        |   0.257148 |\n\n|      | Table A3. The results obtained based on the Diabetes dataset.   | Table A3. The results obtained based on the Diabetes dataset.   | Table A3. The results obtained based on the Diabetes dataset.   | Table A3. The results obtained based on the Diabetes dataset.   | Table A3. The results obtained based on the Diabetes dataset.   | Table A3. The results obtained based on the Diabetes dataset.   |\n|------|-----------------------------------------------------------------|-----------------------------------------------------------------|-----------------------------------------------------------------|-----------------------------------------------------------------|-----------------------------------------------------------------|-----------------------------------------------------------------|\n| Fold | Index                                                           | SVM                                                             | F 25                                                            | F 50                                                            | F 75                                                            | F 100                                                           |\n| 1    | AVG                                                             | 76.712326                                                       | 83.515981                                                       | 83.926940                                                       | 84.018263                                                       | 84.018263                                                       |\n|      | MAX                                                             | 76.712326                                                       | 84.931503                                                       | 84.931503                                                       | 84.931503                                                       | 84.931503                                                       |\n|      | MIN                                                             | 76.712326                                                       | 80.821915                                                       | 80.821915                                                       | 80.821915                                                       | 80.821915                                                       |\n|      | STDEV                                                           | 0.000000                                                        | 1.271038                                                        | 1.133810                                                        | 1.156413                                                        | 1.156413                                                        |\n| 2    | AVG                                                             | 76.811592                                                       | 82.512076                                                       | 82.946859                                                       | 83.140096                                                       | 83.236714                                                       |\n|      | MAX                                                             | 76.811592                                                       | 85.507248                                                       | 85.507248                                                       | 85.507248                                                       | 85.507248                                                       |\n|      | MIN                                                             | 76.811592                                                       | 79.710144                                                       | 81.159416                                                       | 81.159416                                                       | 81.159416                                                       |\n|      | STDEV                                                           | 0.000000                                                        | 1.314764                                                        | 1.054974                                                        | 1.041153                                                        | 0.983928                                                        |\n| 3    | AVG                                                             | 82.022469                                                       | 86.404494                                                       | 86.666666                                                       | 86.891385                                                       | 86.891385                                                       |\n|      | MAX                                                             | 82.022469                                                       | 87.640450                                                       | 87.640450                                                       | 87.640450                                                       | 87.640450                                                       |\n|      | MIN                                                             | 82.022469                                                       | 85.393257                                                       | 85.393257                                                       | 85.393257                                                       | 85.393257                                                       |\n|      | STDEV                                                           | 0.000000                                                        | 0.743555                                                        | 0.765669                                                        | 0.681437                                                        | 0.681437                                                        |\n| 4    | AVG                                                             | 76.470589                                                       | 79.558825                                                       | 79.705884                                                       | 79.803923                                                       | 79.852943                                                       |\n|      | MAX                                                             | 76.470589                                                       | 80.882355                                                       | 80.882355                                                       | 80.882355                                                       | 80.882355                                                       |\n|      | MIN                                                             | 76.470589                                                       | 79.411766                                                       | 79.411766                                                       | 79.411766                                                       | 79.411766                                                       |\n|      | STDEV                                                           | 0.000000                                                        | 0.448719                                                        | 0.598292                                                        | 0.661436                                                        | 0.685429                                                        |\n| 5    | AVG                                                             | 71.232880                                                       | 71.278542                                                       | 71.324203                                                       | 71.369865                                                       | 71.369865                                                       |\n|      | MAX                                                             | 71.232880                                                       | 72.602737                                                       | 72.602737                                                       | 72.602737                                                       | 72.602737                                                       |\n|      | MIN                                                             | 71.232880                                                       | 71.232880                                                       | 71.232880                                                       | 71.232880                                                       | 71.232880                                                       |\n|      | STDEV                                                           | 0.000000                                                        | 0.250101                                                        | 0.347544                                                        | 0.417983                                                        | 0.417983                                                        |\n| 6    | AVG                                                             | 68.292686                                                       | 71.138209                                                       | 71.910567                                                       | 72.154470                                                       | 72.154470                                                       |\n|      | MAX                                                             | 68.292686                                                       | 73.170731                                                       | 73.170731                                                       | 73.170731                                                       | 73.170731                                                       |\n|      | MIN                                                             | 68.292686                                                       | 69.512192                                                       | 70.731705                                                       | 70.731705                                                       | 70.731705                                                       |\n|      | STDEV                                                           | 0.000000                                                        | 0.924510                                                        | 0.815458                                                        | 0.852356                                                        | 0.852356                                                        |\n| 7    | AVG                                                             | 82.558144                                                       | 82.558144                                                       | 82.596903                                                       | 82.596903                                                       | 82.635663                                                       |\n|      | MAX                                                             | 82.558144                                                       | 82.558144                                                       | 83.720932                                                       | 83.720932                                                       | 83.720932                                                       |\n|      | MIN                                                             | 82.558144                                                       | 82.558144                                                       | 82.558144                                                       | 7.000000                                                        | 82.558144                                                       |\n|      | STDEV                                                           | 0.000000                                                        | 0.000000                                                        | 0.212295                                                        | 0.212295                                                        | 0.295009                                                        |\n| 8    | AVG                                                             | 71.428574                                                       | 74.025972                                                       | 74.329003                                                       | 74.632034                                                       | 74.805194                                                       |\n|      | MAX                                                             | 71.428574                                                       | 75.324677                                                       | 75.324677                                                       | 75.324677                                                       | 75.324677                                                       |\n|      | MIN                                                             | 71.428574                                                       | 71.428574                                                       | 72.727272                                                       | 72.727272                                                       | 72.727272                                                       |\n|      | STDEV                                                           | 0.000000                                                        | 1.023167                                                        | 0.881703                                                        | 0.742010                                                        | 0.731485                                                        |\n| 9    | AVG                                                             | 80.519478                                                       | 83.246752                                                       | 83.679652                                                       | 83.852811                                                       | 83.939391                                                       |\n|      | MAX                                                             | 80.519478                                                       | 84.415581                                                       | 84.415581                                                       | 84.415581                                                       | 84.415581                                                       |\n|      | MIN                                                             | 80.519478                                                       | 81.818184                                                       | 81.818184                                                       | 81.818184                                                       | 83.116882                                                       |\n|      | STDEV                                                           | 0.000000                                                        | 0.859431                                                        | 0.738077                                                        | 0.738077                                                        | 0.636534                                                        |\n| 10   | AVG                                                             | 78.378380                                                       | 84.009009                                                       | 84.594594                                                       | 84.909910                                                       | 85.000000                                                       |\n|      | MAX                                                             | 78.378380                                                       | 87.837837                                                       | 87.837837                                                       | 87.837837                                                       | 87.837837                                                       |\n|      | MIN                                                             | 78.378380                                                       | 82.432434                                                       | 82.432434                                                       | 82.432434                                                       | 82.432434                                                       |\n|      | STDEV                                                           | 0.000000                                                        | 1.378265                                                        | 1.610712                                                        | 1.590385                                                        | 1.677734                                                        |\n\nTable A4. The results obtained based on the Fourclass dataset.\n\n| Fold   | Index   |     SVM |      F 25 |      F 50 |      F 75 |     F 100 |\n|--------|---------|---------|-----------|-----------|-----------|-----------|\n| 1      | AVG     | 80.2326 | 86.2403   | 87.0155   | 87.2481   | 87.4031   |\n|        | MAX     | 80.2326 | 88.3721   | 88.3721   | 88.3721   | 88.3721   |\n|        | MIN     | 80.2326 | 83.7209   | 83.7209   | 84.8837   | 84.8837   |\n|        | STDEV   |  0      |  1.67487  |  1.26213  |  1.20157  |  1.10453  |\n| 2      | AVG     | 79      | 81.1333   | 81.3333   | 81.4333   | 81.5333   |\n|        | MAX     | 79      | 82        | 82        | 82        | 82        |\n|        | MIN     | 79      | 81        | 81        | 81        | 81        |\n|        | STDEV   |  0      |  0.345746 |  0.479463 |  0.504007 |  0.507416 |\n| 3      | AVG     | 75.8621 | 81.0728   | 81.4176   | 81.4942   | 81.5709   |\n|        | MAX     | 75.8621 | 82.7586   | 82.7586   | 82.7586   | 82.7586   |\n|        | MIN     | 75.8621 | 78.1609   | 78.1609   | 78.1609   | 80.4598   |\n|        | STDEV   |  0      |  0.83942  |  0.744504 |  0.69819  |  0.367634 |\n| 4      | AVG     | 81.8182 | 83.8636   | 84.1667   | 84.3561   | 84.6591   |\n|        | MAX     | 81.8182 | 86.3636   | 86.3636   | 86.3636   | 86.3636   |\n|        | MIN     | 81.8182 | 82.9545   | 82.9545   | 82.9545   | 82.9545   |\n|        | STDEV   |  0      |  0.811801 |  0.891949 |  0.879379 |  0.882748 |\n| 5      | AVG     | 82.8571 | 84.2857   | 84.2857   | 84.2857   | 84.3333   |\n|        | MAX     | 82.8571 | 84.2857   | 84.2857   | 84.2857   | 85.7143   |\n|        | MIN     | 82.8571 | 84.2857   | 84.2857   | 84.2857   | 84.2857   |\n|        | STDEV   |  0      |  0        |  0        |  0        |  0.260821 |\n| 6      | AVG     | 81.9149 | 82.9787   | 83.0142   | 83.0496   | 83.0851   |\n|        | MAX     | 81.9149 | 82.9787   | 84.0426   | 84.0426   | 84.0426   |\n|        | MIN     | 81.9149 | 82.9787   | 82.9787   | 82.9787   | 82.9787   |\n|        | STDEV   |  0      |  0        |  0.194229 |  0.269904 |  0.324607 |\n| 7      | AVG     | 85.8974 | 85.8974   | 85.8974   | 85.8974   | 85.8974   |\n|        | MAX     | 85.8974 | 85.8974   | 85.8974   | 85.8974   | 85.8974   |\n|        | MIN     | 85.8974 | 85.8974   | 85.8974   | 85.8974   | 85.8974   |\n|        | STDEV   |  0      |  0        |  0        |  0        |  0        |\n| 8      | AVG     | 71.4286 | 77.7528   | 77.9401   | 74.632    | 78.1273   |\n|        | MAX     | 71.4286 | 78.6517   | 78.6517   | 75.3247   | 78.6517   |\n|        | MIN     | 71.4286 | 77.5281   | 77.5281   | 72.7273   | 77.5281   |\n|        | STDEV   |  0      |  0.457122 |  0.550711 |  0.74201  |  0.570131 |\n| 9      | AVG     | 80.5195 | 84.5736   | 85.155    | 83.8528   | 85.4651   |\n|        | MAX     | 80.5195 | 86.0465   | 86.0465   | 84.4156   | 86.0465   |\n|        | MIN     | 80.5195 | 81.3953   | 82.5581   | 81.8182   | 83.7209   |\n|        | STDEV   |  0      |  1.36335  |  1.04376  |  0.738077 |  0.732235 |\n| 10     | AVG     | 78.3784 | 89.2857   | 89.2857   | 84.9099   | 89.2857   |\n|        | MAX     | 78.3784 | 89.2857   | 89.2857   | 87.8378   | 89.2857   |\n|        | MIN     | 78.3784 | 89.2857   | 89.2857   | 82.4324   | 89.2857   |\n|        | STDEV   |  0      |  0        |  0        |  1.59038  |  0        |\n\nTable A5. The results obtained based on the Heart Disease dataset.\n\n| Fold   | Index   |      SVM |       F 25 |       F 50 |       F 75 |      F 100 |\n|--------|---------|----------|------------|------------|------------|------------|\n| 1      | AVG     |  80      |  88.4      |  88.5333   |  88.9333   |  89.0667   |\n|        | MAX     |  80      |  92        |  92        |  92        |  92        |\n|        | MIN     |  80      |  88        |  88        |  88        |  88        |\n|        | STDEV   |   0      |   1.22051  |   1.38298  |   1.72073  |   1.79911  |\n| 2      | AVG     |  82.7586 |  96.3218   |  96.4368   |  96.5517   |  96.5517   |\n|        | MAX     |  82.7586 |  96.5517   |  96.5517   |  96.5517   |  96.5517   |\n|        | MIN     |  82.7586 |  93.1034   |  93.1034   |  96.5517   |  96.5517   |\n|        | STDEV   |   0      |   0.874857 |   0.629567 |   0        |   0        |\n| 3      | AVG     |  82.7586 |  89.6552   |  89.7701   |  89.7701   |  89.7701   |\n|        | MAX     |  82.7586 |  89.6552   |  93.1034   |  93.1034   |  93.1034   |\n|        | MIN     |  82.7586 |  89.6552   |  89.6552   |  89.6552   |  89.6552   |\n|        | STDEV   |   0      |   0        |   0.629566 |   0.629566 |   0.629566 |\n| 4      | AVG     |  88.2353 |  92.7451   |  93.2353   |  93.4314   |  93.4314   |\n|        | MAX     |  88.2353 |  94.1176   |  94.1176   |  94.1176   |  94.1176   |\n|        | MIN     |  88.2353 |  91.1765   |  91.1765   |  91.1765   |  91.1765   |\n|        | STDEV   |   0      |   1.4924   |   1.37086  |   1.26524  |   1.26524  |\n| 5      | AVG     | 100      | 100        | 100        | 100        | 100        |\n|        | MAX     | 100      | 100        | 100        | 100        | 100        |\n|        | MIN     | 100      | 100        | 100        | 100        | 100        |\n|        | STDEV   |   0      |   0        |   0        |   0        |   0        |\n| 6      | AVG     |  80      |  93.5      |  94.6667   |  94.8333   |  95        |\n|        | MAX     |  80      |  95        |  95        |  95        |  95        |\n|        | MIN     |  80      |  90        |  90        |  90        |  95        |\n|        | STDEV   |   0      |   2.33046  |   1.26854  |   0.912871 |   0        |\n| 7      | AVG     |  82.6087 |  95.6522   |  95.7971   |  95.7971   |  95.7971   |\n|        | MAX     |  82.6087 | 100        | 100        | 100        | 100        |\n|        | MIN     |  82.6087 |  91.3044   |  95.6522   |  95.6522   |  95.6522   |\n|        | STDEV   |   0      |   1.14179  |   0.7938   |   0.7938   |   0.7938   |\n| 8      | AVG     |  89.2857 |  89.881    |  90.4762   |  90.7143   |  91.0714   |\n|        | MAX     |  89.2857 |  92.8571   |  92.8571   |  92.8571   |  92.8571   |\n|        | MIN     |  89.2857 |  89.2857   |  89.2857   |  89.2857   |  89.2857   |\n|        | STDEV   |   0      |   1.35375  |   1.71237  |   1.77954  |   1.81624  |\n| 9      | AVG     |  75      |  86.2037   |  86.3889   |  86.4815   |  86.4815   |\n|        | MAX     |  75      |  88.8889   |  88.8889   |  88.8889   |  88.8889   |\n|        | MIN     |  75      |  86.1111   |  86.1111   |  86.1111   |  86.1111   |\n|        | STDEV   |   0      |   0.507149 |   0.847577 |   0.960403 |   0.960403 |\n| 10     | AVG     |  84.6154 |  88.718    |  88.9744   |  89.2308   |  89.6154   |\n|        | MAX     |  84.6154 |  92.3077   |  92.3077   |  92.3077   |  92.3077   |\n|        | MIN     |  84.6154 |  88.4615   |  88.4615   |  88.4615   |  88.4615   |\n|        | STDEV   |   0      |   0.9758   |   1.32979  |   1.56476  |   1.79266  |",
        "metadata": {
            "section_header": "APPENDEX A.",
            "title": "Convolutional Support Vector Machine",
            "type": "paper"
        }
    }
]