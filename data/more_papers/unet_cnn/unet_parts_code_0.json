[
   {
      "page_content": "",
      "metadata": {
         "section_header": "Global Variables",
         "type": "python global"
      }
   },
   {
      "page_content": "\"\"\" Parts of the U-Net model \"\"\"\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nclass DoubleConv(nn.Module):\n\t\"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n\tdef __init__(self, in_channels, out_channels, mid_channels=None):\n\t\tsuper().__init__()\n\t\tif not mid_channels:\n\t\t\tmid_channels = out_channels\n\t\tself.double_conv = nn.Sequential(\n\t\t\tnn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n\t\t\tnn.BatchNorm2d(mid_channels),\n\t\t\tnn.ReLU(inplace=True),\n\t\t\tnn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n\t\t\tnn.BatchNorm2d(out_channels),\n\t\t\tnn.ReLU(inplace=True)\n\t\t)\n\tdef forward(self, x):\n\t\treturn self.double_conv(x)\nclass Down(nn.Module):\n\t\"\"\"Downscaling with maxpool then double conv\"\"\"\n\tdef __init__(self, in_channels, out_channels):\n\t\tsuper().__init__()\n\t\tself.maxpool_conv = nn.Sequential(\n\t\t\tnn.MaxPool2d(2),\n\t\t\tDoubleConv(in_channels, out_channels)\n\t\t)\n\tdef forward(self, x):\n\t\treturn self.maxpool_conv(x)\nclass Up(nn.Module):\n\t\"\"\"Upscaling then double conv\"\"\"\n\tdef __init__(self, in_channels, out_channels, bilinear=True):\n\t\tsuper().__init__()\n\t\t# if bilinear, use the normal convolutions to reduce the number of channels\n\t\tif bilinear:\n\t\t\tself.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n\t\t\tself.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n\t\telse:\n\t\t\tself.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n\t\t\tself.conv = DoubleConv(in_channels, out_channels)\n\tdef forward(self, x1, x2):\n\t\tx1 = self.up(x1)\n\t\t# input is CHW\n\t\tdiffY = x2.size()[2] - x1.size()[2]\n\t\tdiffX = x2.size()[3] - x1.size()[3]\n\t\tx1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n\t\t\t\t\t\tdiffY // 2, diffY - diffY // 2])\n\t\t# if you have padding issues, see\n\t\t# https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n\t\t# https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n\t\tx = torch.cat([x2, x1], dim=1)\n\t\treturn self.conv(x)\nclass OutConv(nn.Module):\n\tdef __init__(self, in_channels, out_channels):\n\t\tsuper(OutConv, self).__init__()\n\t\tself.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n\tdef forward(self, x):\n\t\treturn self.conv(x)",
      "metadata": {
         "section_header": "Global Other",
         "type": "python global"
      }
   },
   {
      "page_content": "Functions: __init__, forward",
      "metadata": {
         "section_header": "DoubleConv",
         "type": "python class"
      }
   },
   {
      "page_content": "\tdef __init__(self, in_channels, out_channels, mid_channels=None):\n\t\tsuper().__init__()\n\t\tif not mid_channels:\n\t\t\tmid_channels = out_channels\n\t\tself.double_conv = nn.Sequential(\n\t\t\tnn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n\t\t\tnn.BatchNorm2d(mid_channels),\n\t\t\tnn.ReLU(inplace=True),\n\t\t\tnn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n\t\t\tnn.BatchNorm2d(out_channels),\n\t\t\tnn.ReLU(inplace=True)\n\t\t)",
      "metadata": {
         "section_header": "__init__",
         "type": "python function"
      }
   },
   {
      "page_content": "\tdef forward(self, x):\n\t\treturn self.double_conv(x)",
      "metadata": {
         "section_header": "forward",
         "type": "python function"
      }
   },
   {
      "page_content": "Functions: __init__, forward",
      "metadata": {
         "section_header": "Down",
         "type": "python class"
      }
   },
   {
      "page_content": "\tdef __init__(self, in_channels, out_channels):\n\t\tsuper().__init__()\n\t\tself.maxpool_conv = nn.Sequential(\n\t\t\tnn.MaxPool2d(2),\n\t\t\tDoubleConv(in_channels, out_channels)\n\t\t)",
      "metadata": {
         "section_header": "__init__",
         "type": "python function"
      }
   },
   {
      "page_content": "\tdef forward(self, x):\n\t\treturn self.maxpool_conv(x)",
      "metadata": {
         "section_header": "forward",
         "type": "python function"
      }
   },
   {
      "page_content": "Functions: __init__, forward",
      "metadata": {
         "section_header": "Up",
         "type": "python class"
      }
   },
   {
      "page_content": "\tdef __init__(self, in_channels, out_channels, bilinear=True):\n\t\tsuper().__init__()\n\t\t# if bilinear, use the normal convolutions to reduce the number of channels\n\t\tif bilinear:\n\t\t\tself.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n\t\t\tself.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n\t\telse:\n\t\t\tself.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n\t\t\tself.conv = DoubleConv(in_channels, out_channels)",
      "metadata": {
         "section_header": "__init__",
         "type": "python function"
      }
   },
   {
      "page_content": "\tdef forward(self, x1, x2):\n\t\tx1 = self.up(x1)\n\t\t# input is CHW\n\t\tdiffY = x2.size()[2] - x1.size()[2]\n\t\tdiffX = x2.size()[3] - x1.size()[3]\n\t\tx1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n\t\t\t\t\t\tdiffY // 2, diffY - diffY // 2])\n\t\t# if you have padding issues, see\n\t\t# https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n\t\t# https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n\t\tx = torch.cat([x2, x1], dim=1)\n\t\treturn self.conv(x)",
      "metadata": {
         "section_header": "forward",
         "type": "python function"
      }
   },
   {
      "page_content": "Functions: __init__, forward",
      "metadata": {
         "section_header": "OutConv",
         "type": "python class"
      }
   },
   {
      "page_content": "\tdef __init__(self, in_channels, out_channels):\n\t\tsuper(OutConv, self).__init__()\n\t\tself.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)",
      "metadata": {
         "section_header": "__init__",
         "type": "python function"
      }
   },
   {
      "page_content": "\tdef forward(self, x):\n\t\treturn self.conv(x)",
      "metadata": {
         "section_header": "forward",
         "type": "python function"
      }
   }
]