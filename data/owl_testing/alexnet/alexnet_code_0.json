[
   {
      "page_content": "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nNUM_EPOCHS = 90  # original paper\nBATCH_SIZE = 128\nMOMENTUM = 0.9\nLR_DECAY = 0.0005\nLR_INIT = 0.01\nIMAGE_DIM = 227  # pixels\nNUM_CLASSES = 1000  # 1000 classes for imagenet 2012 dataset\nDEVICE_IDS = [0, 1, 2, 3]  # GPUs to use\nINPUT_ROOT_DIR = 'alexnet_data_in'\nTRAIN_IMG_DIR = 'alexnet_data_in/imagenet'\nOUTPUT_DIR = 'alexnet_data_out'\nLOG_DIR = OUTPUT_DIR + '/tblogs'  # tensorboard logs\nCHECKPOINT_DIR = OUTPUT_DIR + '/models'  # model checkpoints",
      "metadata": {
         "section_header": "Global Variables",
         "type": "python global"
      }
   },
   {
      "page_content": "\"\"\"\nImplementation of AlexNet, from paper\n\"ImageNet Classification with Deep Convolutional Neural Networks\" by Alex Krizhevsky et al.\nSee: https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\n\"\"\"\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils import data\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\nfrom tensorboardX import SummaryWriter\nos.makedirs(CHECKPOINT_DIR, exist_ok=True)\nclass AlexNet(torch.nn.Module):\n\t\"\"\"\n\tNeural network model consisting of layers propsed by AlexNet paper.\n\t\"\"\"\n\tdef __init__(self, num_classes=1000):\n\t\t\"\"\"\n\t\tDefine and allocate layers for this neural net.\n\t\tArgs:\n\t\t\tnum_classes (int): number of classes to predict with this model\n\t\t\"\"\"\n\t\tsuper().__init__()\n\t\t# input size should be : (b x 3 x 227 x 227)\n\t\t# The image in the original paper states that width and height are 224 pixels, but\n\t\t# the dimensions after first convolution layer do not lead to 55 x 55.\n\t\tself.net = nn.Sequential(\n\t\t\tnn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4),  # (b x 96 x 55 x 55)\n\t\t\tnn.ReLU(),\n\t\t\tnn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2),  # section 3.3\n\t\t\tnn.MaxPool2d(kernel_size=3, stride=2),  # (b x 96 x 27 x 27)\n\t\t\tnn.Conv2d(96, 256, 5, padding=2),  # (b x 256 x 27 x 27)\n\t\t\tnn.ReLU(),\n\t\t\tnn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2),\n\t\t\tnn.MaxPool2d(kernel_size=3, stride=2),  # (b x 256 x 13 x 13)\n\t\t\tnn.Conv2d(256, 384, 3, padding=1),  # (b x 384 x 13 x 13)\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(384, 384, 3, padding=1),  # (b x 384 x 13 x 13)\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(384, 256, 3, padding=1),  # (b x 256 x 13 x 13)\n\t\t\tnn.ReLU(),\n\t\t\tnn.MaxPool2d(kernel_size=3, stride=2),  # (b x 256 x 6 x 6)\n\t\t)\n\t\t# classifier is just a name for linear layers\n\t\tself.classifier = nn.Sequential(\n\t\t\tnn.Dropout(p=0.5, inplace=True),\n\t\t\tnn.Linear(in_features=(256 * 6 * 6), out_features=4096),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Dropout(p=0.5, inplace=True),\n\t\t\tnn.Linear(in_features=4096, out_features=4096),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Linear(in_features=4096, out_features=num_classes),\n\t\t)\n\t\tself.init_bias()  # initialize bias\n\tdef init_bias(self):\n\t\tfor layer in self.net:\n\t\t\tif isinstance(layer, nn.Conv2d):\n\t\t\t\tnn.init.normal_(layer.weight, mean=0, std=0.01)\n\t\t\t\tnn.init.constant_(layer.bias, 0)\n\t\t# original paper = 1 for Conv2d layers 2nd, 4th, and 5th conv layers\n\t\tnn.init.constant_(self.net[4].bias, 1)\n\t\tnn.init.constant_(self.net[10].bias, 1)\n\t\tnn.init.constant_(self.net[12].bias, 1)\n\tdef forward(self, x):\n\t\t\"\"\"\n\t\tPass the input through the net.\n\t\tArgs:\n\t\t\tx (Tensor): input tensor\n\t\tReturns:\n\t\t\toutput (Tensor): output tensor\n\t\t\"\"\"\n\t\tx = self.net(x)\n\t\tx = x.view(-1, 256 * 6 * 6)  # reduce the dimensions for linear layer input\n\t\treturn self.classifier(x)\nif __name__ == '__main__':\n\t# print the seed value\n\tseed = torch.initial_seed()\n\tprint('Used seed : {}'.format(seed))\n\ttbwriter = SummaryWriter(log_dir=LOG_DIR)\n\tprint('TensorboardX summary writer created')\n\t# create model\n\talexnet = AlexNet(num_classes=NUM_CLASSES).to(device)\n\t# train on multiple GPUs\n\talexnet = torch.nn.parallel.DataParallel(alexnet, device_ids=DEVICE_IDS)\n\tprint(alexnet)\n\tprint('AlexNet created')\n\t# create dataset and data loader\n\tdataset = datasets.ImageFolder(TRAIN_IMG_DIR, transforms.Compose([\n\t\t# transforms.RandomResizedCrop(IMAGE_DIM, scale=(0.9, 1.0), ratio=(0.9, 1.1)),\n\t\ttransforms.CenterCrop(IMAGE_DIM),\n\t\t# transforms.RandomHorizontalFlip(),\n\t\ttransforms.ToTensor(),\n\t\ttransforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n\t]))\n\tprint('Dataset created')\n\tdataloader = data.DataLoader(\n\t\tdataset,\n\t\tshuffle=True,\n\t\tpin_memory=True,\n\t\tnum_workers=8,\n\t\tdrop_last=True,\n\t\tbatch_size=BATCH_SIZE)\n\tprint('Dataloader created')\n\t# create optimizer\n\t# the one that WORKS\n\toptimizer = optim.Adam(params=alexnet.parameters(), lr=0.0001)\n\t### BELOW is the setting proposed by the original paper - which doesn't train....\n\t# optimizer = optim.SGD(\n\t#\t params=alexnet.parameters(),\n\t#\t lr=LR_INIT,\n\t#\t momentum=MOMENTUM,\n\t#\t weight_decay=LR_DECAY)\n\tprint('Optimizer created')\n\t# multiply LR by 1 / 10 after every 30 epochs\n\tlr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n\tprint('LR Scheduler created')\n\t# start training!!\n\tprint('Starting training...')\n\ttotal_steps = 1\n\tfor epoch in range(NUM_EPOCHS):\n\t\tlr_scheduler.step()\n\t\tfor imgs, classes in dataloader:\n\t\t\timgs, classes = imgs.to(device), classes.to(device)\n\t\t\t# calculate the loss\n\t\t\toutput = alexnet(imgs)\n\t\t\tloss = F.cross_entropy(output, classes)\n\t\t\t# update the parameters\n\t\t\toptimizer.zero_grad()\n\t\t\tloss.backward()\n\t\t\toptimizer.step()\n\t\t\t# log the information and add to tensorboard\n\t\t\tif total_steps % 10 == 0:\n\t\t\t\twith torch.no_grad():\n\t\t\t\t\t_, preds = torch.max(output, 1)\n\t\t\t\t\taccuracy = torch.sum(preds == classes)\n\t\t\t\t\tprint('Epoch: {} \\tStep: {} \\tLoss: {:.4f} \\tAcc: {}'\n\t\t\t\t\t\t.format(epoch + 1, total_steps, loss.item(), accuracy.item()))\n\t\t\t\t\ttbwriter.add_scalar('loss', loss.item(), total_steps)\n\t\t\t\t\ttbwriter.add_scalar('accuracy', accuracy.item(), total_steps)\n\t\t\t# print out gradient values and parameter average values\n\t\t\tif total_steps % 100 == 0:\n\t\t\t\twith torch.no_grad():\n\t\t\t\t\t# print and save the grad of the parameters\n\t\t\t\t\t# also print and save parameter values\n\t\t\t\t\tprint('*' * 10)\n\t\t\t\t\tfor name, parameter in alexnet.named_parameters():\n\t\t\t\t\t\tif parameter.grad is not None:\n\t\t\t\t\t\t\tavg_grad = torch.mean(parameter.grad)\n\t\t\t\t\t\t\tprint('\\t{} - grad_avg: {}'.format(name, avg_grad))\n\t\t\t\t\t\t\ttbwriter.add_scalar('grad_avg/{}'.format(name), avg_grad.item(), total_steps)\n\t\t\t\t\t\t\ttbwriter.add_histogram('grad/{}'.format(name),\n\t\t\t\t\t\t\t\t\tparameter.grad.cpu().numpy(), total_steps)\n\t\t\t\t\t\tif parameter.data is not None:\n\t\t\t\t\t\t\tavg_weight = torch.mean(parameter.data)\n\t\t\t\t\t\t\tprint('\\t{} - param_avg: {}'.format(name, avg_weight))\n\t\t\t\t\t\t\ttbwriter.add_histogram('weight/{}'.format(name),\n\t\t\t\t\t\t\t\t\tparameter.data.cpu().numpy(), total_steps)\n\t\t\t\t\t\t\ttbwriter.add_scalar('weight_avg/{}'.format(name), avg_weight.item(), total_steps)\n\t\t\ttotal_steps += 1\n\t\t# save checkpoints\n\t\tcheckpoint_path = os.path.join(CHECKPOINT_DIR, 'alexnet_states_e{}.pkl'.format(epoch + 1))\n\t\tstate = {\n\t\t\t'epoch': epoch,\n\t\t\t'total_steps': total_steps,\n\t\t\t'optimizer': optimizer.state_dict(),\n\t\t\t'model': alexnet.state_dict(),\n\t\t\t'seed': seed,\n\t\t}\n\t\ttorch.save(state, checkpoint_path)",
      "metadata": {
         "section_header": "Global Other",
         "type": "python global"
      }
   },
   {
      "page_content": "Functions: __init__, init_bias, forward",
      "metadata": {
         "section_header": "AlexNet",
         "type": "python class"
      }
   },
   {
      "page_content": "\tdef __init__(self, num_classes=1000):\n\t\t\"\"\"\n\t\tDefine and allocate layers for this neural net.\n\t\tArgs:\n\t\t\tnum_classes (int): number of classes to predict with this model\n\t\t\"\"\"\n\t\tsuper().__init__()\n\t\t# input size should be : (b x 3 x 227 x 227)\n\t\t# The image in the original paper states that width and height are 224 pixels, but\n\t\t# the dimensions after first convolution layer do not lead to 55 x 55.\n\t\tself.net = nn.Sequential(\n\t\t\tnn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4),  # (b x 96 x 55 x 55)\n\t\t\tnn.ReLU(),\n\t\t\tnn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2),  # section 3.3\n\t\t\tnn.MaxPool2d(kernel_size=3, stride=2),  # (b x 96 x 27 x 27)\n\t\t\tnn.Conv2d(96, 256, 5, padding=2),  # (b x 256 x 27 x 27)\n\t\t\tnn.ReLU(),\n\t\t\tnn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2),\n\t\t\tnn.MaxPool2d(kernel_size=3, stride=2),  # (b x 256 x 13 x 13)\n\t\t\tnn.Conv2d(256, 384, 3, padding=1),  # (b x 384 x 13 x 13)\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(384, 384, 3, padding=1),  # (b x 384 x 13 x 13)\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(384, 256, 3, padding=1),  # (b x 256 x 13 x 13)\n\t\t\tnn.ReLU(),\n\t\t\tnn.MaxPool2d(kernel_size=3, stride=2),  # (b x 256 x 6 x 6)\n\t\t)\n\t\t# classifier is just a name for linear layers\n\t\tself.classifier = nn.Sequential(\n\t\t\tnn.Dropout(p=0.5, inplace=True),\n\t\t\tnn.Linear(in_features=(256 * 6 * 6), out_features=4096),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Dropout(p=0.5, inplace=True),\n\t\t\tnn.Linear(in_features=4096, out_features=4096),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Linear(in_features=4096, out_features=num_classes),\n\t\t)\n\t\tself.init_bias()  # initialize bias",
      "metadata": {
         "section_header": "__init__",
         "type": "python function"
      }
   },
   {
      "page_content": "\tdef init_bias(self):\n\t\tfor layer in self.net:\n\t\t\tif isinstance(layer, nn.Conv2d):\n\t\t\t\tnn.init.normal_(layer.weight, mean=0, std=0.01)\n\t\t\t\tnn.init.constant_(layer.bias, 0)\n\t\t# original paper = 1 for Conv2d layers 2nd, 4th, and 5th conv layers\n\t\tnn.init.constant_(self.net[4].bias, 1)\n\t\tnn.init.constant_(self.net[10].bias, 1)\n\t\tnn.init.constant_(self.net[12].bias, 1)",
      "metadata": {
         "section_header": "init_bias",
         "type": "python function"
      }
   },
   {
      "page_content": "\tdef forward(self, x):\n\t\t\"\"\"\n\t\tPass the input through the net.\n\t\tArgs:\n\t\t\tx (Tensor): input tensor\n\t\tReturns:\n\t\t\toutput (Tensor): output tensor\n\t\t\"\"\"\n\t\tx = self.net(x)\n\t\tx = x.view(-1, 256 * 6 * 6)  # reduce the dimensions for linear layer input\n\t\treturn self.classifier(x)",
      "metadata": {
         "section_header": "forward",
         "type": "python function"
      }
   }
]