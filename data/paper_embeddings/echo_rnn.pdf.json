[
    {
        "page_content": "## I. INTRODUCTION\n\nLSTM [22] RNNs form an important class of machine learning models for analyzing sequential data, having applications in language modeling [76, 63], machine translation [5, 64, 7, 75], and speech recognition [16, 15]. In these tasks, LSTM RNNs are trained over large amounts of data samples (e.g., sentences or audio files) to capture their inherent temporal dependencies. Despite their importance, LSTM RNN training tends to be less efficient on modern GPUs compared to other types of deep neural networks (DNNs) such as Convolutional Neural Networks (CNNs) [31, 6]. One of the main reasons for such inefficiency is the high GPU memory consumption of LSTM RNNs that limits the maximum training batch size, which, in turn, limits the GPU compute utilization because of the small amount of available data parallelism [78].\n\nThere have been numerous works [18, 19, 56, 24] that propose techniques for memory footprint reduction in DNNs, but these works, unfortunately, have limited applicability for LSTM\n\nRNN training. Specifically, prior works that propose efficient compression techniques for inference (e.g., [18, 19]) focus on weights rather than the feature maps (which consume the majority of the overall memory in DNN training [56, 24]). Prior works such as vDNN [56] and Gist [24] that attempt to reduce footprint in CNN training cannot be directly applied to LSTM RNNs as they would either lead to (i) high runtime overhead for many small vector layers used in LSTM RNNs, or (ii) limited applicability, as LSTM RNNs use tanh sigmoid / , rather than ReLU activations, resulting in almost no opportunities for the data encodings proposed in Gist [24].\n\nTo better understand the reasons that lead to the high GPU memory consumption in LSTM RNNs, we perform a detailed breakdown analysis of the GPU memory consumption (and complimentary runtime analysis) during the training of the state-of-the-art LSTM RNN-based NMT model [5, 75]. We observe that (i) the feature maps of the attention and RNN layers consume the majority of the GPU memory ( feature maps are intermediate data entries saved in the forward pass to compute the gradients during the backward pass), and (ii) the runtime is unevenly distributed across different layers (fullyconnected layers dominate the runtime while other layers are relatively lightweight). From these two observations, we adopt the idea of selective recomputation , where we can leverage the low computational cost of non-fully-connected layers to recompute their feature maps during the backward pass, rather than stashing them in the GPU memory [11, 17, 37].\n\nAlthough the idea of feature map recomputation has been explored before in prior works [11, 17, 37], they fail to deliver satisfactory footprint reduction in the case of LSTM RNN training and other state-of-the-art training workloads (as we will show in Sections III-D and VI). These previous proposals are ineffective in the LSTM RNN context as a result of not addressing two important challenges:\n\n(1) Accurately estimating footprint reduction. While recomputation obviates the need to store feature maps of some layers (or group of layers), it needs to additionally stash some data entries that are needed to recompute these (group of) layers as new feature maps. Hence, a practical recomputation strategy should involve the comparison between the feature maps that are released and the ones that are newly allocated, but such a comparison is far from being trivial and is overlooked by prior works [11]. First, when making a decision on whether an operator should be recomputed, we should focus not only on the storage allocations that are local to this operator, but also on the global effects of these allocations, and take into account any potential reuse across different operators within the same\n\ncomputation graph. Second, we need a practical mechanism to estimate the recomputation benefits over the entire graph. Naïve implementation has quadratic runtime complexity, which can be impractical in the LSTM RNN context given that the number of the operator nodes in the graph is usually huge (e.g., more than 10000 in large NMT models [21]).\n\n(2) Non-conservatively estimating runtime overhead. Recomputation always comes with runtime overhead (as feature maps have to be recomputed), and hence potential targets for recomputation have to be carefully selected. Naïve implementations simply exclude any compute-heavy layers (e.g., convolutions, fully-connected layers) to keep the recomputation overhead low [11]. Such an approach is too conservative and leads to limited applicability in the LSTM RNN context. We, however, notice that if layer specifics are taken into account, certain layers that are otherwise filtered out can be amenable to recomputation with low overhead. For example, the fully-connected layers do not need recomputation as their gradients' computation does not need their outputs. Other examples of layer specifics include binarization [24] for ReLU activations and dropout layers [62]. Those layers require special handling so that we would not miss opportunities for footprint reduction and still avoid significant runtime overhead.\n\nTo effectively address these challenges and enable practical recomputation in LSTM RNN training, we propose Echo , a new compiler-based optimization scheme. Echo employs two key ideas to achieve this goal. To address the first challenge, Echo makes footprint reduction estimation practical by partitioning the whole computation graph into smaller subgraphs to restrict the scope (and hence reduce the complexity) of the footprint reduction estimation. Compute-heavy layers form the natural boundaries for partition, since they are not recomputed and therefore out of the estimation scope. Echo then analyzes each small subgraph independently and makes accurate footprint reduction estimation for recomputation. To address the second challenge, Echo infers the data dependencies of the gradient operators. Only if the gradient computation requires the forward operators' outputs will the forward operators' runtime be added as a part of the recomputation overhead estimate.\n\nOur major contributions can be summarized as follows:\n\n- (1) We present a detailed breakdown and analysis of how the GPU memory is consumed and where the runtime is spent in NMT training. Our profiling reveals that the feature maps of the attention and RNN layers form the memory bottleneck and the runtime is unevenly distributed across different layers, which motivates us to adopt the idea of selective recomputation to reduce the GPU memory footprint.\n- (2) We find and address the key challenges in making selective recomputation practical by carefully estimating the footprint reduction and runtime overhead, therefore significantly outperforming prior works in both aspects.\n- (3) We implement our ideas in a new graph optimization pass, Echo , that is a part of the graph compiler of the machine learning framework (MXNet [9] NNVM [42]). Echo reduces the GPU memory footprint transparently and automatically for numerous state-of-the-art machine learning models without\n\nany changes to the training source code (even beyond LSTM RNNs). As Section VI will show, we additionally implement hand-tuned CUDA kernels that perform recomputation more efficiently but those kernels take us several weeks to develop for every specific model, whereas Echo is effective for all the models we have examined in a fully automatic way.\n\n(4) We evaluate Echo in a state-of-the-art machine learning framework (MXNet [9]) on four state-of-the-art machine learning models [39, 78] used for machine translation (NMT [5, 75], Transformer [70]), speech recognition (DeepSpeech2 [3]), and image classification (ResNet [20]), and observe GPU memory footprint reduction ratio of 3 13 . × , 1 56 . × , 1 59 . × , and 2 13 . × correspondingly. On the NMT model, we demonstrate that this reduction can be converted into training to the same quality with a larger batch size 1 28 . × faster or training with one GPU as fast as with four, and on Transformer and ResNet models, we further show that our approach can help increase the maximum number of layers by 1 83 . × and 4 0 . × respectively while using the same GPU memory budget.",
        "metadata": {
            "section_header": "I. INTRODUCTION",
            "title": "Echo: Compiler-based GPU Memory Footprint Reduction for LSTM RNN Training",
            "type": "paper"
        }
    },
    {
        "page_content": "## III. MOTIVATION",
        "metadata": {
            "section_header": "III. MOTIVATION",
            "title": "Echo: Compiler-based GPU Memory Footprint Reduction for LSTM RNN Training",
            "type": "paper"
        }
    },
    {
        "page_content": "## A. Why does GPU memory footprint matter?\n\nThere are two major benefits of GPU memory footprint reduction. First comes from boosting the training performance\n\nby using larger training batch size, 1 and second from allowing to train wider and deeper models with the same GPU resources.\n\nIncrease Training Throughput with Larger Batch Size. We compare the training throughput between ResNet-50 [20] (CNN-based model used for image classification) and NMT [5, 75] (LSTM RNN-based model) with respect to their training batch size. Figure 3a shows the correlation between training throughput (measured as samples / second ) and batch size for ResNet-50 (detailed methodology in Section VI-A). We notice that the training throughput saturates as the batch size increases. Previous work by Zhu et al. [78] reveals that the GPU compute units have been almost fully utilized (starting from batch size of 32 ) and therefore further increasing the batch size yields little benefit on the training throughput. However, the story is different in LSTM RNNs. Figure 3b shows a similar graph for NMT. We observe that the training throughput increases linearly with the batch size, but such increase stops when the model hits the GPU memory capacity wall on a modern 11 GB RTX 2080 Ti GPU [45] at the batch size of 128 , and cannot increase any further. From the comparison, we draw the conclusion that, in LSTM RNN-based model, performance is limited by the GPU memory capacity , and hence this justifies why footprint reduction techniques can further increase the training throughput for such models (as we will show in Section VI-B).\n\n<!-- image -->\n\nFig. 3: (a) Training Throughput of ResNet-50 versus Batch Size (b) Training Throughput and GPU Memory Usage of NMT versus Batch Size (using one RTX 2080 Ti GPU)\n\n<!-- image -->\n\nRun Wider and Deeper Models. Although models such as ResNet might not always be able to benefit from the footprint reduction to achieve performance gains, they can still benefit indirectly by becoming wider and deeper while using the same GPU memory budget. For example, data encoding approach such as Gist can increase the maximum number of layers in ResNet from 851 to 1202 with a batch size of 16 [24]. In fact, as deep learning models grow larger, recent years have seen models that cannot fit into a single GPU even with a batch size of 1 [55, 66]. All these models can become more practical if efficient footprint reduction techniques are applied.",
        "metadata": {
            "section_header": "A. Why does GPU memory footprint matter?",
            "title": "Echo: Compiler-based GPU Memory Footprint Reduction for LSTM RNN Training",
            "type": "paper"
        }
    },
    {
        "page_content": "## B. Memory Consumption Breakdown\n\nTo understand the reason behind NMT's large GPU memory footprint, we develop a memory profiler to show the detailed breakdown. Our analysis is based on two orthogonal ways of categorizing memory: (1) by layer types (e.g., RNN), and (2) by data structures. Major data structures include:\n\n1 Although one might argue that large training batch size might hurt convergence, in Section VI-B we show actual training curves that increase convergence speed to the same quality when training with larger batch size.\n\n- (1) Feature Maps : Each layer needs memory for its own input and output variables. If any of those variables are needed in the backward pass to compute the gradients, it is stashed persistently in memory as feature maps, while those that are not can be released back to the storage pool. For example, consider the tanh activation function Y = tanh( X ) . Since we have Y ' = 1 -tanh ( 2 X ) , the value of tanh( X ) needs to be stored for the gradient computation in the backward pass.\n- (2) Weights : Layers such as fully-connected layers (Equation 1) have parameters W,B that are optimized as training progresses. In the following text, we use the term Weights as a generic term that includes W,B , plus their respective gradients and optimizer states which are used to do weight updates.\n- (3) Workspace is the scratchpad of a layer to compute the results. When a layer completes its own forward or backward pass, its workspace, if previously requested, can be freed.\n\nFig. 4: Memory Consumption Breakdown by Layer Types (Left) and Data Structures (Right)\n\n<!-- image -->",
        "metadata": {
            "section_header": "B. Memory Consumption Breakdown",
            "title": "Echo: Compiler-based GPU Memory Footprint Reduction for LSTM RNN Training",
            "type": "paper"
        }
    },
    {
        "page_content": "## C. Runtime Breakdown\n\nTo motivate the use of selective recomputation approach, we do a runtime profile analysis that shows the runtime distribution across different layers. Figure 5 shows the NMT runtime breakdown on one training iteration. The profile is obtained from the NVProf tool [48]. We observe that the runtime is unevenly distributed across different layers , with 50% going into the fully-connected layers and the other 50% into many small compute kernels. The longest kernel of the latter runs for only 5 ms (the mshadow bar represents the tensor library backend of MXNet and consists of multiple CUDA kernels).",
        "metadata": {
            "section_header": "C. Runtime Breakdown",
            "title": "Echo: Compiler-based GPU Memory Footprint Reduction for LSTM RNN Training",
            "type": "paper"
        }
    },
    {
        "page_content": "## D. Selective Recomputation\n\nGiven that the GPU memory consumption limits training performance and that the execution time is distributed unevenly\n\nFigure 4 shows the NMT memory consumption breakdown, with the left bar classifying memory consumption by layer types and the right by data structures. The two striped bars at the bottom show the discrepancy between the total amount of memory consumption reported by the memory profiler versus the actual memory usage given by the nvidia-smi tool [46]. Such discrepancy can be caused by memory fragmentation or allocations by the CUDA libraries [77]. We conclude from the figure that the feature maps of the attention layers, followed by RNN and output, are the major memory consumers , as they are responsible for a total of 87% ( 8 0 . GB ) of the GPU memory used by NMT [5, 75].\n\n<!-- image -->\n\nacross different layers, selective recomputation [11, 17, 37] can be a viable option to trade runtime with memory capacity.\n\nTo illustrate the key idea behind selective recomputation, consider the computation graph in Figure 6, where we have a sequence of n operator nodes. Assume that the gradient node i ' on the backward pass has data dependency on the output edge of its corresponding forward node i . The dependency is shown as an edge pointing from the forward to the backward pass\n\n- ( 1 ), which is marked as the feature map that has to be stashed. Therefore, the memory allocated for those edges cannot be released back to the storage pool, resulting in four persistent storage units by the time the forward pass completes ( 2 ). If recomputation is used, those four dependency edges can be replaced with only one edge on the input to Node",
        "metadata": {
            "section_header": "D. Selective Recomputation",
            "title": "Echo: Compiler-based GPU Memory Footprint Reduction for LSTM RNN Training",
            "type": "paper"
        }
    },
    {
        "page_content": "# 1 ( 3 ). This releases storage pressure as the inputs to Node # 2 ∼ 4 can now be taken by their respective outputs (and hence do not need to be stashed anymore), but it comes with the cost of having to redo the forward computation when the backward pass starts ( 4 ). This certainly comes with runtime overhead, but such overhead can be controlled if the recomputed nodes (shown in gray in Figure 6b) are restricted to those that are computationally cheap. Hence, selective recomputation has the potential to reduce the memory footprint at small runtime cost.\n\nFig. 6: A Computation Graph with Recomputation Applied (Red Arrows denote Persistent Feature Maps Storage)\n\n<!-- image -->\n\nIn practice, however, we observe that the current state-of-theart recomputation approach has limited benefits on the NMT training. Table I compares the training performance and GPU memory footprint between training with and without selective recomputation, where the recomputation implementation is based on the prior work [11]. We observe that recomputation causes the performance to drop by 17% , and it can only give a footprint reduction of 27% in return, which does not provide enough memory space to increase the performance by, for example, increasing the batch size.\n\nTABLE I: NMT [21] Training with &amp; without Recomputation\n\n|                                 | Baseline   | Chen et al. [11]   |\n|---------------------------------|------------|--------------------|\n| Avg. Throughput ( samples / s ) | 1192       | 983                |\n| GPU Memory Footprint ( GB )     | 10 0 .     | 7 4 .              |",
        "metadata": {
            "section_header": "This releases storage pressure as the inputs to Node # 2 ∼ 4 can now be taken by their respective outputs (and hence do not need to be stashed anymore), but it comes with the cost of having to redo the forward computation when the backward pass starts ( 4 ). This certainly comes with runtime overhead, but such overhead can be controlled if the recomputed nodes (shown in gray in Figure 6b) are restricted to those that are computationally cheap. Hence, selective recomputation has the potential to reduce the memory footprint at small runtime cost.",
            "title": "Echo: Compiler-based GPU Memory Footprint Reduction for LSTM RNN Training",
            "type": "paper"
        }
    },
    {
        "page_content": "## IV. KEY IDEAS\n\nThe major reason for the ineffectiveness of the state-of-theart implementations of the recomputation approach is because they fail to adequately address two important challenges:",
        "metadata": {
            "section_header": "IV. KEY IDEAS",
            "title": "Echo: Compiler-based GPU Memory Footprint Reduction for LSTM RNN Training",
            "type": "paper"
        }
    },
    {
        "page_content": "## A. Footprint Reduction Estimation\n\nChallenge #1. Accurately estimating footprint reduction. The effect of each operator recomputation on the total memory footprint needs to be carefully estimated. We observe from the red arrows in Figure 6b that although recomputation removes the feature maps on the outputs of Node #1 ∼ 4 , it also brings in a new data entry which is the input to Node #1 that now has to be stashed. In real models, it is possible for the latter to be larger than the former combined.\n\nConsider a more concrete example in Figure 7a, where X and Y , both being 1D arrays of shape [ N ] , are elementwiseadded first before passing through a tanh activation function. The output Z is also of shape [ N ] . This example is a simplified version of the LSTM cell described in Section II (Figure 1).\n\n<!-- image -->\n\nIn the baseline, the framework only stashes Z as the feature map ( 1 ), because it is the only value that is needed to compute the gradient in the backward pass. With the + and tanh listed as recomputed in Figure 7b, the backward dependency is brought forward from Z to X and Y ( 2 ), similar to Figure 6b. However, the only memory saving is from the feature map Z , while the new ones ( X and Y ) need new allocations, doubling the amount of persistent storage required from N to 2 N .\n\nWe therefore conclude that a practical recomputation strategy should involve the comparison between the feature maps that are released and those that are newly allocated. However, such a comparison is far from being trivial and is overlooked by prior works [11]. Although a simple comparison between the inputs size and outputs suffices to do the job in the example above, it only considers the storage allocations that are local to each operator and ignores the global impact of those allocations as it fails to consider the storage reuse across different operators within the same computation graph.\n\nConsider another example in Figure 8, where we have T tensors of shape N broadcast-added with the same tensor of shape [ T × N ] and then tanh -activated. The example is a simplified version of the attention scoring function introduced in Section II Figure 2. If we restrain the analysis scope within the dashed box and naïvely compare the inputs size versus outputs, we will arrive at the same conclusion as Figure 7 that recomputation is not needed. However, with recomputation the total feature map size can be reduced from T 2 × N to T × 2 N and the key reason is because the storage of [ T × N ] is shared by multiple operators. Therefore, we conclude that a global graph analysis is needed to take reuse effects into account when using recomputation.\n\nFig. 8: T Tensors of Shape [ N ] Broadcast-Added with [ T × N ] and tanh -Activated\n\n<!-- image -->\n\nA simple but impractical solution could be an entire computation graph traversal to verify the memory benefits of recomputing for every operator in the graph. This solution has quadratic runtime complexity, which is challenging in the LSTM RNN context because the number of operators in the graph is usually huge (e.g., more than 10K in large NMT models [21]). To address this challenge, we propose to partition the whole computation graph into small subgraphs to restrict the scope (and hence reduce the complexity) of the footprint reduction analysis. Compute-heavy layers (e.g., fully-connected layers) form the natural boundaries. This is because the goal of the footprint reduction analysis is to estimate the effect of recomputation on the total memory footprint. Since computeheavy layers are never recomputed to avoid significant runtime overhead (as we mention in Section III-D), they are excluded from the scope of analysis and hence can serve as a good place to partition. After the partitioning, each subgraph captures any reuse across tensor edges in the computation graph. We then analyze each small subgraph independently and accurately estimate potential footprint reduction from recomputation.\n\nIn Section V-A, we show examples how footprint reduction estimation can help to (i) avoid pathological cases such as Figure 7 where recomputation increases the memory footprint, and (ii) reduce the recomputation runtime overhead.",
        "metadata": {
            "section_header": "A. Footprint Reduction Estimation",
            "title": "Echo: Compiler-based GPU Memory Footprint Reduction for LSTM RNN Training",
            "type": "paper"
        }
    },
    {
        "page_content": "## B. Runtime Overhead Estimation\n\nChallenge #2. Non-conservatively estimating runtime overhead. The effect of every operator recomputation on the total execution time needs to be carefully estimated taking layer specifics into consideration. From Figure 6b, we observe that recomputation always comes with runtime overhead, because feature maps have to be recomputed in the backward pass. Therefore, in practical implementations recomputation is always done selectively. One naïve way to do this is to simply exclude all the feature maps of the compute-heavy operators (e.g., convolutions, fully-connected) from being recomputed to keep the runtime overhead low [11]. This approach, however, is too conservative and leads to limited applicability in the LSTM RNN context. A common example is the fully-connected layer that we have listed in Section II Equation 1:\n\n<!-- formula-not-decoded -->\n\nwhere E is the training loss that has to be optimized.\n\nWe observe that the gradients of the fully-connected layer only have data dependency on X and W (but not Y ), both of which are the inputs to the fully-connected layer. This implies that to recompute the feature maps of the fully-connected layer one does not have to recompute the layer itself (as the output Y is not needed). Such a property is layer specific. There are also other types of layer specific properties that can enable efficient footprint reduction. For example, the feature maps of ReLU\n\nactivations [24] and dropout layers [62] can be stored in 1 -bit binary format. Those layers require special handling so that we do not miss opportunities for footprint reduction and still avoid significant runtime overhead.\n\nTo address this challenge, we design a non-conservative runtime overhead estimator that infers the layer specifics before estimating the recomputation overhead. In the context of fullyconnected layers, the idea is that the gradient computation of these layers only need the inputs rather than the outputs. The estimator's goal is therefore to distinguish between those two cases, and only if the feature maps of an operator are part of its outputs will the operator's runtime be added as a part of the overhead estimate. The key challenge for the estimator design lies in its generality. Although hard coding the layer specifics is possible, it is not a scalable solution given the number of layers supported in a machine learning framework [9]. Hence, proper dataflow analysis is needed, as we show in Section V-B.",
        "metadata": {
            "section_header": "B. Runtime Overhead Estimation",
            "title": "Echo: Compiler-based GPU Memory Footprint Reduction for LSTM RNN Training",
            "type": "paper"
        }
    },
    {
        "page_content": "## C. Automatic and Transparent Compiler Pass Design\n\nMachine learning programmers always have the option to do recomputation manually by writing hand-tuned GPU kernels. As Section VI will show, manual recomputation has the advantage of greatly reducing the recomputation overhead (and even improve runtime due to fewer memory accesses).\n\nDespite its merits, manual recomputation has the following major shortcomings: (i) it is hard to pinpoint the correct places where recomputation should be done, especially in an LSTM RNN model with more than 10000 operators [21], and (ii) it requires nontrivial knowledge of GPU programming, machine learning algorithms, and framework system integration. This places significant burden on the programmer. We first added the recomputation optimization manually to the NMT model and spent more than two weeks seeking recomputation opportunities and testing the manual implementations in MXNet [9]. This non-trivial effort hence motivates us to push for an automatic and transparent method to perform this optimization (and we will show the performance comparison of manual versus automatic version in Section VI-B). An automatic and transparent recomputation scheme should reduce the GPU memory footprint without any changes needed to the training source code, while guaranteeing that (i) the recomputation impact on training performance is minimal and (ii) the GPU memory footprint is never worse than that in the baseline.\n\nTo this end, we present Echo , a compiler-based optimization scheme that can reduce the GPU memory footprint automatically and transparently. Echo addresses the two key challenges for accurate footprint reduction and non-conservative runtime overhead estimation in a computation graph compiler middleend, without leveraging domain-specific knowledge of the computation graph structures. We present the implementation details of Echo in Section V.",
        "metadata": {
            "section_header": "C. Automatic and Transparent Compiler Pass Design",
            "title": "Echo: Compiler-based GPU Memory Footprint Reduction for LSTM RNN Training",
            "type": "paper"
        }
    },
    {
        "page_content": "## V. IMPLEMENTATION DETAILS\n\nWe integrate the Echo compiler pass into the NNVM [42] graph compilation workflow, which is used as the computation\n\ngraph compiler middle-end for the MXNet [9] machine learning framework. Its compilation workflow is shown in Figure 9.\n\n| Pass Name         | Pass Description                                 |\n|-------------------|--------------------------------------------------|\n| Gradient          | Recomputation (Optional) Gradient Node Insertion |\n| InferShape &amp; Type | Shape and Data Type Inference                    |\n| PlanMemory        | Device Memory Allocation Planning                |\n\nGradient → InferShape &amp; Type → PlanMemory Fig. 9: MXNet NNVM Pass Workflow (in Sequence)",
        "metadata": {
            "section_header": "V. IMPLEMENTATION DETAILS",
            "title": "Echo: Compiler-based GPU Memory Footprint Reduction for LSTM RNN Training",
            "type": "paper"
        }
    },
    {
        "page_content": "## A. Footprint Reduction Estimation\n\nWe observe that with the current graph compilation workflow, it is impossible to accurately estimate footprint reduction, because critical information such as the shape and data type of each tensor edge is only available after the recomputation algorithm has been executed. We know from the example in Section IV-A (Figures 7 and 8) that this information is required to accurately estimate footprint reduction. We therefore start by changing the compilation workflow in Figure 9 to Figure 10, so that more information is available by the time Echo is executed.",
        "metadata": {
            "section_header": "A. Footprint Reduction Estimation",
            "title": "Echo: Compiler-based GPU Memory Footprint Reduction for LSTM RNN Training",
            "type": "paper"
        }
    },
    {
        "page_content": "## Gradient → InferShape &amp; Type → EdgeUseRef\n\n- → Echo → DeadNodeElimination\n\n→\n\nInferShape\n\n&amp;\n\nType\n\n→\n\nAllocateMemory\n\nFig. 10: Adjusted NNVM Pass Workflow\n\n(The first Gradient pass does not perform recomputation)\n\nWith the new workflow, allocation-relevant information (e.g., shape and data type) is now available prior to Echo , which gives it the opportunity to accurately estimate the footprint reduction brought by recomputation using a bidirectional dataflow analysis. Algorithm 1 shows Echo 's high-level workflow. Echo first performs a backward pass (line 6-10) to obtain all the possible targets for recomputation, and partitions the entire graph at the compute-heavy layers to avoid significant overhead in case these layers are included in recomputation. It then performs a forward pass (line 12-28) to remove recomputations that do not reduce memory footprint. The forward pass reduces the recomputation runtime overhead, and also guarantees that the total memory consumption after recomputation is performed will never increase compared with the baseline.\n\nFigure 11 illustrates an example similar to Figure 7, with two fully-connected layers added before the elementwise-add operator. The backward pass, shown in Figure 11a, starts from the top (i.e., output) of the graph, propagates backward along the dashed edges, and then stops at the fully-connected layers ( 1 ), because these two layers do not belong to the possible targets for recomputation as being too compute-heavy (see the runtime profile in Section III-C, Figure 5).\n\nAfter the backward pass, Echo takes the nodes and edges that the backward pass has processed (Figure 11b), which form a partitioned subgraph of the original computation graph, and assumes that all the operator nodes within this subgraph can be recomputed. This is illustrated in Figure 11c as a graph shown in gray that mirrors the subgraph. The mirrored graph is the recomputation path , similar to the gray segments in Figures 6 and 7. Due to the recomputation path, the feature maps that",
        "metadata": {
            "section_header": "Gradient → InferShape &amp; Type → EdgeUseRef",
            "title": "Echo: Compiler-based GPU Memory Footprint Reduction for LSTM RNN Training",
            "type": "paper"
        }
    },
    {
        "page_content": "## Algorithm 1: Echo 's High-Level Workflow\n\nFigure 13 shows how Echo leverages the use references information in its analysis. Let us revisit the example illustrated in Section IV-A (Figure 8), where we have T tensors of shape [ N ] broadcast-added with the same tensor of shape [ T × N ] and then tanh -activated. Although the backward pass in this example (shown in Figure 13a) is similar to that in Figure 11c, where all operators are listed as recomputed, the forward pass is different. This is because the tensor [ T × N ] is used by T different operators, and trying to remove any of those operators on the recomputation path will cause all the other operators to be removed as well, since they all need the tensor [ T × N ] to be stashed as feature maps to do recomputation. This is illustrated as T parallel arrows on the recomputation path in Figure 13a. Echo therefore needs to compare the storage allocated for all T operators' inputs with their outputs. It then notices that, since\n\n<!-- image -->\n\nare originally at the output edge of tanh in Figure 11a, are now placed at the beginning of the recomputation path ( 2 ), as we have shown in Section III-D, Figure 6.\n\nAfter the recomputation path has been formed, a followup forward pass removes recomputations that cannot reduce memory footprint. A node can be removed from the recomputation path if, by removing that node, the storage released from its inputs is greater than or equal to that allocated for its outputs.\n\nBased on the previous example, we consider to remove the + and tanh nodes from the recomputation path in succession. Figure 12a shows that the removal of + is successful, because its inputs size ( N + N ) is greater than its output ( N ). Similarly, Figure 12b shows that the removal of tanh is also successful, because its input size ( N ) is equal to its output size ( N ). After the removals, the final graph (Figure 12c) does not have any recomputation. This indicates that recomputation is unable to reduce memory footprint in this specific example (as we have seen in Section IV-A), but Echo is still able to preserve both the same performance and memory footprint as the baseline. This is in stark contrast to prior works [11, 17, 37] that introduce unnecessary recomputation which doubles the feature maps storage (shown in Section IV-A, Figure 7b).\n\nAfter the forward pass, the backward pass resumes at the inputs of the fully-connected layers in Figure 11a for the next\n\npartitioned subgraph. This backward-forward loop continues until the inputs of the whole graph are reached. As one might notice, since the next subgraph continues at the places where the previous subgraph stops, each subgraph is disjoint . In practice, for the wide set of models in our evaluation (Section VI), the number of subgraphs ranges from 100 to 2000 , with each one including no more than 10 operator nodes. These properties (disjoint and small) keep the runtime complexity of Algorithm 1 reasonable (less than 300 ms for the models in Section VI in the hardware environment listed in Section VI-A).\n\nFig. 12: Forward Analysis Example (Figure 11 Continued)\n\n<!-- image -->\n\nAs we have discussed in the example in Section IV-A (Figure 8), the comparison between the storage released and allocated requires more than just the comparison between input versus output sizes that only reflects allocations that are local to each operator. For accurate footprint reduction estimation, Echo considers the global effect (i.e., reuse ) of storage allocations within each subgraph independently , because each subgraph is disjoint and hence there is no reuse across subgraphs. Echo abstracts the reuse using the use references on each tensor edge, as it represents the number of times a particular tensor (and hence the storage allocated to that tensor) is reused . The tensor edge use references come from the EdgeUseRef pass (see Figure 10), where the compiler traverses through the whole computation graph and, for each tensor, records the number of times it is referenced by different operators.\n\nthe storage allocated for [ T × N ] is shared by all the T operators, the total inputs storage size ( T × N + T × N = T × 2 N ) is smaller than the outputs ( T × T × N = T 2 × N ) when T is large enough (usually T is in the range of 50 ∼ 100 [5, 21]). Echo therefore stops trimming the recomputation path right at the beginning, leaving all operators marked as recomputed. Indeed, the total feature maps storage in the final graph is T × 2 N , which is an order of magnitude smaller than storage required in the baseline ( T 2 × N ).\n\nWe conclude that, compared with prior works [11, 17, 37] that naïvely ignore compute-heavy operators as potential targets for recomputation. Echo uses rigorous dataflow analysis to avoid pathological cases where recomputation is not needed while preserving those where it is beneficial. This explains why Echo keeps overhead minimal and never increases the memory footprint , as our results in Section VI-C1 will show.\n\nFig. 13: Analysis Example based on Figure 8\n\n<!-- image -->\n\n(a) Recomputation Path\n\n(b) Final Graph",
        "metadata": {
            "section_header": "Algorithm 1: Echo 's High-Level Workflow",
            "title": "Echo: Compiler-based GPU Memory Footprint Reduction for LSTM RNN Training",
            "type": "paper"
        }
    },
    {
        "page_content": "## B. Runtime Overhead Estimation\n\nAs in Section IV-B, the gradients of the fully-connected layers only need their inputs. Layers such as convolutions and batched dot product also have similar property. Prior works [11] simply skip all the compute-heavy layers (Figure 14c). In contrast, Echo deems these layers as potentially good targets for recomputation by performing non-conservative runtime overhead estimation (Algorithm 1 line 13-17). Echo starts by inferring the data dependencies of the gradient operator that are specific to different types of layers. It does so by (i) creating a dummy gradient operator node, (ii) applying the gradient function to the (gradient, forward) operator tuple, and (iii) analyzing the data dependencies of the gradient operator on the forward operator. If the dependencies do not include the outputs of the forward operator, this implies that recomputation in this case does not require to recompute the operator itself, and hence its runtime is not added to the total runtime overhead when recomputing feature maps. If this is the case, Echo creates a 'dead' node on the recomputation path ( 1 ) that forwards the output edge of the previous recomputation node to the gradient node. However, the node itself is disconnected from the next node on the recomputation path. This implies that the node's outputs are never referenced by any other nodes, making the node effectively dead and thereby avoiding any unnecessary recomputation. Such an approach releases the feature maps on the input edges of the compute-heavy nodes but does not lead to situations with huge runtime overhead (as in Figure 14b), and can therefore further reduce the GPU memory footprint.\n\nAlthough the newly inserted dead nodes are never referenced, they still remain as part of the computation graph and hence\n\nFig. 14: Recomputation Strategies for Compute-Heavy Layers (shown as Red H 's) whose Gradients only need H 's Inputs\n\n<!-- image -->\n\nrequire storage allocations that are not necessary. We introduce the DeadNodeElimination pass that properly cleans up those auxiliary nodes after Echo finishes (see Figure 10).\n\nIn addition to leveraging the layer specific gradient dependencies, Echo also uses layer specific encoding to binarize [24] the feature maps of the dropout layers [62] (Algorithm 1 line 18). It encodes the dropout feature maps to 1 -bit in the forward pass and decodes them back to 32 -bit in the backward pass. Such layer specific optimizations give more footprint reduction with small runtime overhead.\n\nIn summary, we have demonstrated how Echo benefits from layer specific information and uses non-conservative runtime overhead estimation to reduce the memory footprint with low runtime overhead, as our results in Section VI will show.",
        "metadata": {
            "section_header": "B. Runtime Overhead Estimation",
            "title": "Echo: Compiler-based GPU Memory Footprint Reduction for LSTM RNN Training",
            "type": "paper"
        }
    },
    {
        "page_content": "## VI. EVALUATION",
        "metadata": {
            "section_header": "VI. EVALUATION",
            "title": "Echo: Compiler-based GPU Memory Footprint Reduction for LSTM RNN Training",
            "type": "paper"
        }
    },
    {
        "page_content": "## A. Methodology\n\nInfrastructure. Our major compute platform is a single machine with 32 -core AMD EPYC 7371 CPU [2] (with 128 GB DDR4 [26] memory) and 4 NVIDIA RTX 2080 Ti GPUs [45] (Turing architecture [47] with 11 GB GDDR6 memory [25]) connected via PCIe v3 [72], installed with CUDA 10.0 [44], cuDNN 7.6.3 [49], and MXNet v0.12.1 [40].\n\nApplications. We evaluate Echo by training the Sockeye [21] NMT toolkit on the IWSLT15 English-Vietnamese (small) [36] and WMT16 English-German (large) [73] datasets, using the hyperparameters from Zhu et al. [78] for the single-GPU experiments on the small dataset and Hieber et al. [21] for the multi-GPU experiments on the large dataset.\n\nWe also demonstrate results on DeepSpeech2 [3], the stateof-the-art (SOTA) speech recognition model [39, 78], using the LibriSpeech dataset [51]; on Transformer [70] (SOTA machine translation model [39, 78]) using the WMT16 English-German dataset [73]; and on ResNet-152 (SOTA image classification model [39, 78]) using the ImageNet dataset [14].\n\nBaselines. We compare our fully automated and transparent approach, Echo , with two baselines: the baseline system without recomputation, which we refer to as Baseline , and the state-ofthe-art implementation of recomputation by Chen et al. [11], which we refer to as Mirror . Although recomputation is also implemented in other frameworks (e.g., TensorFlow [13]), those implementations follow the prior work [11] with the limitations we previously described. These limitations have been flagged\n\nas causing memory issues (e.g., increasing the GPU memory footprint instead of decreasing it [68]), making the usability of these implementations questionable.\n\nFor the experiments on NMT [5, 75], we use the superscript ( † ) to denote our hand-tuned implementation. We leverage the information provided by Echo to pinpoint the places where recomputation is beneficial and manually fuse the operators that are on the recomputation path into a single operator (i.e., Node",
        "metadata": {
            "section_header": "A. Methodology",
            "title": "Echo: Compiler-based GPU Memory Footprint Reduction for LSTM RNN Training",
            "type": "paper"
        }
    },
    {
        "page_content": "# 1 ∼ 4 in Figure 6b are fused together as a single CUDA kernel). As we discussed in Section IV-C, this requires nontrivial effort and expertise in CUDA programming, machine learning algorithms, and framework system integration, but it shows potential benefits that recomputation can provide.\n\nMetrics. We show the (1) GPU memory consumption and (2) throughput as the key metrics. We also show the training curves in the NMT experiments, and power/energy consumption on the GPUs in the multi-GPU experiments. The training curves are expanded using the CPU wall clock time. Those curves use BLEU score [52] to quantify the machine translation quality, where a higher BLEU score means better translation quality and a BLEU score that is greater than 20 is considered strong [21, 32, 28, 52]. As training progresses, we periodically query the memory and power usage of the GPUs using the nvidia-smi tool [46] and approximate the GPU energy consumption as power over time. The throughput is reported as the average of the throughput numbers given by the MXNet speedometer [41], which measures throughput by dividing the number of training samples by the CPU wall clock time.",
        "metadata": {
            "section_header": "in Figure 6b are fused together as a single CUDA kernel). As we discussed in Section IV-C, this requires nontrivial effort and expertise in CUDA programming, machine learning algorithms, and framework system integration, but it shows potential benefits that recomputation can provide.",
            "title": "Echo: Compiler-based GPU Memory Footprint Reduction for LSTM RNN Training",
            "type": "paper"
        }
    },
    {
        "page_content": "## B. Machine Translation Results with NMT Model\n\n1) English-Vietnamese: Figure 15a and 15b illustrate the comparison of the GPU memory usage and the training throughput under different batch sizes on the English-Vietnamese translation task (the subscript B denotes the batch size used). We observe that Echo can achieve 3 13 . × footprint reduction ratio over Baseline and 2 31 . × over Mirror under the same batch size of 128 ( 35 4% . of the reduction comes from more aggressive recomputation due to non-conservative runtime overhead estimation (Section IV-B). We also observe that Echo only has 1% runtime overhead, which is 18 × less than Mirror .\n\nBecause Baseline B =128 and Mirror B =128 consume around 10 0 . GB and 7 4 . GB of GPU memory respectively when the batch size is 128 , their batch size can no longer be doubled (otherwise the GPU will run out of memory). The situation is different for Echo . Since it only consumes around 3 0 . GB of memory, Echo 's training batch size can be further increased to 256 , producing the training curve Echo B =256 in Figure 15c. By training the NMT model with larger batches, we increase the throughput by 1 27 . × and converge to the same validation BLEU score 1 28 . × faster than the baseline. The reason why the achieved speedup 1 28 . × is smaller than those shown in Figure 3b is two-fold: (1) Similar to the ResNet model [20] (Figure 3a), throughput can saturate at large batch size, as the compute resource utilization increases with the batch size. (2) The recomputation overhead lessens the performance benefits of having a larger batch size. However, such negative impact can\n\n(b) Throughput (c) Validation Curve BLEU Score Fig. 15: (a) GPU Memory Consumption, (b) Throughput, and (c) Validation Curve BLEU Score compared between Baseline , Mirror , and Echo ( † ) on English-Vietnamese NMT\n\n<!-- image -->\n\nbe mitigated by the hand-tuned implementation Echo † . As one might notice, at the same batch size of 128 , Echo † (manually optimized version of Echo ) reduces the GPU memory footprint by 3 × while increasing, rather than decreasing, the throughput by 33% . The reason for the increase is because the overhead of the recomputation is so small that it is outweighed by the benefits of kernel fusion. Such benefits include reduction in (i) the cudaLaunch overhead and (ii) the number of GPU memory accesses [74]. As Figure 15c illustrates, after doubling the batch size, Echo † improves the speed of convergence further by 1 56 . × compared with the baseline.\n\n2) English-German: Multi-GPU training is a common way to reduce the training time [75, 16], however, in multi-GPU training, communication can potentially become a bottleneck. Moreover, power and energy is now a primary concern as GPU cards such as RTX 2080 Ti is known for being power-hungry (having a TDP at around 250 W ) [45].\n\nFigure 16a and 16c show the memory usage under different batch sizes and device settings on the English-German translation task (the superscript Dev denotes the number of GPUs used, and if multiple GPUs are used, the memory usages are aggregated up across all GPUs). We observe from Figure 16a that Baseline already has a memory consumption of more than 8 GB on a single GPU when the batch size is 16 , meaning that we need to use 4 GPUs to train on a batch size of 64 . However, with Echo we can train on just a single GPU with a batch of 64 , and even 128 , as the memory consumption ( 9 6 . GB ) can still fit in the memory capacity of one RTX 2080 Ti card [45].\n\nFigure 16b and 16d show the throughput comparison between different implementations. We observe that although Echo is 14% behind Baseline when the batch size is 16 , Echo running on one GPU outperforms the baseline on four GPUs by 1 35 . × , both using the maximum training batch size. The reason for the low scalability of the multi-GPU baseline ( 2 14 . × ) is twofold: (1) the nature of the translation model that limits the scalability [33, 34] and (2) relatively low bandwidth of the PCIe interconnect. Although NVLink-enhanced compute systems such as the ones in Amazon EC2 p3.8xlarge instance [4] ( 4 NVIDIA Tesla V100 [43] GPUs connected via NVLink [50])\n\n<!-- image -->\n\ncan be used to boost the scalability up to 3 40 . × , such systems are significantly more expensive (as much as 6 × [29, 30]). Moreover, even in this hardware setup, we observe that Echo running on one GPU achieves nearly the same performance with the Baseline running on four GPUs, as Figure 17 shows.\n\nWe further observe that Echo can significantly reduce the power and energy consumption on the GPUs. In Figure 18a we show the validation BLEU score curve in one training epoch expanded by the CPU wall clock time and Figure 18b the averaged power and accumulated energy consumption of all the GPUs. After one epoch, all implementations reach a BLEU score of 28 0 . on the validation dataset, however, Echo Dev=1 B =128 completes the epoch 1 35 . × faster than Baseline Dev=4 B =64 . It also saves 65% of the energy consumption on the GPUs, because it requires only one GPU to train on this large batch size.\n\nOn the other hand, although Mirror can halve the number of GPUs required for training at a batch size of 64 , it cannot further squeeze the training model onto a single GPU, let along further doubling the batch size. Hence, it is 31% behind Echo in convergence speed and consumes 2 × more energy on the GPUs to complete the training.\n\nIn summary, we have shown the benefit of Echo on the GPU memory footprint and how to convert such benefit into faster convergence speed and/or lower GPU energy consumption. In fact, we have also implicitly shown how one could train larger and deeper model with Echo , as the model for English-German translation has 2 × more layers than that for English-Vietnamese [21, 78], but with Echo they can both train properly on a single GPU under a batch size of 128 . We conclude that Echo can not only provide performance gain and energy consumption reduction, but also allow us to train deeper models with the same amount of GPU resources.",
        "metadata": {
            "section_header": "B. Machine Translation Results with NMT Model",
            "title": "Echo: Compiler-based GPU Memory Footprint Reduction for LSTM RNN Training",
            "type": "paper"
        }
    },
    {
        "page_content": "## C. Echo Generality Across Machine Learning Models\n\nSince Echo 's key ideas are independent of the structure of the computation graph, it is potentially applicable to any\n\nmachine learning models. In this section, we evaluate Echo on four state-of-the-art (SOTA) machine learning models.\n\n- 1) DeepSpeech2: Figure 19(1) shows the comparison on the GPU memory consumption and training throughput by training the DeepSpeech2 (DS2) model [3], which is the SOTA model for speech recognition [39, 78]. We use 40K audio samples from the LibriSpeech [51] dataset for training.\n\nWe observe from Figure 19(1a) that when the batch size is small, Echo cannot provide significant footprint reduction over the baseline. The reason is because in DS2 [3] 2 93 . GB of the GPU memory is allocated for weights, making feature maps less important under small batch size. However, as the batch size increases from 8 to 24 , the GPU memory footprint with Echo increases much slower compared with the baseline and Mirror , because relative proportion of feature maps increases and the latter two cannot adequately address this increase. Thereby, Echo does not hit the GPU memory capacity wall even under a batch size of 32 , allowing the training throughput to further scale up, as is illustrated in Figure 19(1b). The recomputation runtime overhead of Echo is within 2% of the baseline under the same batch size, giving it the opportunity to compensate for the performance loss by increasing the batch size. As the rightmost bar of Figure 19(1b) shows, the throughput of Echo B =32 is 3 6 . × that of Baseline B =8 and 1 3 . × that of Baseline B =24 (the best throughput in the baseline).\n\nWe further observe from Figure 19(1a) that Mirror consumes more (rather than less) GPU memory than the baseline. This is because it fails to accurately estimate the footprint reduction effects after recomputation (Challenge #1, Section IV-A). It also has 5 × more runtime overhead compared to Echo .\n\n- 2) Transformer: Both the NMT and DS2 are RNN-based models [5, 75, 3]. To demonstrate Echo 's generality beyond RNNs, we evaluate the effect of Echo on the Transformer model [70], which is the state-of-the-art model for machine translation [39, 78] and does not have a RNN component in it. Figure 19(2) shows that Echo achieves a footprint reduction\n\nFig. 19: (a) GPU Memory Consumption and (b) Throughput compared between Baseline , Mirror , and Echo on DS2 (1), Transformer (TX, 2), and ResNet (3)\n\n<!-- image -->\n\nratio of 1 59 . × over the baseline with 3 0 . × less overhead than Mirror , where 38% of the footprint reduction over the baseline comes from layer specific knowledge that the feature maps of the dropout layer can be binarized [24] (Section IV-B).\n\n3) ResNet: All the previous models belong to the domain of sequence-to-sequence learning. To show Echo 's generality in other domains, we further evaluate the effect of Echo on the ResNet-152 model, which is the state-of-the-art model for image classification [39, 78]. Figure 19(3) shows that Echo achieves a footprint ratio of 2 13 . × over the baseline and 1 57 . × over Mirror with 1 67 . × less overhead than Mirror .\n\nAlthough models such as Transformer and ResNet might not be always able to directly benefit from the footprint reduction by achieving performance gains, they can still benefit indirectly by becoming deeper under the same GPU memory budget. Table II shows the maximum number of Transformer and ResNet layers that we can run on one RTX 2080 Ti [45]. We choose the number of layers specified in Hieber et al. [21] and He et al. [20] and pick the maximum batch sizes that can fit into the 11 GB GPU memory budget in the baseline. As we experiment on Mirror , Echo while keeping the batch sizes fixed, we observe that Echo increases the maximum number of layers by 1 83 . × and 4 0 . × on Transformer and ResNet respectively. Such increase in depth aligns with the recent trends in deep learning that have significant demand for more layers [14, 61].\n\nTABLE II: Maximum Number of Layers of Transformer [70] and ResNet [20] on one RTX 2080 Ti [45]\n\n| Model       |   Baseline |   MXNet |   Echo |\n|-------------|------------|---------|--------|\n| Transformer |          6 |       6 |     11 |\n| ResNet      |         50 |     101 |    200 |\n\nIn summary, by evaluating Echo on diverse workloads, we have shown the generality of the ideas behind Echo . We also conclude that Echo is able to efficiently and effectively reduce the memory footprint for RNNs [5, 75, 3], Transformer [70], and CNNs (ResNet [20]) with very low overhead. Such footprint benefits can be converted into performance improvements and/or an increase in the number of layers that can be executed while using the same batch size and GPU memory budget.",
        "metadata": {
            "section_header": "C. Echo Generality Across Machine Learning Models",
            "title": "Echo: Compiler-based GPU Memory Footprint Reduction for LSTM RNN Training",
            "type": "paper"
        }
    },
    {
        "page_content": "## VIII. CONCLUSION\n\nIn this paper, we propose Echo , a compiler-based optimization scheme that reduces memory footprint automatically and transparently. On four state-of-the-art machine learning models, Echo reduces the GPU memory footprint by 1 89 . × on average and 3 13 . × maximum with marginal runtime overhead. System researchers, machine learning practitioners can all benefit from Echo , as it speeds up training convergence, reduces the GPU energy consumption, and allows for larger and deeper models. We hope that Echo would become an efficient platform for further research on memory footprint optimizations and efficient system design for key machine learning applications.",
        "metadata": {
            "section_header": "VIII. CONCLUSION",
            "title": "Echo: Compiler-based GPU Memory Footprint Reduction for LSTM RNN Training",
            "type": "paper"
        }
    }
]